{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data validation using Python type annotations. pydantic enforces type hints at runtime, and provides user-friendly errors when data is invalid. Define how data should be in pure, canonical Python; validate it with pydantic . Sponsors \u00b6 Development of pydantic is made possible by the following sponsors: Salesforce FastAPI AWS Explosion TutorCruncher ExoFlare Robusta SendCloud Jina AI And many more who kindly sponsor Samuel Colvin on GitHub Sponsors . // randomize the order of sponsors const ul = document.querySelector('.sponsors') for (let i = ul.children.length; i >= 0; i--) { ul.appendChild(ul.children[Math.random() * i | 0]) } Example \u00b6 What's going on here: id is of type int; the annotation-only declaration tells pydantic that this field is required. Strings, bytes or floats will be coerced to ints if possible; otherwise an exception will be raised. name is inferred as a string from the provided default; because it has a default, it is not required. signup_ts is a datetime field which is not required (and takes the value None if it's not supplied). pydantic will process either a unix timestamp int (e.g. 1496498400 ) or a string representing the date & time. friends uses Python's typing system, and requires a list of integers. As with id , integer-like objects will be converted to integers. If validation fails pydantic will raise an error with a breakdown of what was wrong: Rationale \u00b6 So pydantic uses some cool new language features, but why should I actually go and use it? plays nicely with your IDE/linter/brain There's no new schema definition micro-language to learn. If you know how to use Python type hints, you know how to use pydantic . Data structures are just instances of classes you define with type annotations, so auto-completion, linting, mypy , IDEs (especially PyCharm ), and your intuition should all work properly with your validated data. fast pydantic has always taken performance seriously, most of the library is compiled with cython giving a ~50% speedup, it's generally as fast or faster than most similar libraries. validate complex structures use of recursive pydantic models , typing 's standard types (e.g. List , Tuple , Dict etc.) and validators allow complex data schemas to be clearly and easily defined, validated, and parsed. extensible pydantic allows custom data types to be defined or you can extend validation with methods on a model decorated with the validator decorator. dataclasses integration As well as BaseModel , pydantic provides a dataclass decorator which creates (almost) vanilla Python dataclasses with input data parsing and validation. Using Pydantic \u00b6 Hundreds of organisations and packages are using pydantic , including: FastAPI a high performance API framework, easy to learn, fast to code and ready for production, based on pydantic and Starlette. Project Jupyter developers of the Jupyter notebook are using pydantic for subprojects , through the FastAPI-based Jupyter server Jupyverse , and for FPS 's configuration management. Microsoft are using pydantic (via FastAPI) for numerous services , some of which are \"getting integrated into the core Windows product and some Office products.\" Amazon Web Services are using pydantic in gluon-ts , an open-source probabilistic time series modeling library. The NSA are using pydantic in WALKOFF , an open-source automation framework. Uber are using pydantic in Ludwig , an open-source TensorFlow wrapper. Cuenca are a Mexican neobank that uses pydantic for several internal tools (including API validation) and for open source projects like stpmex , which is used to process real-time, 24/7, inter-bank transfers in Mexico. The Molecular Sciences Software Institute are using pydantic in QCFractal , a massively distributed compute framework for quantum chemistry. Reach trusts pydantic (via FastAPI) and arq (Samuel's excellent asynchronous task queue) to reliably power multiple mission-critical microservices. Robusta.dev are using pydantic to automate Kubernetes troubleshooting and maintenance. For example, their open source tools to debug and profile Python applications on Kubernetes use pydantic models. For a more comprehensive list of open-source projects using pydantic see the list of dependents on github , Or you can find some awesome projects using pydantic in awesome-pydantic . Discussion of Pydantic \u00b6 Podcasts and videos discussing pydantic. Talk Python To Me Michael Kennedy and Samuel Colvin, the creator of pydantic , dive into the history of pydantic and its many uses and benefits. Podcast.__init__ Discussion about where pydantic came from and ideas for where it might go next with Samuel Colvin the creator of pydantic. Python Bytes Podcast \" This is a sweet simple framework that solves some really nice problems... Data validations and settings management using Python type annotations, and it's the Python type annotations that makes me really extra happy... It works automatically with all the IDE's you already have. \" --Michael Kennedy Python pydantic Introduction \u2013 Give your data classes super powers a talk by Alexander Hultn\u00e9r originally for the Python Pizza Conference introducing new users to pydantic and walking through the core features of pydantic.","title":"Overview"},{"location":"#sponsors","text":"Development of pydantic is made possible by the following sponsors: Salesforce FastAPI AWS Explosion TutorCruncher ExoFlare Robusta SendCloud Jina AI And many more who kindly sponsor Samuel Colvin on GitHub Sponsors . // randomize the order of sponsors const ul = document.querySelector('.sponsors') for (let i = ul.children.length; i >= 0; i--) { ul.appendChild(ul.children[Math.random() * i | 0]) }","title":"Sponsors"},{"location":"#example","text":"What's going on here: id is of type int; the annotation-only declaration tells pydantic that this field is required. Strings, bytes or floats will be coerced to ints if possible; otherwise an exception will be raised. name is inferred as a string from the provided default; because it has a default, it is not required. signup_ts is a datetime field which is not required (and takes the value None if it's not supplied). pydantic will process either a unix timestamp int (e.g. 1496498400 ) or a string representing the date & time. friends uses Python's typing system, and requires a list of integers. As with id , integer-like objects will be converted to integers. If validation fails pydantic will raise an error with a breakdown of what was wrong:","title":"Example"},{"location":"#rationale","text":"So pydantic uses some cool new language features, but why should I actually go and use it? plays nicely with your IDE/linter/brain There's no new schema definition micro-language to learn. If you know how to use Python type hints, you know how to use pydantic . Data structures are just instances of classes you define with type annotations, so auto-completion, linting, mypy , IDEs (especially PyCharm ), and your intuition should all work properly with your validated data. fast pydantic has always taken performance seriously, most of the library is compiled with cython giving a ~50% speedup, it's generally as fast or faster than most similar libraries. validate complex structures use of recursive pydantic models , typing 's standard types (e.g. List , Tuple , Dict etc.) and validators allow complex data schemas to be clearly and easily defined, validated, and parsed. extensible pydantic allows custom data types to be defined or you can extend validation with methods on a model decorated with the validator decorator. dataclasses integration As well as BaseModel , pydantic provides a dataclass decorator which creates (almost) vanilla Python dataclasses with input data parsing and validation.","title":"Rationale"},{"location":"#using-pydantic","text":"Hundreds of organisations and packages are using pydantic , including: FastAPI a high performance API framework, easy to learn, fast to code and ready for production, based on pydantic and Starlette. Project Jupyter developers of the Jupyter notebook are using pydantic for subprojects , through the FastAPI-based Jupyter server Jupyverse , and for FPS 's configuration management. Microsoft are using pydantic (via FastAPI) for numerous services , some of which are \"getting integrated into the core Windows product and some Office products.\" Amazon Web Services are using pydantic in gluon-ts , an open-source probabilistic time series modeling library. The NSA are using pydantic in WALKOFF , an open-source automation framework. Uber are using pydantic in Ludwig , an open-source TensorFlow wrapper. Cuenca are a Mexican neobank that uses pydantic for several internal tools (including API validation) and for open source projects like stpmex , which is used to process real-time, 24/7, inter-bank transfers in Mexico. The Molecular Sciences Software Institute are using pydantic in QCFractal , a massively distributed compute framework for quantum chemistry. Reach trusts pydantic (via FastAPI) and arq (Samuel's excellent asynchronous task queue) to reliably power multiple mission-critical microservices. Robusta.dev are using pydantic to automate Kubernetes troubleshooting and maintenance. For example, their open source tools to debug and profile Python applications on Kubernetes use pydantic models. For a more comprehensive list of open-source projects using pydantic see the list of dependents on github , Or you can find some awesome projects using pydantic in awesome-pydantic .","title":"Using Pydantic"},{"location":"#discussion-of-pydantic","text":"Podcasts and videos discussing pydantic. Talk Python To Me Michael Kennedy and Samuel Colvin, the creator of pydantic , dive into the history of pydantic and its many uses and benefits. Podcast.__init__ Discussion about where pydantic came from and ideas for where it might go next with Samuel Colvin the creator of pydantic. Python Bytes Podcast \" This is a sweet simple framework that solves some really nice problems... Data validations and settings management using Python type annotations, and it's the Python type annotations that makes me really extra happy... It works automatically with all the IDE's you already have. \" --Michael Kennedy Python pydantic Introduction \u2013 Give your data classes super powers a talk by Alexander Hultn\u00e9r originally for the Python Pizza Conference introducing new users to pydantic and walking through the core features of pydantic.","title":"Discussion of Pydantic"},{"location":"changelog/","text":"","title":"Changelog"},{"location":"contributing/","text":"We'd love you to contribute to pydantic ! Issues \u00b6 Questions, feature requests and bug reports are all welcome as discussions or issues . However, to report a security vulnerability, please see our security policy . To make it as simple as possible for us to help you, please include the output of the following call in your issue: python -c \"import pydantic.utils; print(pydantic.utils.version_info())\" If you're using pydantic prior to v1.3 (when version_info() was added), please manually include OS, Python version and pydantic version. Please try to always include the above unless you're unable to install pydantic or know it's not relevant to your question or feature request. Pull Requests \u00b6 It should be extremely simple to get started and create a Pull Request. pydantic is released regularly so you should see your improvements release in a matter of days or weeks. Note Unless your change is trivial (typo, docs tweak etc.), please create an issue to discuss the change before creating a pull request. If you're looking for something to get your teeth into, check out the \"help wanted\" label on github. To make contributing as easy and fast as possible, you'll want to run tests and linting locally. Luckily, pydantic has few dependencies, doesn't require compiling and tests don't need access to databases, etc. Because of this, setting up and running the tests should be very simple. You'll need to have a version between Python 3.7 and 3.11 , virtualenv , git , and make installed. # 1. clone your fork and cd into the repo directory git clone git@github.com:<your username>/pydantic.git cd pydantic # 2. Set up a virtualenv for running tests virtualenv -p ` which python3.8 ` env source env/bin/activate # Building docs requires 3.8. If you don't need to build docs you can use # whichever version; 3.7 will work too. # 3. Install pydantic, dependencies, test dependencies and doc dependencies make install # 4. Checkout a new branch and make your changes git checkout -b my-new-feature-branch # make your changes... # 5. Fix formatting and imports make format # Pydantic uses black to enforce formatting and isort to fix imports # (https://github.com/ambv/black, https://github.com/timothycrosley/isort) # 6. Run tests and linting make # there are a few sub-commands in Makefile like `test`, `testcov` and `lint` # which you might want to use, but generally just `make` should be all you need # 7. Build documentation make docs # if you have changed the documentation make sure it builds successfully # you can also use `make docs-serve` to serve the documentation at localhost:8000 # ... commit, push, and create your pull request tl;dr : use make format to fix formatting, make to run tests and linting & make docs to build the docs.","title":"Contributing to pydantic"},{"location":"contributing/#issues","text":"Questions, feature requests and bug reports are all welcome as discussions or issues . However, to report a security vulnerability, please see our security policy . To make it as simple as possible for us to help you, please include the output of the following call in your issue: python -c \"import pydantic.utils; print(pydantic.utils.version_info())\" If you're using pydantic prior to v1.3 (when version_info() was added), please manually include OS, Python version and pydantic version. Please try to always include the above unless you're unable to install pydantic or know it's not relevant to your question or feature request.","title":"Issues"},{"location":"contributing/#pull-requests","text":"It should be extremely simple to get started and create a Pull Request. pydantic is released regularly so you should see your improvements release in a matter of days or weeks. Note Unless your change is trivial (typo, docs tweak etc.), please create an issue to discuss the change before creating a pull request. If you're looking for something to get your teeth into, check out the \"help wanted\" label on github. To make contributing as easy and fast as possible, you'll want to run tests and linting locally. Luckily, pydantic has few dependencies, doesn't require compiling and tests don't need access to databases, etc. Because of this, setting up and running the tests should be very simple. You'll need to have a version between Python 3.7 and 3.11 , virtualenv , git , and make installed. # 1. clone your fork and cd into the repo directory git clone git@github.com:<your username>/pydantic.git cd pydantic # 2. Set up a virtualenv for running tests virtualenv -p ` which python3.8 ` env source env/bin/activate # Building docs requires 3.8. If you don't need to build docs you can use # whichever version; 3.7 will work too. # 3. Install pydantic, dependencies, test dependencies and doc dependencies make install # 4. Checkout a new branch and make your changes git checkout -b my-new-feature-branch # make your changes... # 5. Fix formatting and imports make format # Pydantic uses black to enforce formatting and isort to fix imports # (https://github.com/ambv/black, https://github.com/timothycrosley/isort) # 6. Run tests and linting make # there are a few sub-commands in Makefile like `test`, `testcov` and `lint` # which you might want to use, but generally just `make` should be all you need # 7. Build documentation make docs # if you have changed the documentation make sure it builds successfully # you can also use `make docs-serve` to serve the documentation at localhost:8000 # ... commit, push, and create your pull request tl;dr : use make format to fix formatting, make to run tests and linting & make docs to build the docs.","title":"Pull Requests"},{"location":"datamodel_code_generator/","text":"The datamodel-code-generator project is a library and command-line utility to generate pydantic models from just about any data source, including: OpenAPI 3 (YAML/JSON) JSON Schema JSON/YAML Data (which will converted to JSON Schema) Whenever you find yourself with any data convertible JSON but without pydantic models, this tool will allow you to generate type-safe model hierarchies on demand. Installation \u00b6 pip install datamodel-code-generator Example \u00b6 In this case, datamodel-code-generator creates pydantic models from a JSON Schema file. datamodel-codegen --input person.json --input-file-type jsonschema --output model.py person.json: { \"$id\" : \"person.json\" , \"$schema\" : \"http://json-schema.org/draft-07/schema#\" , \"title\" : \"Person\" , \"type\" : \"object\" , \"properties\" : { \"first_name\" : { \"type\" : \"string\" , \"description\" : \"The person's first name.\" }, \"last_name\" : { \"type\" : \"string\" , \"description\" : \"The person's last name.\" }, \"age\" : { \"description\" : \"Age in years.\" , \"type\" : \"integer\" , \"minimum\" : 0 }, \"pets\" : { \"type\" : \"array\" , \"items\" : [ { \"$ref\" : \"#/definitions/Pet\" } ] }, \"comment\" : { \"type\" : \"null\" } }, \"required\" : [ \"first_name\" , \"last_name\" ], \"definitions\" : { \"Pet\" : { \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"age\" : { \"type\" : \"integer\" } } } } } model.py: More information can be found on the official documentation","title":"Code Generation"},{"location":"datamodel_code_generator/#installation","text":"pip install datamodel-code-generator","title":"Installation"},{"location":"datamodel_code_generator/#example","text":"In this case, datamodel-code-generator creates pydantic models from a JSON Schema file. datamodel-codegen --input person.json --input-file-type jsonschema --output model.py person.json: { \"$id\" : \"person.json\" , \"$schema\" : \"http://json-schema.org/draft-07/schema#\" , \"title\" : \"Person\" , \"type\" : \"object\" , \"properties\" : { \"first_name\" : { \"type\" : \"string\" , \"description\" : \"The person's first name.\" }, \"last_name\" : { \"type\" : \"string\" , \"description\" : \"The person's last name.\" }, \"age\" : { \"description\" : \"Age in years.\" , \"type\" : \"integer\" , \"minimum\" : 0 }, \"pets\" : { \"type\" : \"array\" , \"items\" : [ { \"$ref\" : \"#/definitions/Pet\" } ] }, \"comment\" : { \"type\" : \"null\" } }, \"required\" : [ \"first_name\" , \"last_name\" ], \"definitions\" : { \"Pet\" : { \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"age\" : { \"type\" : \"integer\" } } } } } model.py: More information can be found on the official documentation","title":"Example"},{"location":"hypothesis_plugin/","text":"Hypothesis is the Python library for property-based testing . Hypothesis can infer how to construct type-annotated classes, and supports builtin types, many standard library types, and generic types from the typing and typing_extensions modules by default. From Pydantic v1.8 and Hypothesis v5.29.0 , Hypothesis will automatically load support for custom types like PaymentCardNumber and PositiveFloat , so that the st.builds() and st.from_type() strategies support them without any user configuration. Warning Please note, while the plugin supports these types, hypothesis will(currently) generate values outside of given args for the constrained function types. Example tests \u00b6 Use with JSON Schemas \u00b6 To test client-side code, you can use Model.model_json_schema() with the hypothesis-jsonschema package to generate arbitrary JSON instances matching the schema. For web API testing, Schemathesis provides a higher-level wrapper and can detect both errors and security vulnerabilities.","title":"Hypothesis plugin"},{"location":"hypothesis_plugin/#example-tests","text":"","title":"Example tests"},{"location":"hypothesis_plugin/#use-with-json-schemas","text":"To test client-side code, you can use Model.model_json_schema() with the hypothesis-jsonschema package to generate arbitrary JSON instances matching the schema. For web API testing, Schemathesis provides a higher-level wrapper and can detect both errors and security vulnerabilities.","title":"Use with JSON Schemas"},{"location":"install/","text":"Installation is as simple as: pip install pydantic pydantic has no required dependencies except Python 3.7, 3.8, 3.9, 3.10 or 3.11 and typing-extensions . If you've got Python 3.7+ and pip installed, you're good to go. Pydantic is also available on conda under the conda-forge channel: conda install pydantic -c conda-forge Compiled with Cython \u00b6 pydantic can optionally be compiled with cython which should give a 30-50% performance improvement. By default pip install provides optimized binaries via PyPI for Linux, MacOS and 64bit Windows. If you're installing manually, install cython before installing pydantic and compilation should happen automatically. To test if pydantic is compiled run: import pydantic print ( 'compiled:' , pydantic . compiled ) Performance vs package size trade-off \u00b6 Compiled binaries can increase the size of your Python environment. If for some reason you want to reduce the size of your pydantic installation you can avoid installing any binaries using the pip --no-binary option. Make sure Cython is not in your environment, or that you have the SKIP_CYTHON environment variable set to avoid re-compiling pydantic libraries: SKIP_CYTHON = 1 pip install --no-binary pydantic pydantic Note pydantic is repeated here intentionally, --no-binary pydantic tells pip you want no binaries for pydantic, the next pydantic tells pip which package to install. Alternatively, you can re-compile pydantic with custom build options , this would require having the Cython package installed before re-compiling pydantic with: CFLAGS = \"-Os -g0 -s\" pip install \\ --no-binary pydantic \\ --global-option = build_ext \\ pydantic Optional dependencies \u00b6 pydantic has one optional dependencies: If you require email validation you can add email-validator To install these along with pydantic : pip install pydantic [ email ] Of course, you can also install these requirements manually with pip install email-validator . Install from repository \u00b6 And if you prefer to install pydantic directly from the repository: pip install git+git://github.com/pydantic/pydantic@main#egg = pydantic # or with extras pip install git+git://github.com/pydantic/pydantic@main#egg = pydantic [ email ]","title":"Install"},{"location":"install/#compiled-with-cython","text":"pydantic can optionally be compiled with cython which should give a 30-50% performance improvement. By default pip install provides optimized binaries via PyPI for Linux, MacOS and 64bit Windows. If you're installing manually, install cython before installing pydantic and compilation should happen automatically. To test if pydantic is compiled run: import pydantic print ( 'compiled:' , pydantic . compiled )","title":"Compiled with Cython"},{"location":"install/#performance-vs-package-size-trade-off","text":"Compiled binaries can increase the size of your Python environment. If for some reason you want to reduce the size of your pydantic installation you can avoid installing any binaries using the pip --no-binary option. Make sure Cython is not in your environment, or that you have the SKIP_CYTHON environment variable set to avoid re-compiling pydantic libraries: SKIP_CYTHON = 1 pip install --no-binary pydantic pydantic Note pydantic is repeated here intentionally, --no-binary pydantic tells pip you want no binaries for pydantic, the next pydantic tells pip which package to install. Alternatively, you can re-compile pydantic with custom build options , this would require having the Cython package installed before re-compiling pydantic with: CFLAGS = \"-Os -g0 -s\" pip install \\ --no-binary pydantic \\ --global-option = build_ext \\ pydantic","title":"Performance vs package size trade-off"},{"location":"install/#optional-dependencies","text":"pydantic has one optional dependencies: If you require email validation you can add email-validator To install these along with pydantic : pip install pydantic [ email ] Of course, you can also install these requirements manually with pip install email-validator .","title":"Optional dependencies"},{"location":"install/#install-from-repository","text":"And if you prefer to install pydantic directly from the repository: pip install git+git://github.com/pydantic/pydantic@main#egg = pydantic # or with extras pip install git+git://github.com/pydantic/pydantic@main#egg = pydantic [ email ]","title":"Install from repository"},{"location":"internals/","text":"Internals \u00b6 How the magic of DirtyEquals.__eq__ works? \u00b6 When you call x == y , Python first calls x.__eq__(y) . This would not help us much, because we would have to keep an eye on order of the arguments when comparing to DirtyEquals objects. But that's where were another feature of Python comes in. When x.__eq__(y) returns the NotImplemented object, then Python will try to call y.__eq__(x) . Objects in the standard library return that value when they don't know how to compare themselves to objects of type(y) (Without checking the C source I can't be certain if this assumption holds for all classes, but it works for all the basic ones). In pathlib.PurePath you can see an example how that is implemented in Python. By default, object implements __eq__() by using is , returning NotImplemented in the case of a false comparison: True if x is y else NotImplemented . See the Python documentation for more information ( object.__eq__ ).","title":"Internals"},{"location":"internals/#internals","text":"","title":"Internals"},{"location":"internals/#how-the-magic-of-dirtyequals__eq__-works","text":"When you call x == y , Python first calls x.__eq__(y) . This would not help us much, because we would have to keep an eye on order of the arguments when comparing to DirtyEquals objects. But that's where were another feature of Python comes in. When x.__eq__(y) returns the NotImplemented object, then Python will try to call y.__eq__(x) . Objects in the standard library return that value when they don't know how to compare themselves to objects of type(y) (Without checking the C source I can't be certain if this assumption holds for all classes, but it works for all the basic ones). In pathlib.PurePath you can see an example how that is implemented in Python. By default, object implements __eq__() by using is , returning NotImplemented in the case of a false comparison: True if x is y else NotImplemented . See the Python documentation for more information ( object.__eq__ ).","title":"How the magic of DirtyEquals.__eq__ works?"},{"location":"mypy_plugin/","text":"Pydantic works well with mypy right out of the box . However, Pydantic also ships with a mypy plugin that adds a number of important pydantic-specific features to mypy that improve its ability to type-check your code. For example, consider the following script: Without any special configuration, mypy catches one of the errors (see here for usage instructions): 13: error: \"Model\" has no attribute \"middle_name\" But with the plugin enabled , it catches both: 13: error: \"Model\" has no attribute \"middle_name\" 16: error: Missing named argument \"age\" for \"Model\" 16: error: Missing named argument \"list_of_ints\" for \"Model\" With the pydantic mypy plugin, you can fearlessly refactor your models knowing mypy will catch any mistakes if your field names or types change. There are other benefits too! See below for more details. Plugin Capabilities \u00b6 Generate a signature for Model.__init__ \u00b6 Any required fields that don't have dynamically-determined aliases will be included as required keyword arguments. If Config.allow_population_by_field_name=True , the generated signature will use the field names, rather than aliases. If Config.extra=\"forbid\" and you don't make use of dynamically-determined aliases, the generated signature will not allow unexpected inputs. Optional: If the init_forbid_extra plugin setting is set to True , unexpected inputs to __init__ will raise errors even if Config.extra is not \"forbid\" . Optional: If the init_typed plugin setting is set to True , the generated signature will use the types of the model fields (otherwise they will be annotated as Any to allow parsing). Generate a typed signature for Model.model_construct \u00b6 The model_construct method is a faster alternative to __init__ when input data is known to be valid and does not need to be parsed. But because this method performs no runtime validation, static checking is important to detect errors. Respect Config.allow_mutation \u00b6 If Config.allow_mutation is False , you'll get a mypy error if you try to change the value of a model field; cf. faux immutability . Respect Config.orm_mode \u00b6 If Config.orm_mode is False , you'll get a mypy error if you try to call .from_orm() ; cf. ORM mode Generate a signature for dataclasses \u00b6 classes decorated with @pydantic.dataclasses.dataclass are type checked the same as standard Python dataclasses The @pydantic.dataclasses.dataclass decorator accepts a config keyword argument which has the same meaning as the Config sub-class . Respect the type of the Field 's default and default_factory \u00b6 Field with both a default and a default_factory will result in an error during static checking. The type of the default and default_factory value must be compatible with the one of the field. Optional Capabilities: \u00b6 Prevent the use of required dynamic aliases \u00b6 If the warn_required_dynamic_aliases plugin setting is set to True , you'll get a mypy error any time you use a dynamically-determined alias or alias generator on a model with Config.allow_population_by_field_name=False . This is important because if such aliases are present, mypy cannot properly type check calls to __init__ . In this case, it will default to treating all arguments as optional. Prevent the use of untyped fields \u00b6 If the warn_untyped_fields plugin setting is set to True , you'll get a mypy error any time you create a field on a model without annotating its type. This is important because non-annotated fields may result in validators being applied in a surprising order . In addition, mypy may not be able to correctly infer the type of the field, and may miss checks or raise spurious errors. Enabling the Plugin \u00b6 To enable the plugin, just add pydantic.mypy to the list of plugins in your mypy config file (this could be mypy.ini or setup.cfg ). To get started, all you need to do is create a mypy.ini file with following contents: [mypy] plugins = pydantic.mypy The plugin is compatible with mypy versions >=0.930 . See the mypy usage and plugin configuration docs for more details. Configuring the Plugin \u00b6 To change the values of the plugin settings, create a section in your mypy config file called [pydantic-mypy] , and add any key-value pairs for settings you want to override. A mypy.ini file with all plugin strictness flags enabled (and some other mypy strictness flags, too) might look like: [mypy] plugins = pydantic.mypy follow_imports = silent warn_redundant_casts = True warn_unused_ignores = True disallow_any_generics = True check_untyped_defs = True no_implicit_reexport = True # for strict mypy: (this is the tricky one :-)) disallow_untyped_defs = True [pydantic-mypy] init_forbid_extra = True init_typed = True warn_required_dynamic_aliases = True warn_untyped_fields = True As of mypy>=0.900 , mypy config may also be included in the pyproject.toml file rather than mypy.ini . The same configuration as above would be: [tool.mypy] plugins = [ \"pydantic.mypy\" ] follow_imports = \"silent\" warn_redundant_casts = true warn_unused_ignores = true disallow_any_generics = true check_untyped_defs = true no_implicit_reexport = true # for strict mypy: (this is the tricky one :-)) disallow_untyped_defs = true [tool.pydantic-mypy] init_forbid_extra = true init_typed = true warn_required_dynamic_aliases = true warn_untyped_fields = true","title":"Mypy plugin"},{"location":"mypy_plugin/#plugin-capabilities","text":"","title":"Plugin Capabilities"},{"location":"mypy_plugin/#generate-a-signature-for-model__init__","text":"Any required fields that don't have dynamically-determined aliases will be included as required keyword arguments. If Config.allow_population_by_field_name=True , the generated signature will use the field names, rather than aliases. If Config.extra=\"forbid\" and you don't make use of dynamically-determined aliases, the generated signature will not allow unexpected inputs. Optional: If the init_forbid_extra plugin setting is set to True , unexpected inputs to __init__ will raise errors even if Config.extra is not \"forbid\" . Optional: If the init_typed plugin setting is set to True , the generated signature will use the types of the model fields (otherwise they will be annotated as Any to allow parsing).","title":"Generate a signature for Model.__init__"},{"location":"mypy_plugin/#generate-a-typed-signature-for-modelmodel_construct","text":"The model_construct method is a faster alternative to __init__ when input data is known to be valid and does not need to be parsed. But because this method performs no runtime validation, static checking is important to detect errors.","title":"Generate a typed signature for Model.model_construct"},{"location":"mypy_plugin/#respect-configallow_mutation","text":"If Config.allow_mutation is False , you'll get a mypy error if you try to change the value of a model field; cf. faux immutability .","title":"Respect Config.allow_mutation"},{"location":"mypy_plugin/#respect-configorm_mode","text":"If Config.orm_mode is False , you'll get a mypy error if you try to call .from_orm() ; cf. ORM mode","title":"Respect Config.orm_mode"},{"location":"mypy_plugin/#generate-a-signature-for-dataclasses","text":"classes decorated with @pydantic.dataclasses.dataclass are type checked the same as standard Python dataclasses The @pydantic.dataclasses.dataclass decorator accepts a config keyword argument which has the same meaning as the Config sub-class .","title":"Generate a signature for dataclasses"},{"location":"mypy_plugin/#respect-the-type-of-the-fields-default-and-default_factory","text":"Field with both a default and a default_factory will result in an error during static checking. The type of the default and default_factory value must be compatible with the one of the field.","title":"Respect the type of the Field's default and default_factory"},{"location":"mypy_plugin/#optional-capabilities","text":"","title":"Optional Capabilities:"},{"location":"mypy_plugin/#prevent-the-use-of-required-dynamic-aliases","text":"If the warn_required_dynamic_aliases plugin setting is set to True , you'll get a mypy error any time you use a dynamically-determined alias or alias generator on a model with Config.allow_population_by_field_name=False . This is important because if such aliases are present, mypy cannot properly type check calls to __init__ . In this case, it will default to treating all arguments as optional.","title":"Prevent the use of required dynamic aliases"},{"location":"mypy_plugin/#prevent-the-use-of-untyped-fields","text":"If the warn_untyped_fields plugin setting is set to True , you'll get a mypy error any time you create a field on a model without annotating its type. This is important because non-annotated fields may result in validators being applied in a surprising order . In addition, mypy may not be able to correctly infer the type of the field, and may miss checks or raise spurious errors.","title":"Prevent the use of untyped fields"},{"location":"mypy_plugin/#enabling-the-plugin","text":"To enable the plugin, just add pydantic.mypy to the list of plugins in your mypy config file (this could be mypy.ini or setup.cfg ). To get started, all you need to do is create a mypy.ini file with following contents: [mypy] plugins = pydantic.mypy The plugin is compatible with mypy versions >=0.930 . See the mypy usage and plugin configuration docs for more details.","title":"Enabling the Plugin"},{"location":"mypy_plugin/#configuring-the-plugin","text":"To change the values of the plugin settings, create a section in your mypy config file called [pydantic-mypy] , and add any key-value pairs for settings you want to override. A mypy.ini file with all plugin strictness flags enabled (and some other mypy strictness flags, too) might look like: [mypy] plugins = pydantic.mypy follow_imports = silent warn_redundant_casts = True warn_unused_ignores = True disallow_any_generics = True check_untyped_defs = True no_implicit_reexport = True # for strict mypy: (this is the tricky one :-)) disallow_untyped_defs = True [pydantic-mypy] init_forbid_extra = True init_typed = True warn_required_dynamic_aliases = True warn_untyped_fields = True As of mypy>=0.900 , mypy config may also be included in the pyproject.toml file rather than mypy.ini . The same configuration as above would be: [tool.mypy] plugins = [ \"pydantic.mypy\" ] follow_imports = \"silent\" warn_redundant_casts = true warn_unused_ignores = true disallow_any_generics = true check_untyped_defs = true no_implicit_reexport = true # for strict mypy: (this is the tricky one :-)) disallow_untyped_defs = true [tool.pydantic-mypy] init_forbid_extra = true init_typed = true warn_required_dynamic_aliases = true warn_untyped_fields = true","title":"Configuring the Plugin"},{"location":"pycharm_plugin/","text":"While pydantic will work well with any IDE out of the box, a PyCharm plugin offering improved pydantic integration is available on the JetBrains Plugins Repository for PyCharm. You can install the plugin for free from the plugin marketplace (PyCharm's Preferences -> Plugin -> Marketplace -> search \"pydantic\"). The plugin currently supports the following features: For pydantic.BaseModel.__init__ : Inspection Autocompletion Type-checking For fields of pydantic.BaseModel : Refactor-renaming fields updates __init__ calls, and affects sub- and super-classes Refactor-renaming __init__ keyword arguments updates field names, and affects sub- and super-classes More information can be found on the official plugin page and Github repository .","title":"PyCharm plugin"},{"location":"usage/","text":"Boolean Logic \u00b6 dirty-equals types can be combined based on either & (and, all checks must be True for the combined check to be True ) or | (or, any check can be True for the combined check to be True ). Types can also be inverted using the ~ operator, this is equivalent to using != instead of == . Example: Boolean Combination of Types from dirty_equals import HasLen , Contains assert [ 'a' , 'b' , 'c' ] == HasLen ( 3 ) & Contains ( 'a' ) #(1)! assert [ 'a' , 'b' , 'c' ] == HasLen ( 3 ) | Contains ( 'z' ) #(2)! assert [ 'a' , 'b' , 'c' ] != Contains ( 'z' ) assert [ 'a' , 'b' , 'c' ] == ~ Contains ( 'z' ) The object on the left has to both have length 3 and contain \"a\" The object on the left has to either have length 3 or contain \"z\" Initialised vs. Class comparison \u00b6 Warning This does not work with PyPy. dirty-equals allows comparison with types regardless of whether they've been initialised. This saves users adding () in lots of places. Example: Initialised vs. Uninitialised from dirty_equals import IsInt # these two cases are the same assert 1 == IsInt assert 1 == IsInt () Note Types that require at least on argument when being initialised (like IsApprox ) cannot be used like this, comparisons will just return False . __repr__ and pytest compatibility \u00b6 dirty-equals types have reasonable __repr__ methods, which describe types and generally are a close match of how they would be created: __repr__ from dirty_equals import IsInt , IsApprox assert repr ( IsInt ) == 'IsInt' assert repr ( IsInt ()) == 'IsInt()' assert repr ( IsApprox ( 42 )) == 'IsApprox(approx=42)' However the repr method of types changes when an equals ( == ) operation on them returns a True , in this case the __repr__ method will return repr(other) . repr() after comparison from dirty_equals import IsInt v = IsInt () assert 42 == v assert repr ( v ) == '42' This black magic is designed to make the output of pytest when asserts on large objects fail as simple as possible to read. Consider the following unit test: pytest error example from datetime import datetime from dirty_equals import IsPositiveInt , IsNow def test_partial_dict (): api_response_data = { 'id' : 1 , #(1)! 'first_name' : 'John' , 'last_name' : 'Doe' , 'created_at' : datetime . now () . isoformat (), 'phone' : '+44 123456789' , } assert api_response_data == { 'id' : IsPositiveInt (), 'first_name' : 'John' , 'last_name' : 'Doe' , 'created_at' : IsNow ( iso_string = True ), # phone number is missing, so the test will fail } For simplicity we've hardcoded id here, but in a test it could be any positive int, hence why we need IsPositiveInt() Here's an except from the output of pytest -vv show the error details: pytest output E Common items: E {'created_at': '2022-02-25T15:41:38.493512', E 'first_name': 'John', E 'id': 1, E 'last_name': 'Doe'} E Left contains 1 more item: E {'phone': '+44 123456789'} E Full diff: E { E 'created_at': '2022-02-25T15:41:38.493512', E 'first_name': 'John', E 'id': 1, E 'last_name': 'Doe', E + 'phone': '+44 123456789', E } It's easy to see that the phone key is missing, id and created_at are represented by the exact values they were compared to, so don't show as different in the \"Full diff\" section. Warning This black magic only works when using initialised types, if IsPositiveInt was used instead IsPositiveInt() in the above example, the output would not be as clean.","title":"Usage"},{"location":"usage/#boolean-logic","text":"dirty-equals types can be combined based on either & (and, all checks must be True for the combined check to be True ) or | (or, any check can be True for the combined check to be True ). Types can also be inverted using the ~ operator, this is equivalent to using != instead of == . Example: Boolean Combination of Types from dirty_equals import HasLen , Contains assert [ 'a' , 'b' , 'c' ] == HasLen ( 3 ) & Contains ( 'a' ) #(1)! assert [ 'a' , 'b' , 'c' ] == HasLen ( 3 ) | Contains ( 'z' ) #(2)! assert [ 'a' , 'b' , 'c' ] != Contains ( 'z' ) assert [ 'a' , 'b' , 'c' ] == ~ Contains ( 'z' ) The object on the left has to both have length 3 and contain \"a\" The object on the left has to either have length 3 or contain \"z\"","title":"Boolean Logic"},{"location":"usage/#initialised-vs-class-comparison","text":"Warning This does not work with PyPy. dirty-equals allows comparison with types regardless of whether they've been initialised. This saves users adding () in lots of places. Example: Initialised vs. Uninitialised from dirty_equals import IsInt # these two cases are the same assert 1 == IsInt assert 1 == IsInt () Note Types that require at least on argument when being initialised (like IsApprox ) cannot be used like this, comparisons will just return False .","title":"Initialised vs. Class comparison"},{"location":"usage/#__repr__-and-pytest-compatibility","text":"dirty-equals types have reasonable __repr__ methods, which describe types and generally are a close match of how they would be created: __repr__ from dirty_equals import IsInt , IsApprox assert repr ( IsInt ) == 'IsInt' assert repr ( IsInt ()) == 'IsInt()' assert repr ( IsApprox ( 42 )) == 'IsApprox(approx=42)' However the repr method of types changes when an equals ( == ) operation on them returns a True , in this case the __repr__ method will return repr(other) . repr() after comparison from dirty_equals import IsInt v = IsInt () assert 42 == v assert repr ( v ) == '42' This black magic is designed to make the output of pytest when asserts on large objects fail as simple as possible to read. Consider the following unit test: pytest error example from datetime import datetime from dirty_equals import IsPositiveInt , IsNow def test_partial_dict (): api_response_data = { 'id' : 1 , #(1)! 'first_name' : 'John' , 'last_name' : 'Doe' , 'created_at' : datetime . now () . isoformat (), 'phone' : '+44 123456789' , } assert api_response_data == { 'id' : IsPositiveInt (), 'first_name' : 'John' , 'last_name' : 'Doe' , 'created_at' : IsNow ( iso_string = True ), # phone number is missing, so the test will fail } For simplicity we've hardcoded id here, but in a test it could be any positive int, hence why we need IsPositiveInt() Here's an except from the output of pytest -vv show the error details: pytest output E Common items: E {'created_at': '2022-02-25T15:41:38.493512', E 'first_name': 'John', E 'id': 1, E 'last_name': 'Doe'} E Left contains 1 more item: E {'phone': '+44 123456789'} E Full diff: E { E 'created_at': '2022-02-25T15:41:38.493512', E 'first_name': 'John', E 'id': 1, E 'last_name': 'Doe', E + 'phone': '+44 123456789', E } It's easy to see that the phone key is missing, id and created_at are represented by the exact values they were compared to, so don't show as different in the \"Full diff\" section. Warning This black magic only works when using initialised types, if IsPositiveInt was used instead IsPositiveInt() in the above example, the output would not be as clean.","title":"__repr__ and pytest compatibility"},{"location":"visual_studio_code/","text":"pydantic works well with any editor or IDE out of the box because it's made on top of standard Python type annotations. When using Visual Studio Code (VS Code) , there are some additional editor features supported, comparable to the ones provided by the PyCharm plugin . This means that you will have autocompletion (or \"IntelliSense\") and error checks for types and required arguments even while creating new pydantic model instances. Configure VS Code \u00b6 To take advantage of these features, you need to make sure you configure VS Code correctly, using the recommended settings. In case you have a different configuration, here's a short overview of the steps. Install Pylance \u00b6 You should use the Pylance extension for VS Code. It is the recommended, next-generation, official VS Code plug-in for Python. Pylance is installed as part of the Python Extension for VS Code by default, so it should probably just work. Otherwise, you can double check it's installed and enabled in your editor. Configure your environment \u00b6 Then you need to make sure your editor knows the Python environment (probably a virtual environment) for your Python project. This would be the environment in where you installed pydantic . Configure Pylance \u00b6 With the default configurations, you will get support for autocompletion, but Pylance might not check for type errors. You can enable type error checks from Pylance with these steps: Open the \"User Settings\" Search for Type Checking Mode You will find an option under Python \u203a Analysis: Type Checking Mode Set it to basic or strict (by default it's off ) Now you will not only get autocompletion when creating new pydantic model instances but also error checks for required arguments . And you will also get error checks for invalid data types . Technical Details Pylance is the VS Code extension, it's closed source, but free to use. Underneath, Pylance uses an open source tool (also from Microsoft) called Pyright that does all the heavy lifting. You can read more about it in the Pylance Frequently Asked Questions . Configure mypy \u00b6 You might also want to configure mypy in VS Code to get mypy error checks inline in your editor (alternatively/additionally to Pylance). This would include the errors detected by the pydantic mypy plugin , if you configured it. To enable mypy in VS Code, do the following: Open the \"User Settings\" Search for Mypy Enabled You will find an option under Python \u203a Linting: Mypy Enabled Check the box (by default it's unchecked) Tips and tricks \u00b6 Here are some additional tips and tricks to improve your developer experience when using VS Code with pydantic . Strict errors \u00b6 The way this additional editor support works is that Pylance will treat your pydantic models as if they were Python's pure dataclasses . And it will show strict type error checks about the data types passed in arguments when creating a new pydantic model instance. In this example you can see that it shows that a str of '23' is not a valid int for the argument age . It would expect age=23 instead of age='23' . Nevertheless, the design, and one of the main features of pydantic , is that it is very lenient with data types . It will actually accept the str with value '23' and will convert it to an int with value 23 . These strict error checks are very useful most of the time and can help you detect many bugs early . But there are cases, like with age='23' , where they could be inconvenient by reporting a \"false positive\" error. This example above with age='23' is intentionally simple, to show the error and the differences in types. But more common cases where these strict errors would be inconvenient would be when using more sophisticated data types, like int values for datetime fields, or dict values for pydantic sub-models. For example, this is valid for pydantic : from pydantic import BaseModel class Knight ( BaseModel ): title : str age : int color : str = 'blue' class Quest ( BaseModel ): title : str knight : Knight quest = Quest ( title = 'To seek the Holy Grail' , knight = { 'title' : 'Sir Lancelot' , 'age' : 23 } ) The type of the field knight is declared with the class Knight (a pydantic model) and the code is passing a literal dict instead. This is still valid for pydantic , and the dict would be automatically converted to a Knight instance. Nevertheless, it would be detected as a type error: In those cases, there are several ways to disable or ignore strict errors in very specific places, while still preserving them in the rest of the code. Below are several techniques to achieve it. Disable type checks in a line \u00b6 You can disable the errors for a specific line using a comment of: # type: ignore or (to be specific to pylance/pyright): # pyright: ignore ( pyright is the language server used by Pylance.). coming back to the example with age='23' , it would be: from pydantic import BaseModel class Knight ( BaseModel ): title : str age : int color : str = 'blue' lancelot = Knight ( title = 'Sir Lancelot' , age = '23' ) # pyright: ignore that way Pylance and mypy will ignore errors in that line. Pros : it's a simple change in that line to remove errors there. Cons : any other error in that line will also be omitted, including type checks, misspelled arguments, required arguments not provided, etc. Override the type of a variable \u00b6 You can also create a variable with the value you want to use and declare it's type explicitly with Any . from typing import Any from pydantic import BaseModel class Knight ( BaseModel ): title : str age : int color : str = 'blue' age_str : Any = '23' lancelot = Knight ( title = 'Sir Lancelot' , age = age_str ) that way Pylance and mypy will interpret the variable age_str as if they didn't know its type, instead of knowing it has a type of str when an int was expected (and then showing the corresponding error). Pros : errors will be ignored only for a specific value, and you will still see any additional errors for the other arguments. Cons : it requires importing Any and a new variable in a new line for each argument that needs ignoring errors. Override the type of a value with cast \u00b6 The same idea from the previous example can be put on the same line with the help of cast() . This way, the type declaration of the value is overriden inline, without requiring another variable. from typing import Any , cast from pydantic import BaseModel class Knight ( BaseModel ): title : str age : int color : str = 'blue' lancelot = Knight ( title = 'Sir Lancelot' , age = cast ( Any , '23' )) cast(Any, '23') doesn't affect the value, it's still just '23' , but now Pylance and mypy will assume it is of type Any , which means, they will act as if they didn't know the type of the value. So, this is the equivalent of the previous example, without the additional variable. Pros : errors will be ignored only for a specific value, and you will still see any additional errors for the other arguments. There's no need for additional variables. Cons : it requires importing Any and cast , and if you are not used to using cast() , it could seem strange at first. Config in class arguments \u00b6 pydantic has a rich set of Model Configurations available. These configurations can be set in an internal class Config on each model: from pydantic import BaseModel class Knight ( BaseModel ): title : str age : int color : str = 'blue' class Config : frozen = True or passed as keyword arguments when defining the model class: from pydantic import BaseModel class Knight ( BaseModel , frozen = True ): title : str age : int color : str = 'blue' The specific configuration frozen (in beta) has a special meaning. It prevents other code from changing a model instance once it's created, keeping it \"frozen\" . When using the second version to declare frozen=True (with keyword arguments in the class definition), Pylance can use it to help you check in your code and detect errors when something is trying to set values in a model that is \"frozen\". Adding a default with Field \u00b6 Pylance/pyright requires default to be a keyword argument to Field in order to infer that the field is optional. from pydantic import BaseModel , Field class Knight ( BaseModel ): title : str = Field ( default = 'Sir Lancelot' ) # this is okay age : int = Field ( 23 ) # this works fine at runtime but will case an error for pyright lance = Knight () # error: Argument missing for parameter \"age\" This is a limitation of dataclass transforms and cannot be fixed in pydantic. Technical Details \u00b6 Warning As a pydantic user, you don't need the details below. Feel free to skip the rest of this section. These details are only useful for other library authors, etc. This additional editor support works by implementing the proposed draft standard for Dataclass Transform . The proposed draft standard is written by Eric Traut, from the Microsoft team, the same author of the open source package Pyright (used by Pylance to provide Python support in VS Code). The intention of the standard is to provide a way for libraries like pydantic and others to tell editors and tools that they (the editors) should treat these libraries (e.g. pydantic ) as if they were dataclasses , providing autocompletion, type checks, etc. The draft standard also includes an Alternate Form for early adopters, like pydantic , to add support for it right away, even before the new draft standard is finished and approved. This new draft standard, with the Alternate Form, is already supported by Pyright, so it can be used via Pylance in VS Code. As it is being proposed as an official standard for Python, other editors can also easily add support for it. And authors of other libraries similar to pydantic can also easily adopt the standard right away (using the \"Alternate Form\") and get the benefits of these additional editor features.","title":"Visual Studio Code"},{"location":"visual_studio_code/#configure-vs-code","text":"To take advantage of these features, you need to make sure you configure VS Code correctly, using the recommended settings. In case you have a different configuration, here's a short overview of the steps.","title":"Configure VS Code"},{"location":"visual_studio_code/#install-pylance","text":"You should use the Pylance extension for VS Code. It is the recommended, next-generation, official VS Code plug-in for Python. Pylance is installed as part of the Python Extension for VS Code by default, so it should probably just work. Otherwise, you can double check it's installed and enabled in your editor.","title":"Install Pylance"},{"location":"visual_studio_code/#configure-your-environment","text":"Then you need to make sure your editor knows the Python environment (probably a virtual environment) for your Python project. This would be the environment in where you installed pydantic .","title":"Configure your environment"},{"location":"visual_studio_code/#configure-pylance","text":"With the default configurations, you will get support for autocompletion, but Pylance might not check for type errors. You can enable type error checks from Pylance with these steps: Open the \"User Settings\" Search for Type Checking Mode You will find an option under Python \u203a Analysis: Type Checking Mode Set it to basic or strict (by default it's off ) Now you will not only get autocompletion when creating new pydantic model instances but also error checks for required arguments . And you will also get error checks for invalid data types . Technical Details Pylance is the VS Code extension, it's closed source, but free to use. Underneath, Pylance uses an open source tool (also from Microsoft) called Pyright that does all the heavy lifting. You can read more about it in the Pylance Frequently Asked Questions .","title":"Configure Pylance"},{"location":"visual_studio_code/#configure-mypy","text":"You might also want to configure mypy in VS Code to get mypy error checks inline in your editor (alternatively/additionally to Pylance). This would include the errors detected by the pydantic mypy plugin , if you configured it. To enable mypy in VS Code, do the following: Open the \"User Settings\" Search for Mypy Enabled You will find an option under Python \u203a Linting: Mypy Enabled Check the box (by default it's unchecked)","title":"Configure mypy"},{"location":"visual_studio_code/#tips-and-tricks","text":"Here are some additional tips and tricks to improve your developer experience when using VS Code with pydantic .","title":"Tips and tricks"},{"location":"visual_studio_code/#strict-errors","text":"The way this additional editor support works is that Pylance will treat your pydantic models as if they were Python's pure dataclasses . And it will show strict type error checks about the data types passed in arguments when creating a new pydantic model instance. In this example you can see that it shows that a str of '23' is not a valid int for the argument age . It would expect age=23 instead of age='23' . Nevertheless, the design, and one of the main features of pydantic , is that it is very lenient with data types . It will actually accept the str with value '23' and will convert it to an int with value 23 . These strict error checks are very useful most of the time and can help you detect many bugs early . But there are cases, like with age='23' , where they could be inconvenient by reporting a \"false positive\" error. This example above with age='23' is intentionally simple, to show the error and the differences in types. But more common cases where these strict errors would be inconvenient would be when using more sophisticated data types, like int values for datetime fields, or dict values for pydantic sub-models. For example, this is valid for pydantic : from pydantic import BaseModel class Knight ( BaseModel ): title : str age : int color : str = 'blue' class Quest ( BaseModel ): title : str knight : Knight quest = Quest ( title = 'To seek the Holy Grail' , knight = { 'title' : 'Sir Lancelot' , 'age' : 23 } ) The type of the field knight is declared with the class Knight (a pydantic model) and the code is passing a literal dict instead. This is still valid for pydantic , and the dict would be automatically converted to a Knight instance. Nevertheless, it would be detected as a type error: In those cases, there are several ways to disable or ignore strict errors in very specific places, while still preserving them in the rest of the code. Below are several techniques to achieve it.","title":"Strict errors"},{"location":"visual_studio_code/#disable-type-checks-in-a-line","text":"You can disable the errors for a specific line using a comment of: # type: ignore or (to be specific to pylance/pyright): # pyright: ignore ( pyright is the language server used by Pylance.). coming back to the example with age='23' , it would be: from pydantic import BaseModel class Knight ( BaseModel ): title : str age : int color : str = 'blue' lancelot = Knight ( title = 'Sir Lancelot' , age = '23' ) # pyright: ignore that way Pylance and mypy will ignore errors in that line. Pros : it's a simple change in that line to remove errors there. Cons : any other error in that line will also be omitted, including type checks, misspelled arguments, required arguments not provided, etc.","title":"Disable type checks in a line"},{"location":"visual_studio_code/#override-the-type-of-a-variable","text":"You can also create a variable with the value you want to use and declare it's type explicitly with Any . from typing import Any from pydantic import BaseModel class Knight ( BaseModel ): title : str age : int color : str = 'blue' age_str : Any = '23' lancelot = Knight ( title = 'Sir Lancelot' , age = age_str ) that way Pylance and mypy will interpret the variable age_str as if they didn't know its type, instead of knowing it has a type of str when an int was expected (and then showing the corresponding error). Pros : errors will be ignored only for a specific value, and you will still see any additional errors for the other arguments. Cons : it requires importing Any and a new variable in a new line for each argument that needs ignoring errors.","title":"Override the type of a variable"},{"location":"visual_studio_code/#override-the-type-of-a-value-with-cast","text":"The same idea from the previous example can be put on the same line with the help of cast() . This way, the type declaration of the value is overriden inline, without requiring another variable. from typing import Any , cast from pydantic import BaseModel class Knight ( BaseModel ): title : str age : int color : str = 'blue' lancelot = Knight ( title = 'Sir Lancelot' , age = cast ( Any , '23' )) cast(Any, '23') doesn't affect the value, it's still just '23' , but now Pylance and mypy will assume it is of type Any , which means, they will act as if they didn't know the type of the value. So, this is the equivalent of the previous example, without the additional variable. Pros : errors will be ignored only for a specific value, and you will still see any additional errors for the other arguments. There's no need for additional variables. Cons : it requires importing Any and cast , and if you are not used to using cast() , it could seem strange at first.","title":"Override the type of a value with cast"},{"location":"visual_studio_code/#config-in-class-arguments","text":"pydantic has a rich set of Model Configurations available. These configurations can be set in an internal class Config on each model: from pydantic import BaseModel class Knight ( BaseModel ): title : str age : int color : str = 'blue' class Config : frozen = True or passed as keyword arguments when defining the model class: from pydantic import BaseModel class Knight ( BaseModel , frozen = True ): title : str age : int color : str = 'blue' The specific configuration frozen (in beta) has a special meaning. It prevents other code from changing a model instance once it's created, keeping it \"frozen\" . When using the second version to declare frozen=True (with keyword arguments in the class definition), Pylance can use it to help you check in your code and detect errors when something is trying to set values in a model that is \"frozen\".","title":"Config in class arguments"},{"location":"visual_studio_code/#adding-a-default-with-field","text":"Pylance/pyright requires default to be a keyword argument to Field in order to infer that the field is optional. from pydantic import BaseModel , Field class Knight ( BaseModel ): title : str = Field ( default = 'Sir Lancelot' ) # this is okay age : int = Field ( 23 ) # this works fine at runtime but will case an error for pyright lance = Knight () # error: Argument missing for parameter \"age\" This is a limitation of dataclass transforms and cannot be fixed in pydantic.","title":"Adding a default with Field"},{"location":"visual_studio_code/#technical-details","text":"Warning As a pydantic user, you don't need the details below. Feel free to skip the rest of this section. These details are only useful for other library authors, etc. This additional editor support works by implementing the proposed draft standard for Dataclass Transform . The proposed draft standard is written by Eric Traut, from the Microsoft team, the same author of the open source package Pyright (used by Pylance to provide Python support in VS Code). The intention of the standard is to provide a way for libraries like pydantic and others to tell editors and tools that they (the editors) should treat these libraries (e.g. pydantic ) as if they were dataclasses , providing autocompletion, type checks, etc. The draft standard also includes an Alternate Form for early adopters, like pydantic , to add support for it right away, even before the new draft standard is finished and approved. This new draft standard, with the Alternate Form, is already supported by Pyright, so it can be used via Pylance in VS Code. As it is being proposed as an official standard for Python, other editors can also easily add support for it. And authors of other libraries similar to pydantic can also easily adopt the standard right away (using the \"Alternate Form\") and get the benefits of these additional editor features.","title":"Technical Details"},{"location":"blog/pydantic-v2/","text":"Pydantic V2 Plan \u00b6 Samuel Colvin \u2022 \u2022 \u2022 Jul 10, 2022 \u2022 25 min read Updated late 10 Jul 2022, see pydantic#4226 . I've spoken to quite a few people about pydantic V2, and mention it in passing even more. I owe people a proper explanation of the plan for V2: What we will add What we will remove What we will change How I'm intending to go about completing it and getting it released Some idea of timeframe Here goes... Enormous thanks to Eric Jolibois , Laurence Watson , Sebasti\u00e1n Ram\u00edrez , Adrian Garcia Badaracco , Tom Hamilton Stubber , Zac Hatfield-Dodds , Tom & Hasan Ramezani for reviewing this blog post, putting up with (and correcting) my horrible typos and making great suggestions that have made this post and Pydantic V2 materially better. Plan & Timeframe \u00b6 I'm currently taking a kind of sabbatical after leaving my last job to get pydantic V2 released. Why? I ask myself that question quite often. I'm very proud of how much pydantic is used, but I'm less proud of its internals. Since it's something people seem to care about and use quite a lot (26m downloads a month, used by 72k public repos, 10k stars). I want it to be as good as possible. While I'm on the subject of why, how and my odd sabbatical: if you work for a large company who use pydantic a lot, you might encourage the company to sponsor me a meaningful amount , like Salesforce did (if your organisation is not open to donations, I can also offer consulting services). This is not charity, recruitment or marketing - the argument should be about how much the company will save if pydantic is 10x faster, more stable and more powerful - it would be worth paying me 10% of that to make it happen. Before pydantic V2 can be released, we need to release pydantic V1.10 - there are lots of changes in the main branch of pydantic contributed by the community, it's only fair to provide a release including those changes, many of them will remain unchanged for V2, the rest will act as a requirement to make sure pydantic V2 includes the capabilities they implemented. The basic road map for me is as follows: Implement a few more features in pydantic-core, and release a first version, see below Work on getting pydantic V1.10 out - basically merge all open PRs that are finished Release pydantic V1.10 Delete all stale PRs which didn't make it into V1.10, apologise profusely to their authors who put their valuable time into pydantic only to have their PRs closed (and explain when and how they can rebase and recreate the PR) Rename master to main , seems like a good time to do this Change the main branch of pydantic to target V2 Start tearing pydantic code apart and see how many existing tests can be made to pass Rinse, repeat Release pydantic V2 Plan is to have all this done by the end of October, definitely by the end of the year. Breaking Changes & Compatibility \u00b6 While we'll do our best to avoid breaking changes, some things will break. As per the greatest pun in modern TV history . You can't make a Tomelette without breaking some Greggs. Where possible, if breaking changes are unavoidable, we'll try to provide warnings or errors to make sure those changes are obvious to developers. Motivation & pydantic-core \u00b6 Since pydantic's initial release, with the help of wonderful contributors Eric Jolibois , Sebasti\u00e1n Ram\u00edrez , David Montague and many others, the package and its usage have grown enormously. The core logic however has remained mostly unchanged since the initial experiment. It's old, it smells, it needs to be rebuilt. The release of version 2 is an opportunity to rebuild pydantic and correct many things that don't make sense - to make pydantic amazing . The core validation logic of pydantic V2 will be performed by a separate package pydantic-core which I've been building over the last few months. pydantic-core is written in Rust using the excellent pyo3 library which provides rust bindings for python. The motivation for building pydantic-core in Rust is as follows: Performance , see below Recursion and code separation - with no stack and little-to-no overhead for extra function calls, Rust allows pydantic-core to be implemented as a tree of small validators which call each other, making code easier to understand and extend without harming performance Safety and complexity - pydantic-core is a fairly complex piece of code which has to draw distinctions between many different errors, Rust is great in situations like this, it should minimise bugs ( ) and allow the codebase to be extended for a long time to come Note The python interface to pydantic shouldn't change as a result of using pydantic-core, instead pydantic will use type annotations to build a schema for pydantic-core to use. pydantic-core is usable now, albeit with an unintuitive API, if you're interested, please give it a try. pydantic-core provides validators for common data types, see a list here . Other, less commonly used data types will be supported via validator functions implemented in pydantic, in Python. See pydantic-core#153 for a summary of what needs to be completed before its first release. Headlines \u00b6 Here are some of the biggest changes expected in V2. Performance \u00b6 As a result of the move to Rust for the validation logic (and significant improvements in how validation objects are structured) pydantic V2 will be significantly faster than pydantic V1. Looking at the pydantic-core benchmarks today, pydantic V2 is between 4x and 50x faster than pydantic V1.9.1. In general, pydantic V2 is about 17x faster than V1 when validating a model containing a range of common fields. Strict Mode \u00b6 People have long complained about pydantic for coercing data instead of throwing an error. E.g. input to an int field could be 123 or the string \"123\" which would be converted to 123 While this is very useful in many scenarios (think: URL parameters, environment variables, user input), there are some situations where it's not desirable. pydantic-core comes with \"strict mode\" built in. With this, only the exact data type is allowed, e.g. passing \"123\" to an int field would result in a validation error. This will allow pydantic V2 to offer a strict switch which can be set on either a model or a field. Formalised Conversion Table \u00b6 As well as complaints about coercion, another legitimate complaint was inconsistency around data conversion. In pydantic V2, the following principle will govern when data should be converted in \"lax mode\" ( strict=False ): If the input data has a SINGLE and INTUITIVE representation, in the field's type, AND no data is lost during the conversion, then the data will be converted; otherwise a validation error is raised. There is one exception to this rule: string fields - virtually all data has an intuitive representation as a string (e.g. repr() and str() ), therefore a custom rule is required: only str , bytes and bytearray are valid as inputs to string fields. Some examples of what that means in practice: Field Type Input Single & Intuitive R. All Data Preserved Result int \"123\" Convert int 123.0 Convert int 123.1 Error date \"2020-01-01\" Convert date \"2020-01-01T00:00:00\" Convert date \"2020-01-01T12:00:00\" Error int b\"1\" Error (For the last case converting bytes to an int could reasonably mean int(bytes_data.decode()) or int.from_bytes(b'1', 'big/little') , hence an error) In addition to the general rule, we'll provide a conversion table which defines exactly what data will be allowed to which field types. See the table below for a start on this. Built in JSON support \u00b6 pydantic-core can parse JSON directly into a model or output type, this both improves performance and avoids issue with strictness - e.g. if you have a strict model with a datetime field, the input must be a datetime object, but clearly that makes no sense when parsing JSON which has no datatime type. Same with bytes and many other types. Pydantic V2 will therefore allow some conversion when validating JSON directly, even in strict mode (e.g. ISO8601 string -> datetime , str -> bytes ) even though this would not be allowed when validating a python object. In future direct validation of JSON will also allow: parsing in a separate thread while starting validation in the main thread line numbers from JSON to be included in the validation errors (These features will not be included in V2, but instead will hopefully be added later.) Note Pydantic has always had special support for JSON, that is not going to change. While in theory other formats could be specifically supported, the overheads and development time are significant and I don't think there's another format that's used widely enough to be worth specific logic. Other formats can be parsed to python then validated, similarly when serialising, data can be exported to a python object, then serialised, see below . Validation without a Model \u00b6 In pydantic V1 the core of all validation was a pydantic model, this led to a significant performance penalty and extra complexity when the output data type was not a model. pydantic-core operates on a tree of validators with no \"model\" type required at the base of that tree. It can therefore validate a single string or datetime value, a TypedDict or a Model equally easily. This feature will provide significant addition performance improvements in scenarios like: Adding validation to dataclasses Validating URL arguments, query strings, headers, etc. in FastAPI Adding validation to TypedDict Function argument validation Adding validation to your custom classes, decorators... In effect - anywhere where you don't care about a traditional model class instance. We'll need to add standalone methods for generating JSON Schema and dumping these objects to JSON, etc. Required vs. Nullable Cleanup \u00b6 Pydantic previously had a somewhat confused idea about \"required\" vs. \"nullable\". This mostly resulted from my misgivings about marking a field as Optional[int] but requiring a value to be provided but allowing it to be None - I didn't like using the word \"optional\" in relation to a field which was not optional. In pydantic V2, pydantic will move to match dataclasses, thus: Required vs. Nullable from pydantic import BaseModel class Foo ( BaseModel ): f1 : str # required, cannot be None f2 : str | None # required, can be None - same as Optional[str] / Union[str, None] f3 : str | None = None # not required, can be None f4 : str = 'Foobar' # not required, but cannot be None Validator Function Improvements \u00b6 This is one of the changes in pydantic V2 that I'm most excited about, I've been talking about something like this for a long time, see pydantic#1984 , but couldn't find a way to do this until now. Fields which use a function for validation can be any of the following types: function before mode - where the function is called before the inner validator is called function after mode - where the function is called after the inner validator is called plain mode - where there's no inner validator wrap mode - where the function takes a reference to a function which calls the inner validator, and can therefore modify the input before inner validation, modify the output after inner validation, conditionally not call the inner validator or catch errors from the inner validator and return a default value, or change the error An example how a wrap validator might look: Wrap mode validator function from datetime import datetime from pydantic import BaseModel , ValidationError , validator class MyModel ( BaseModel ): timestamp : datetime @validator ( 'timestamp' , mode = 'wrap' ) def validate_timestamp ( cls , v , handler ): if v == 'now' : # we don't want to bother with further validation, # just return the new value return datetime . now () try : return handler ( v ) except ValidationError : # validation failed, in this case we want to # return a default value return datetime ( 2000 , 1 , 1 ) As well as being powerful, this provides a great \"escape hatch\" when pydantic validation doesn't do what you need. More powerful alias(es) \u00b6 pydantic-core can support alias \"paths\" as well as simple string aliases to flatten data as it's validated. Best demonstrated with an example: Alias paths from pydantic import BaseModel , Field class Foo ( BaseModel ): bar : str = Field ( aliases = [[ 'baz' , 2 , 'qux' ]]) data = { 'baz' : [ { 'qux' : 'a' }, { 'qux' : 'b' }, { 'qux' : 'c' }, { 'qux' : 'd' }, ] } foo = Foo ( ** data ) assert foo . bar == 'c' aliases is a list of lists because multiple paths can be provided, if so they're tried in turn until a value is found. Tagged unions will use the same logic as aliases meaning nested attributes can be used to select a schema to validate against. Improvements to Dumping/Serialization/Export \u00b6 (I haven't worked on this yet, so these ideas are only provisional) There has long been a debate about how to handle converting data when extracting it from a model. One of the features people have long requested is the ability to convert data to JSON compliant types while converting a model to a dict. My plan is to move data export into pydantic-core, with that, one implementation can support all export modes without compromising (and hopefully significantly improving) performance. I see four different export/serialisation scenarios: Extracting the field values of a model with no conversion, effectively model.__dict__ but with the current filtering logic provided by .dict() Extracting the field values of a model recursively (effectively what .dict() does now) - sub-models are converted to dicts, but other fields remain unchanged. Extracting data and converting at the same time (e.g. to JSON compliant types) Serialising data straight to JSON I think all 4 modes can be supported in a single implementation, with a kind of \"3.5\" mode where a python function is used to convert the data as the user wishes. The current include and exclude logic is extremely complicated, but hopefully it won't be too hard to translate it to Rust. We should also add support for validate_alias and dump_alias as well as the standard alias to allow for customising field keys. Validation Context \u00b6 Pydantic V2 will add a new optional context argument to model_validate and model_validate_json which will allow you to pass information not available when creating a model to validators. See pydantic#1549 for motivation. Here's an example of context might be used: Context during Validation from pydantic import BaseModel , EmailStr , validator class User ( BaseModel ): email : EmailStr home_country : str @validator ( 'home_country' ) def check_home_country ( cls , v , context ): if v not in context [ 'countries' ]: raise ValueError ( 'invalid country choice' ) return v async def add_user ( post_data : bytes ): countries = set ( await db_connection . fetch_all ( 'select code from country' )) user = User . model_validate_json ( post_data , context = { 'countries' : countries }) ... Note We (actually mostly Sebasti\u00e1n ) will have to make some changes to FastAPI to fully leverage context as we'd need some kind of dependency injection to build context before validation so models can still be passed as arguments to views. I'm sure he'll be game. Warning Although this will make it slightly easier to run synchronous IO (HTTP requests, DB. queries, etc.) from within validators, I strongly advise you keep IO separate from validation - do it before and use context, do it afterwards, avoid where possible making queries inside validation. Model Namespace Cleanup \u00b6 For years I've wanted to clean up the model namespace, see pydantic#1001 . This would avoid confusing gotchas when field names clash with methods on a model, it would also make it safer to add more methods to a model without risking new clashes. After much deliberation (and even giving a lightning talk at the python language submit about alternatives, see this discussion ). I've decided to go with the simplest and clearest approach, at the expense of a bit more typing: All methods on models will start with model_ , fields' names will not be allowed to start with \"model\" (aliases can be used if required). This will mean BaseModel will have roughly the following signature. New BaseModel methods class BaseModel : model_fields : List [ FieldInfo ] \"\"\"previously `__fields__`, although the format will change a lot\"\"\" @classmethod def model_validate ( cls , data : Any , * , context = None ) -> Self : # (1) \"\"\" previously `parse_obj()`, validate data \"\"\" @classmethod def model_validate_json ( cls , data : str | bytes | bytearray , * , context = None ) -> Self : \"\"\" previously `parse_raw(..., content_type='application/json')` validate data from JSON \"\"\" @classmethod def model_is_instance ( cls , data : Any , * , context = None ) -> bool : # (2) \"\"\" new, check if data is value for the model \"\"\" @classmethod def model_is_instance_json ( cls , data : str | bytes | bytearray , * , context = None ) -> bool : \"\"\" Same as `model_is_instance`, but from JSON \"\"\" def model_dump ( self , include : ... = None , exclude : ... = None , by_alias : bool = False , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , mode : Literal [ 'unchanged' , 'dicts' , 'json-compliant' ] = 'unchanged' , converter : Callable [[ Any ], Any ] | None = None ) -> Any : \"\"\" previously `dict()`, as before with new `mode` argument \"\"\" def model_dump_json ( self , ... ) -> str : \"\"\" previously `json()`, arguments as above effectively equivalent to `json.dump(self.model_dump(..., mode='json'))`, but more performant \"\"\" def model_json_schema ( self , ... ) -> dict [ str , Any ]: \"\"\" previously `schema()`, arguments roughly as before JSON schema as a dict \"\"\" def model_update_forward_refs ( self ) -> None : \"\"\" previously `update_forward_refs()`, update forward references \"\"\" @classmethod def model_construct ( self , _fields_set : set [ str ] | None = None , ** values : Any ) -> Self : \"\"\" previously `construct()`, arguments roughly as before construct a model with no validation \"\"\" @classmethod def model_customize_schema ( cls , schema : dict [ str , Any ]) -> dict [ str , Any ]: \"\"\" new, way to customize validation, e.g. if you wanted to alter how the model validates certain types, or add validation for a specific type without custom types or decorated validators \"\"\" class ModelConfig : \"\"\" previously `Config`, configuration class for models \"\"\" see Validation Context for more information on context see is_instance checks The following methods will be removed: .parse_file() - was a mistake, should never have been in pydantic .parse_raw() - partially replaced by .model_validate_json() , the other functionality was a mistake .from_orm() - the functionality has been moved to config, see other improvements below .schema_json() - mostly since it causes confusion between pydantic validation schema and JSON schema, and can be replaced with just json.dumps(m.model_json_schema()) .copy() instead we'll implement __copy__ and let people use the copy module (this removes some functionality) from copy() but there are bugs and ambiguities with the functionality anyway Strict API & API documentation \u00b6 When preparing for pydantic V2, we'll make a strict distinction between the public API and private functions & classes. Private objects will be clearly identified as private via a _internal sub package to discourage use. The public API will have API documentation. I've recently been working with the wonderful mkdocstrings package for both dirty-equals and watchfiles documentation. I intend to use mkdocstrings to generate complete API documentation for V2. This wouldn't replace the current example-based somewhat informal documentation style but instead will augment it. Error descriptions \u00b6 The way line errors (the individual errors within a ValidationError ) are built has become much more sophisticated in pydantic-core. There's a well-defined set of error codes and messages . More will be added when other types are validated via pure python validators in pydantic. I would like to add a dedicated section to the documentation with extra information for each type of error. This would be another key in a line error: documentation , which would link to the appropriate section in the docs. Thus, errors might look like: Line Errors Example [ { 'kind' : 'greater_than_equal' , 'loc' : [ 'age' ], 'message' : 'Value must be greater than or equal to 18' , 'input_value' : 11 , 'context' : { 'ge' : 18 }, 'documentation' : 'https://pydantic.dev/errors/#greater_than_equal' , }, { 'kind' : 'bool_parsing' , 'loc' : [ 'is_developer' ], 'message' : 'Value must be a valid boolean, unable to interpret input' , 'input_value' : 'foobar' , 'documentation' : 'https://pydantic.dev/errors/#bool_parsing' , }, ] I own the pydantic.dev domain and will use it for at least these errors so that even if the docs URL changes, the error will still link to the correct documentation. If developers don't want to show these errors to users, they can always process the errors list and filter out items from each error they don't need or want. No pure python implementation \u00b6 Since pydantic-core is written in Rust, and I have absolutely no intention of rewriting it in python, pydantic V2 will only work where a binary package can be installed. pydantic-core will provide binaries in PyPI for (at least): Linux : x86_64 , aarch64 , i686 , armv7l , musl-x86_64 & musl-aarch64 MacOS : x86_64 & arm64 (except python 3.7) Windows : amd64 & win32 Web Assembly : wasm32 (pydantic-core is already compiled for wasm32 using emscripten and unit tests pass, except where cpython itself has problems ) Binaries for pypy are a work in progress and will be added if possible, see pydantic-core#154 . Other binaries can be added provided they can be (cross-)compiled on github actions. If no binary is available from PyPI, pydantic-core can be compiled from source if Rust stable is available. The only place where I know this will cause problems is Raspberry Pi, which is a mess when it comes to packages written in Rust for Python. Effectively, until that's fixed you'll likely have to install pydantic with pip install -i https://pypi.org/simple/ pydantic . Pydantic becomes a pure python package \u00b6 Pydantic V1.X is a pure python code base but is compiled with cython to provide some performance improvements. Since the \"hot\" code is moved to pydantic-core, pydantic itself can go back to being a pure python package. This should significantly reduce the size of the pydantic package and make unit tests of pydantic much faster. In addition: some constraints on pydantic code can be removed once it no-longer has to be compilable with cython debugging will be easier as you'll be able to drop straight into the pydantic codebase as you can with other, pure python packages Some pieces of edge logic could get a little slower as they're no longer compiled. is_instance like checks \u00b6 Strict mode also means it makes sense to provide an is_instance method on models which effectively run validation then throws away the result while avoiding the (admittedly small) overhead of creating and raising an error or returning the validation result. To be clear, this isn't a real isinstance call, rather it is equivalent to is_instance class BaseModel : ... @classmethod def model_is_instance ( cls , data : Any ) -> bool : try : cls ( ** data ) except ValidationError : return False else : return True I'm dropping the word \"parse\" and just using \"validate\" \u00b6 Partly due to the issues with the lack of strict mode, I've gone back and forth between using the terms \"parse\" and \"validate\" for what pydantic does. While pydantic is not simply a validation library (and I'm sure some would argue validation is not strictly what it does), most people use the word \"validation\" . It's time to stop fighting that, and use consistent names. The word \"parse\" will no longer be used except when talking about JSON parsing, see model methods above. Changes to custom field types \u00b6 Since the core structure of validators has changed from \"a list of validators to call one after another\" to \"a tree of validators which call each other\", the __get_validators__ way of defining custom field types no longer makes sense. Instead, we'll look for the attribute __pydantic_validation_schema__ which must be a pydantic-core compliant schema for validating data to this field type (the function item can be a string, if so a function of that name will be taken from the class, see 'validate' below). Here's an example of how a custom field type could be defined: New custom field types from pydantic import ValidationSchema class Foobar : def __init__ ( self , value : str ): self . value = value __pydantic_validation_schema__ : ValidationSchema = { 'type' : 'function' , 'mode' : 'after' , 'function' : 'validate' , 'schema' : { 'type' : 'str' } } @classmethod def validate ( cls , value ): if 'foobar' in value : return Foobar ( value ) else : raise ValueError ( 'expected foobar' ) What's going on here: __pydantic_validation_schema__ defines a schema which effectively says: Validate input data as a string, then call the validate function with that string, use the returned value as the final result of validation. ValidationSchema is just an alias to pydantic_core.Schema which is a type defining the schema for validation schemas. Note pydantic-core schema has full type definitions although since the type is recursive, mypy can't provide static type analysis, pyright however can. We can probably provide one or more helper functions to make __pydantic_validation_schema__ easier to generate. Other Improvements \u00b6 Some other things which will also change, IMHO for the better: Recursive models with cyclic references - although recursive models were supported by pydantic V1, data with cyclic references caused recursion errors, in pydantic-core cyclic references are correctly detected and a validation error is raised The reason I've been so keen to get pydantic-core to compile and run with wasm is that I want all examples in the docs of pydantic V2 to be editable and runnable in the browser Full support for TypedDict , including total=False - e.g. omitted keys, providing validation schema to a TypedDict field/item will use Annotated , e.g. Annotated[str, Field(strict=True)] from_orm has become from_attributes and is now defined at schema generation time (either via model config or field config) input_value has been added to each line error in a ValidationError , making errors easier to understand, and more comprehensive details of errors to be provided to end users, pydantic#784 on_error logic in a schema which allows either a default value to be used in the event of an error, or that value to be omitted (in the case of a total=False TypedDict ), pydantic-core#151 datetime , date , time & timedelta validation is improved, see the speedate Rust library I built specifically for this purpose for more details Powerful \"priority\" system for optionally merging or overriding config in sub-models for nested schemas Pydantic will support annotated-types , so you can do stuff like Annotated[set[int], Len(0, 10)] or Name = Annotated[str, Len(1, 1024)] A single decorator for general usage - we should add a validate decorator which can be used: on functions (replacing validate_arguments ) on dataclasses, pydantic.dataclasses.dataclass will become an alias of this on TypedDict s On any supported type, e.g. Union[...] , Dict[str, Thing] On Custom field types - e.g. anything with a __pydantic_schema__ attribute Easier validation error creation, I've often found myself wanting to raise ValidationError s outside models, particularly in FastAPI ( here is one method I've used), we should provide utilities to generate these errors Improve the performance of __eq__ on models Computed fields, these having been an idea for a long time in pydantic - we should get them right Model validation that avoids instances of subclasses leaking data (particularly important for FastAPI), see pydantic-core#155 We'll now follow semvar properly and avoid breaking changes between minor versions, as a result, major versions will become more common Improve generics to use M(Basemodel, Generic[T]) instead of M(GenericModel, Generic[T]) - e.g. GenericModel can be removed; this results from no-longer needing to compile pydantic code with cython Removed Features & Limitations \u00b6 The emoji here is just for variation, I'm not frowning about any of this, these changes are either good IMHO (will make pydantic cleaner, easier to learn and easier to maintain) or irrelevant to 99.9+% of users. __root__ custom root models are no longer necessary since validation on any supported data type is allowed without a model .parse_file() and .parse_raw() , partially replaced with .model_validate_json() , see model methods .schema_json() & .copy() , see model methods TypeError are no longer considered as validation errors, but rather as internal errors, this is to better catch errors in argument names in function validators. Subclasses of builtin types like str , bytes and int are coerced to their parent builtin type, this is a limitation of how pydantic-core converts these types to Rust types during validation, if you have a specific need to keep the type, you can use wrap validators or custom type validation as described above integers are represented in rust code as i64 , meaning if you want to use ints where abs(v) > 2^63 \u2212 1 (9,223,372,036,854,775,807), you'll need to use a wrap validator and your own logic Settings Management ??? - I definitely don't want to remove the functionality, but it's something of a historical curiosity that it lives within pydantic, perhaps it should move to a separate package, perhaps installable alongside pydantic with pip install pydantic[settings] ? The following Config properties will be removed: fields - it's very old (it pre-dates Field ), can be removed allow_mutation will be removed, instead frozen will be used error_msg_templates , it's not properly documented anyway, error messages can be customized with external logic if required getter_dict - pydantic-core has hardcoded from_attributes logic json_loads - again this is hard coded in pydantic-core json_dumps - possibly json_encoders - see the export \"mode\" discussion above underscore_attrs_are_private we should just choose a sensible default smart_union - all unions are now \"smart\" dict(model) functionality should be removed, there's a much clearer distinction now that in 2017 when I implemented this between a model and a dict Features Remaining \u00b6 The following features will remain (mostly) unchanged: JSONSchema, internally this will need to change a lot, but hopefully the external interface will remain unchanged dataclass support, again internals might change, but not the external interface validate_arguments , might be renamed, but otherwise remain hypothesis plugin, might be able to improve this as part of the general cleanup Questions \u00b6 I hope the explanation above is useful. I'm sure people will have questions and feedback; I'm aware I've skipped over some features with limited detail (this post is already fairly long ). To allow feedback without being overwhelmed, I've created a \"Pydantic V2\" category for discussions on github - please feel free to create a discussion if you have any questions or suggestions. We will endeavour to read and respond to everyone. Implementation Details \u00b6 (This is yet to be built, so these are nascent ideas which might change) At the center of pydantic v2 will be a PydanticValidator class which looks roughly like this (note: this is just pseudo-code, it's not even valid python and is only supposed to be used to demonstrate the idea): PydanticValidator # type identifying data which has been validated, # as per pydantic-core, this can include \"fields_set\" data ValidData = ... # any type we can perform validation for AnyOutputType = ... class PydanticValidator : def __init__ ( self , output_type : AnyOutputType , config : Config ): ... def validate ( self , input_data : Any ) -> ValidData : ... def validate_json ( self , input_data : str | bytes | bytearray ) -> ValidData : ... def is_instance ( self , input_data : Any ) -> bool : ... def is_instance_json ( self , input_data : str | bytes | bytearray ) -> bool : ... def json_schema ( self ) -> dict : ... def dump ( self , data : ValidData , include : ... = None , exclude : ... = None , by_alias : bool = False , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , mode : Literal [ 'unchanged' , 'dicts' , 'json-compliant' ] = 'unchanged' , converter : Callable [[ Any ], Any ] | None = None ) -> Any : ... def dump_json ( self , ... ) -> str : ... This could be used directly, but more commonly will be used by the following: BaseModel the validate decorator described above pydantic.dataclasses.dataclass (which might be an alias of validate ) generics The aim will be to get pydantic V2 to a place were the vast majority of tests continue to pass unchanged. Thereby guaranteeing (as much as possible) that the external interface to pydantic and its behaviour are unchanged. Conversion Table \u00b6 The table below provisionally defines what input value types are allowed to which field types. An updated and complete version of this table will be included in the docs for V2. Note Some type conversion shown here is a significant departure from existing behavior, we may have to provide a config flag for backwards compatibility for a few of them, however pydantic V2 cannot be entirely backward compatible, see pydantic-core#152 . Field Type Input Mode Input Source Conditions str str both python, JSON - str bytes lax python assumes UTF-8, error on unicode decoding error str bytearray lax python assumes UTF-8, error on unicode decoding error bytes bytes both python - bytes str both JSON - bytes str lax python - bytes bytearray lax python - int int strict python, JSON max abs value 2^64 - i64 is used internally, bool explicitly forbidden int int lax python, JSON i64 int float lax python, JSON i64 , must be exact int, e.g. f % 1 == 0 , nan , inf raise errors int Decimal lax python, JSON i64 , must be exact int, e.g. f % 1 == 0 int bool lax python, JSON - int str lax python, JSON i64 , must be numeric only, e.g. [0-9]+ float float strict python, JSON bool explicitly forbidden float float lax python, JSON - float int lax python, JSON - float str lax python, JSON must match [0-9]+(\\.[0-9]+)? float Decimal lax python - float bool lax python, JSON - bool bool both python, JSON - bool int lax python, JSON allowed: 0, 1 bool float lax python, JSON allowed: 0, 1 bool Decimal lax python, JSON allowed: 0, 1 bool str lax python, JSON allowed: 'f', 'n', 'no', 'off', 'false', 't', 'y', 'on', 'yes', 'true' None None both python, JSON - date date both python - date datetime lax python must be exact date, eg. no H, M, S, f date str both JSON format YYYY-MM-DD date str lax python format YYYY-MM-DD date bytes lax python format YYYY-MM-DD (UTF-8) date int lax python, JSON interpreted as seconds or ms from epoch, see speedate , must be exact date date float lax python, JSON interpreted as seconds or ms from epoch, see speedate , must be exact date datetime datetime both python - datetime date lax python - datetime str both JSON format YYYY-MM-DDTHH:MM:SS.f etc. see speedate datetime str lax python format YYYY-MM-DDTHH:MM:SS.f etc. see speedate datetime bytes lax python format YYYY-MM-DDTHH:MM:SS.f etc. see speedate , (UTF-8) datetime int lax python, JSON interpreted as seconds or ms from epoch, see speedate datetime float lax python, JSON interpreted as seconds or ms from epoch, see speedate time time both python - time str both JSON format HH:MM:SS.FFFFFF etc. see speedate time str lax python format HH:MM:SS.FFFFFF etc. see speedate time bytes lax python format HH:MM:SS.FFFFFF etc. see speedate , (UTF-8) time int lax python, JSON interpreted as seconds, range 0 - 86399 time float lax python, JSON interpreted as seconds, range 0 - 86399.9* time Decimal lax python, JSON interpreted as seconds, range 0 - 86399.9* timedelta timedelta both python - timedelta str both JSON format ISO8601 etc. see speedate timedelta str lax python format ISO8601 etc. see speedate timedelta bytes lax python format ISO8601 etc. see speedate , (UTF-8) timedelta int lax python, JSON interpreted as seconds timedelta float lax python, JSON interpreted as seconds timedelta Decimal lax python, JSON interpreted as seconds dict dict both python - dict Object both JSON - dict mapping lax python must implement the mapping interface and have an items() method TypedDict dict both python - TypedDict Object both JSON - TypedDict Any both python builtins not allowed, uses getattr , requires from_attributes=True TypedDict mapping lax python must implement the mapping interface and have an items() method list list both python - list Array both JSON - list tuple lax python - list set lax python - list frozenset lax python - list dict_keys lax python - tuple tuple both python - tuple Array both JSON - tuple list lax python - tuple set lax python - tuple frozenset lax python - tuple dict_keys lax python - set set both python - set Array both JSON - set list lax python - set tuple lax python - set frozenset lax python - set dict_keys lax python - frozenset frozenset both python - frozenset Array both JSON - frozenset list lax python - frozenset tuple lax python - frozenset set lax python - frozenset dict_keys lax python - is_instance Any both python isinstance() check returns True is_instance - both JSON never valid callable Any both python callable() check returns True callable - both JSON never valid The ModelClass validator (use to create instances of a class) uses the TypedDict validator, then creates an instance with __dict__ and __fields_set__ set, so same rules apply as TypedDict .","title":"Pydantic V2 Plan"},{"location":"blog/pydantic-v2/#pydantic-v2-plan","text":"Samuel Colvin \u2022 \u2022 \u2022 Jul 10, 2022 \u2022 25 min read Updated late 10 Jul 2022, see pydantic#4226 . I've spoken to quite a few people about pydantic V2, and mention it in passing even more. I owe people a proper explanation of the plan for V2: What we will add What we will remove What we will change How I'm intending to go about completing it and getting it released Some idea of timeframe Here goes... Enormous thanks to Eric Jolibois , Laurence Watson , Sebasti\u00e1n Ram\u00edrez , Adrian Garcia Badaracco , Tom Hamilton Stubber , Zac Hatfield-Dodds , Tom & Hasan Ramezani for reviewing this blog post, putting up with (and correcting) my horrible typos and making great suggestions that have made this post and Pydantic V2 materially better.","title":"Pydantic V2 Plan"},{"location":"blog/pydantic-v2/#plan-timeframe","text":"I'm currently taking a kind of sabbatical after leaving my last job to get pydantic V2 released. Why? I ask myself that question quite often. I'm very proud of how much pydantic is used, but I'm less proud of its internals. Since it's something people seem to care about and use quite a lot (26m downloads a month, used by 72k public repos, 10k stars). I want it to be as good as possible. While I'm on the subject of why, how and my odd sabbatical: if you work for a large company who use pydantic a lot, you might encourage the company to sponsor me a meaningful amount , like Salesforce did (if your organisation is not open to donations, I can also offer consulting services). This is not charity, recruitment or marketing - the argument should be about how much the company will save if pydantic is 10x faster, more stable and more powerful - it would be worth paying me 10% of that to make it happen. Before pydantic V2 can be released, we need to release pydantic V1.10 - there are lots of changes in the main branch of pydantic contributed by the community, it's only fair to provide a release including those changes, many of them will remain unchanged for V2, the rest will act as a requirement to make sure pydantic V2 includes the capabilities they implemented. The basic road map for me is as follows: Implement a few more features in pydantic-core, and release a first version, see below Work on getting pydantic V1.10 out - basically merge all open PRs that are finished Release pydantic V1.10 Delete all stale PRs which didn't make it into V1.10, apologise profusely to their authors who put their valuable time into pydantic only to have their PRs closed (and explain when and how they can rebase and recreate the PR) Rename master to main , seems like a good time to do this Change the main branch of pydantic to target V2 Start tearing pydantic code apart and see how many existing tests can be made to pass Rinse, repeat Release pydantic V2 Plan is to have all this done by the end of October, definitely by the end of the year.","title":"Plan &amp; Timeframe"},{"location":"blog/pydantic-v2/#breaking-changes-compatibility","text":"While we'll do our best to avoid breaking changes, some things will break. As per the greatest pun in modern TV history . You can't make a Tomelette without breaking some Greggs. Where possible, if breaking changes are unavoidable, we'll try to provide warnings or errors to make sure those changes are obvious to developers.","title":"Breaking Changes &amp; Compatibility"},{"location":"blog/pydantic-v2/#motivation-pydantic-core","text":"Since pydantic's initial release, with the help of wonderful contributors Eric Jolibois , Sebasti\u00e1n Ram\u00edrez , David Montague and many others, the package and its usage have grown enormously. The core logic however has remained mostly unchanged since the initial experiment. It's old, it smells, it needs to be rebuilt. The release of version 2 is an opportunity to rebuild pydantic and correct many things that don't make sense - to make pydantic amazing . The core validation logic of pydantic V2 will be performed by a separate package pydantic-core which I've been building over the last few months. pydantic-core is written in Rust using the excellent pyo3 library which provides rust bindings for python. The motivation for building pydantic-core in Rust is as follows: Performance , see below Recursion and code separation - with no stack and little-to-no overhead for extra function calls, Rust allows pydantic-core to be implemented as a tree of small validators which call each other, making code easier to understand and extend without harming performance Safety and complexity - pydantic-core is a fairly complex piece of code which has to draw distinctions between many different errors, Rust is great in situations like this, it should minimise bugs ( ) and allow the codebase to be extended for a long time to come Note The python interface to pydantic shouldn't change as a result of using pydantic-core, instead pydantic will use type annotations to build a schema for pydantic-core to use. pydantic-core is usable now, albeit with an unintuitive API, if you're interested, please give it a try. pydantic-core provides validators for common data types, see a list here . Other, less commonly used data types will be supported via validator functions implemented in pydantic, in Python. See pydantic-core#153 for a summary of what needs to be completed before its first release.","title":"Motivation &amp; pydantic-core"},{"location":"blog/pydantic-v2/#headlines","text":"Here are some of the biggest changes expected in V2.","title":"Headlines"},{"location":"blog/pydantic-v2/#performance","text":"As a result of the move to Rust for the validation logic (and significant improvements in how validation objects are structured) pydantic V2 will be significantly faster than pydantic V1. Looking at the pydantic-core benchmarks today, pydantic V2 is between 4x and 50x faster than pydantic V1.9.1. In general, pydantic V2 is about 17x faster than V1 when validating a model containing a range of common fields.","title":"Performance"},{"location":"blog/pydantic-v2/#strict-mode","text":"People have long complained about pydantic for coercing data instead of throwing an error. E.g. input to an int field could be 123 or the string \"123\" which would be converted to 123 While this is very useful in many scenarios (think: URL parameters, environment variables, user input), there are some situations where it's not desirable. pydantic-core comes with \"strict mode\" built in. With this, only the exact data type is allowed, e.g. passing \"123\" to an int field would result in a validation error. This will allow pydantic V2 to offer a strict switch which can be set on either a model or a field.","title":"Strict Mode"},{"location":"blog/pydantic-v2/#formalised-conversion-table","text":"As well as complaints about coercion, another legitimate complaint was inconsistency around data conversion. In pydantic V2, the following principle will govern when data should be converted in \"lax mode\" ( strict=False ): If the input data has a SINGLE and INTUITIVE representation, in the field's type, AND no data is lost during the conversion, then the data will be converted; otherwise a validation error is raised. There is one exception to this rule: string fields - virtually all data has an intuitive representation as a string (e.g. repr() and str() ), therefore a custom rule is required: only str , bytes and bytearray are valid as inputs to string fields. Some examples of what that means in practice: Field Type Input Single & Intuitive R. All Data Preserved Result int \"123\" Convert int 123.0 Convert int 123.1 Error date \"2020-01-01\" Convert date \"2020-01-01T00:00:00\" Convert date \"2020-01-01T12:00:00\" Error int b\"1\" Error (For the last case converting bytes to an int could reasonably mean int(bytes_data.decode()) or int.from_bytes(b'1', 'big/little') , hence an error) In addition to the general rule, we'll provide a conversion table which defines exactly what data will be allowed to which field types. See the table below for a start on this.","title":"Formalised Conversion Table"},{"location":"blog/pydantic-v2/#built-in-json-support","text":"pydantic-core can parse JSON directly into a model or output type, this both improves performance and avoids issue with strictness - e.g. if you have a strict model with a datetime field, the input must be a datetime object, but clearly that makes no sense when parsing JSON which has no datatime type. Same with bytes and many other types. Pydantic V2 will therefore allow some conversion when validating JSON directly, even in strict mode (e.g. ISO8601 string -> datetime , str -> bytes ) even though this would not be allowed when validating a python object. In future direct validation of JSON will also allow: parsing in a separate thread while starting validation in the main thread line numbers from JSON to be included in the validation errors (These features will not be included in V2, but instead will hopefully be added later.) Note Pydantic has always had special support for JSON, that is not going to change. While in theory other formats could be specifically supported, the overheads and development time are significant and I don't think there's another format that's used widely enough to be worth specific logic. Other formats can be parsed to python then validated, similarly when serialising, data can be exported to a python object, then serialised, see below .","title":"Built in JSON support"},{"location":"blog/pydantic-v2/#validation-without-a-model","text":"In pydantic V1 the core of all validation was a pydantic model, this led to a significant performance penalty and extra complexity when the output data type was not a model. pydantic-core operates on a tree of validators with no \"model\" type required at the base of that tree. It can therefore validate a single string or datetime value, a TypedDict or a Model equally easily. This feature will provide significant addition performance improvements in scenarios like: Adding validation to dataclasses Validating URL arguments, query strings, headers, etc. in FastAPI Adding validation to TypedDict Function argument validation Adding validation to your custom classes, decorators... In effect - anywhere where you don't care about a traditional model class instance. We'll need to add standalone methods for generating JSON Schema and dumping these objects to JSON, etc.","title":"Validation without a Model"},{"location":"blog/pydantic-v2/#required-vs-nullable-cleanup","text":"Pydantic previously had a somewhat confused idea about \"required\" vs. \"nullable\". This mostly resulted from my misgivings about marking a field as Optional[int] but requiring a value to be provided but allowing it to be None - I didn't like using the word \"optional\" in relation to a field which was not optional. In pydantic V2, pydantic will move to match dataclasses, thus: Required vs. Nullable from pydantic import BaseModel class Foo ( BaseModel ): f1 : str # required, cannot be None f2 : str | None # required, can be None - same as Optional[str] / Union[str, None] f3 : str | None = None # not required, can be None f4 : str = 'Foobar' # not required, but cannot be None","title":"Required vs. Nullable Cleanup"},{"location":"blog/pydantic-v2/#validator-function-improvements","text":"This is one of the changes in pydantic V2 that I'm most excited about, I've been talking about something like this for a long time, see pydantic#1984 , but couldn't find a way to do this until now. Fields which use a function for validation can be any of the following types: function before mode - where the function is called before the inner validator is called function after mode - where the function is called after the inner validator is called plain mode - where there's no inner validator wrap mode - where the function takes a reference to a function which calls the inner validator, and can therefore modify the input before inner validation, modify the output after inner validation, conditionally not call the inner validator or catch errors from the inner validator and return a default value, or change the error An example how a wrap validator might look: Wrap mode validator function from datetime import datetime from pydantic import BaseModel , ValidationError , validator class MyModel ( BaseModel ): timestamp : datetime @validator ( 'timestamp' , mode = 'wrap' ) def validate_timestamp ( cls , v , handler ): if v == 'now' : # we don't want to bother with further validation, # just return the new value return datetime . now () try : return handler ( v ) except ValidationError : # validation failed, in this case we want to # return a default value return datetime ( 2000 , 1 , 1 ) As well as being powerful, this provides a great \"escape hatch\" when pydantic validation doesn't do what you need.","title":"Validator Function Improvements"},{"location":"blog/pydantic-v2/#more-powerful-aliases","text":"pydantic-core can support alias \"paths\" as well as simple string aliases to flatten data as it's validated. Best demonstrated with an example: Alias paths from pydantic import BaseModel , Field class Foo ( BaseModel ): bar : str = Field ( aliases = [[ 'baz' , 2 , 'qux' ]]) data = { 'baz' : [ { 'qux' : 'a' }, { 'qux' : 'b' }, { 'qux' : 'c' }, { 'qux' : 'd' }, ] } foo = Foo ( ** data ) assert foo . bar == 'c' aliases is a list of lists because multiple paths can be provided, if so they're tried in turn until a value is found. Tagged unions will use the same logic as aliases meaning nested attributes can be used to select a schema to validate against.","title":"More powerful alias(es)"},{"location":"blog/pydantic-v2/#improvements-to-dumpingserializationexport","text":"(I haven't worked on this yet, so these ideas are only provisional) There has long been a debate about how to handle converting data when extracting it from a model. One of the features people have long requested is the ability to convert data to JSON compliant types while converting a model to a dict. My plan is to move data export into pydantic-core, with that, one implementation can support all export modes without compromising (and hopefully significantly improving) performance. I see four different export/serialisation scenarios: Extracting the field values of a model with no conversion, effectively model.__dict__ but with the current filtering logic provided by .dict() Extracting the field values of a model recursively (effectively what .dict() does now) - sub-models are converted to dicts, but other fields remain unchanged. Extracting data and converting at the same time (e.g. to JSON compliant types) Serialising data straight to JSON I think all 4 modes can be supported in a single implementation, with a kind of \"3.5\" mode where a python function is used to convert the data as the user wishes. The current include and exclude logic is extremely complicated, but hopefully it won't be too hard to translate it to Rust. We should also add support for validate_alias and dump_alias as well as the standard alias to allow for customising field keys.","title":"Improvements to Dumping/Serialization/Export"},{"location":"blog/pydantic-v2/#validation-context","text":"Pydantic V2 will add a new optional context argument to model_validate and model_validate_json which will allow you to pass information not available when creating a model to validators. See pydantic#1549 for motivation. Here's an example of context might be used: Context during Validation from pydantic import BaseModel , EmailStr , validator class User ( BaseModel ): email : EmailStr home_country : str @validator ( 'home_country' ) def check_home_country ( cls , v , context ): if v not in context [ 'countries' ]: raise ValueError ( 'invalid country choice' ) return v async def add_user ( post_data : bytes ): countries = set ( await db_connection . fetch_all ( 'select code from country' )) user = User . model_validate_json ( post_data , context = { 'countries' : countries }) ... Note We (actually mostly Sebasti\u00e1n ) will have to make some changes to FastAPI to fully leverage context as we'd need some kind of dependency injection to build context before validation so models can still be passed as arguments to views. I'm sure he'll be game. Warning Although this will make it slightly easier to run synchronous IO (HTTP requests, DB. queries, etc.) from within validators, I strongly advise you keep IO separate from validation - do it before and use context, do it afterwards, avoid where possible making queries inside validation.","title":"Validation Context"},{"location":"blog/pydantic-v2/#model-namespace-cleanup","text":"For years I've wanted to clean up the model namespace, see pydantic#1001 . This would avoid confusing gotchas when field names clash with methods on a model, it would also make it safer to add more methods to a model without risking new clashes. After much deliberation (and even giving a lightning talk at the python language submit about alternatives, see this discussion ). I've decided to go with the simplest and clearest approach, at the expense of a bit more typing: All methods on models will start with model_ , fields' names will not be allowed to start with \"model\" (aliases can be used if required). This will mean BaseModel will have roughly the following signature. New BaseModel methods class BaseModel : model_fields : List [ FieldInfo ] \"\"\"previously `__fields__`, although the format will change a lot\"\"\" @classmethod def model_validate ( cls , data : Any , * , context = None ) -> Self : # (1) \"\"\" previously `parse_obj()`, validate data \"\"\" @classmethod def model_validate_json ( cls , data : str | bytes | bytearray , * , context = None ) -> Self : \"\"\" previously `parse_raw(..., content_type='application/json')` validate data from JSON \"\"\" @classmethod def model_is_instance ( cls , data : Any , * , context = None ) -> bool : # (2) \"\"\" new, check if data is value for the model \"\"\" @classmethod def model_is_instance_json ( cls , data : str | bytes | bytearray , * , context = None ) -> bool : \"\"\" Same as `model_is_instance`, but from JSON \"\"\" def model_dump ( self , include : ... = None , exclude : ... = None , by_alias : bool = False , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , mode : Literal [ 'unchanged' , 'dicts' , 'json-compliant' ] = 'unchanged' , converter : Callable [[ Any ], Any ] | None = None ) -> Any : \"\"\" previously `dict()`, as before with new `mode` argument \"\"\" def model_dump_json ( self , ... ) -> str : \"\"\" previously `json()`, arguments as above effectively equivalent to `json.dump(self.model_dump(..., mode='json'))`, but more performant \"\"\" def model_json_schema ( self , ... ) -> dict [ str , Any ]: \"\"\" previously `schema()`, arguments roughly as before JSON schema as a dict \"\"\" def model_update_forward_refs ( self ) -> None : \"\"\" previously `update_forward_refs()`, update forward references \"\"\" @classmethod def model_construct ( self , _fields_set : set [ str ] | None = None , ** values : Any ) -> Self : \"\"\" previously `construct()`, arguments roughly as before construct a model with no validation \"\"\" @classmethod def model_customize_schema ( cls , schema : dict [ str , Any ]) -> dict [ str , Any ]: \"\"\" new, way to customize validation, e.g. if you wanted to alter how the model validates certain types, or add validation for a specific type without custom types or decorated validators \"\"\" class ModelConfig : \"\"\" previously `Config`, configuration class for models \"\"\" see Validation Context for more information on context see is_instance checks The following methods will be removed: .parse_file() - was a mistake, should never have been in pydantic .parse_raw() - partially replaced by .model_validate_json() , the other functionality was a mistake .from_orm() - the functionality has been moved to config, see other improvements below .schema_json() - mostly since it causes confusion between pydantic validation schema and JSON schema, and can be replaced with just json.dumps(m.model_json_schema()) .copy() instead we'll implement __copy__ and let people use the copy module (this removes some functionality) from copy() but there are bugs and ambiguities with the functionality anyway","title":"Model Namespace Cleanup"},{"location":"blog/pydantic-v2/#strict-api-api-documentation","text":"When preparing for pydantic V2, we'll make a strict distinction between the public API and private functions & classes. Private objects will be clearly identified as private via a _internal sub package to discourage use. The public API will have API documentation. I've recently been working with the wonderful mkdocstrings package for both dirty-equals and watchfiles documentation. I intend to use mkdocstrings to generate complete API documentation for V2. This wouldn't replace the current example-based somewhat informal documentation style but instead will augment it.","title":"Strict API &amp; API documentation"},{"location":"blog/pydantic-v2/#error-descriptions","text":"The way line errors (the individual errors within a ValidationError ) are built has become much more sophisticated in pydantic-core. There's a well-defined set of error codes and messages . More will be added when other types are validated via pure python validators in pydantic. I would like to add a dedicated section to the documentation with extra information for each type of error. This would be another key in a line error: documentation , which would link to the appropriate section in the docs. Thus, errors might look like: Line Errors Example [ { 'kind' : 'greater_than_equal' , 'loc' : [ 'age' ], 'message' : 'Value must be greater than or equal to 18' , 'input_value' : 11 , 'context' : { 'ge' : 18 }, 'documentation' : 'https://pydantic.dev/errors/#greater_than_equal' , }, { 'kind' : 'bool_parsing' , 'loc' : [ 'is_developer' ], 'message' : 'Value must be a valid boolean, unable to interpret input' , 'input_value' : 'foobar' , 'documentation' : 'https://pydantic.dev/errors/#bool_parsing' , }, ] I own the pydantic.dev domain and will use it for at least these errors so that even if the docs URL changes, the error will still link to the correct documentation. If developers don't want to show these errors to users, they can always process the errors list and filter out items from each error they don't need or want.","title":"Error descriptions"},{"location":"blog/pydantic-v2/#no-pure-python-implementation","text":"Since pydantic-core is written in Rust, and I have absolutely no intention of rewriting it in python, pydantic V2 will only work where a binary package can be installed. pydantic-core will provide binaries in PyPI for (at least): Linux : x86_64 , aarch64 , i686 , armv7l , musl-x86_64 & musl-aarch64 MacOS : x86_64 & arm64 (except python 3.7) Windows : amd64 & win32 Web Assembly : wasm32 (pydantic-core is already compiled for wasm32 using emscripten and unit tests pass, except where cpython itself has problems ) Binaries for pypy are a work in progress and will be added if possible, see pydantic-core#154 . Other binaries can be added provided they can be (cross-)compiled on github actions. If no binary is available from PyPI, pydantic-core can be compiled from source if Rust stable is available. The only place where I know this will cause problems is Raspberry Pi, which is a mess when it comes to packages written in Rust for Python. Effectively, until that's fixed you'll likely have to install pydantic with pip install -i https://pypi.org/simple/ pydantic .","title":"No pure python implementation"},{"location":"blog/pydantic-v2/#pydantic-becomes-a-pure-python-package","text":"Pydantic V1.X is a pure python code base but is compiled with cython to provide some performance improvements. Since the \"hot\" code is moved to pydantic-core, pydantic itself can go back to being a pure python package. This should significantly reduce the size of the pydantic package and make unit tests of pydantic much faster. In addition: some constraints on pydantic code can be removed once it no-longer has to be compilable with cython debugging will be easier as you'll be able to drop straight into the pydantic codebase as you can with other, pure python packages Some pieces of edge logic could get a little slower as they're no longer compiled.","title":"Pydantic becomes a pure python package"},{"location":"blog/pydantic-v2/#is_instance-like-checks","text":"Strict mode also means it makes sense to provide an is_instance method on models which effectively run validation then throws away the result while avoiding the (admittedly small) overhead of creating and raising an error or returning the validation result. To be clear, this isn't a real isinstance call, rather it is equivalent to is_instance class BaseModel : ... @classmethod def model_is_instance ( cls , data : Any ) -> bool : try : cls ( ** data ) except ValidationError : return False else : return True","title":"is_instance like checks"},{"location":"blog/pydantic-v2/#im-dropping-the-word-parse-and-just-using-validate","text":"Partly due to the issues with the lack of strict mode, I've gone back and forth between using the terms \"parse\" and \"validate\" for what pydantic does. While pydantic is not simply a validation library (and I'm sure some would argue validation is not strictly what it does), most people use the word \"validation\" . It's time to stop fighting that, and use consistent names. The word \"parse\" will no longer be used except when talking about JSON parsing, see model methods above.","title":"I'm dropping the word \"parse\" and just using \"validate\""},{"location":"blog/pydantic-v2/#changes-to-custom-field-types","text":"Since the core structure of validators has changed from \"a list of validators to call one after another\" to \"a tree of validators which call each other\", the __get_validators__ way of defining custom field types no longer makes sense. Instead, we'll look for the attribute __pydantic_validation_schema__ which must be a pydantic-core compliant schema for validating data to this field type (the function item can be a string, if so a function of that name will be taken from the class, see 'validate' below). Here's an example of how a custom field type could be defined: New custom field types from pydantic import ValidationSchema class Foobar : def __init__ ( self , value : str ): self . value = value __pydantic_validation_schema__ : ValidationSchema = { 'type' : 'function' , 'mode' : 'after' , 'function' : 'validate' , 'schema' : { 'type' : 'str' } } @classmethod def validate ( cls , value ): if 'foobar' in value : return Foobar ( value ) else : raise ValueError ( 'expected foobar' ) What's going on here: __pydantic_validation_schema__ defines a schema which effectively says: Validate input data as a string, then call the validate function with that string, use the returned value as the final result of validation. ValidationSchema is just an alias to pydantic_core.Schema which is a type defining the schema for validation schemas. Note pydantic-core schema has full type definitions although since the type is recursive, mypy can't provide static type analysis, pyright however can. We can probably provide one or more helper functions to make __pydantic_validation_schema__ easier to generate.","title":"Changes to custom field types"},{"location":"blog/pydantic-v2/#other-improvements","text":"Some other things which will also change, IMHO for the better: Recursive models with cyclic references - although recursive models were supported by pydantic V1, data with cyclic references caused recursion errors, in pydantic-core cyclic references are correctly detected and a validation error is raised The reason I've been so keen to get pydantic-core to compile and run with wasm is that I want all examples in the docs of pydantic V2 to be editable and runnable in the browser Full support for TypedDict , including total=False - e.g. omitted keys, providing validation schema to a TypedDict field/item will use Annotated , e.g. Annotated[str, Field(strict=True)] from_orm has become from_attributes and is now defined at schema generation time (either via model config or field config) input_value has been added to each line error in a ValidationError , making errors easier to understand, and more comprehensive details of errors to be provided to end users, pydantic#784 on_error logic in a schema which allows either a default value to be used in the event of an error, or that value to be omitted (in the case of a total=False TypedDict ), pydantic-core#151 datetime , date , time & timedelta validation is improved, see the speedate Rust library I built specifically for this purpose for more details Powerful \"priority\" system for optionally merging or overriding config in sub-models for nested schemas Pydantic will support annotated-types , so you can do stuff like Annotated[set[int], Len(0, 10)] or Name = Annotated[str, Len(1, 1024)] A single decorator for general usage - we should add a validate decorator which can be used: on functions (replacing validate_arguments ) on dataclasses, pydantic.dataclasses.dataclass will become an alias of this on TypedDict s On any supported type, e.g. Union[...] , Dict[str, Thing] On Custom field types - e.g. anything with a __pydantic_schema__ attribute Easier validation error creation, I've often found myself wanting to raise ValidationError s outside models, particularly in FastAPI ( here is one method I've used), we should provide utilities to generate these errors Improve the performance of __eq__ on models Computed fields, these having been an idea for a long time in pydantic - we should get them right Model validation that avoids instances of subclasses leaking data (particularly important for FastAPI), see pydantic-core#155 We'll now follow semvar properly and avoid breaking changes between minor versions, as a result, major versions will become more common Improve generics to use M(Basemodel, Generic[T]) instead of M(GenericModel, Generic[T]) - e.g. GenericModel can be removed; this results from no-longer needing to compile pydantic code with cython","title":"Other Improvements"},{"location":"blog/pydantic-v2/#removed-features-limitations","text":"The emoji here is just for variation, I'm not frowning about any of this, these changes are either good IMHO (will make pydantic cleaner, easier to learn and easier to maintain) or irrelevant to 99.9+% of users. __root__ custom root models are no longer necessary since validation on any supported data type is allowed without a model .parse_file() and .parse_raw() , partially replaced with .model_validate_json() , see model methods .schema_json() & .copy() , see model methods TypeError are no longer considered as validation errors, but rather as internal errors, this is to better catch errors in argument names in function validators. Subclasses of builtin types like str , bytes and int are coerced to their parent builtin type, this is a limitation of how pydantic-core converts these types to Rust types during validation, if you have a specific need to keep the type, you can use wrap validators or custom type validation as described above integers are represented in rust code as i64 , meaning if you want to use ints where abs(v) > 2^63 \u2212 1 (9,223,372,036,854,775,807), you'll need to use a wrap validator and your own logic Settings Management ??? - I definitely don't want to remove the functionality, but it's something of a historical curiosity that it lives within pydantic, perhaps it should move to a separate package, perhaps installable alongside pydantic with pip install pydantic[settings] ? The following Config properties will be removed: fields - it's very old (it pre-dates Field ), can be removed allow_mutation will be removed, instead frozen will be used error_msg_templates , it's not properly documented anyway, error messages can be customized with external logic if required getter_dict - pydantic-core has hardcoded from_attributes logic json_loads - again this is hard coded in pydantic-core json_dumps - possibly json_encoders - see the export \"mode\" discussion above underscore_attrs_are_private we should just choose a sensible default smart_union - all unions are now \"smart\" dict(model) functionality should be removed, there's a much clearer distinction now that in 2017 when I implemented this between a model and a dict","title":"Removed Features &amp; Limitations"},{"location":"blog/pydantic-v2/#features-remaining","text":"The following features will remain (mostly) unchanged: JSONSchema, internally this will need to change a lot, but hopefully the external interface will remain unchanged dataclass support, again internals might change, but not the external interface validate_arguments , might be renamed, but otherwise remain hypothesis plugin, might be able to improve this as part of the general cleanup","title":"Features Remaining"},{"location":"blog/pydantic-v2/#questions","text":"I hope the explanation above is useful. I'm sure people will have questions and feedback; I'm aware I've skipped over some features with limited detail (this post is already fairly long ). To allow feedback without being overwhelmed, I've created a \"Pydantic V2\" category for discussions on github - please feel free to create a discussion if you have any questions or suggestions. We will endeavour to read and respond to everyone.","title":"Questions"},{"location":"blog/pydantic-v2/#implementation-details","text":"(This is yet to be built, so these are nascent ideas which might change) At the center of pydantic v2 will be a PydanticValidator class which looks roughly like this (note: this is just pseudo-code, it's not even valid python and is only supposed to be used to demonstrate the idea): PydanticValidator # type identifying data which has been validated, # as per pydantic-core, this can include \"fields_set\" data ValidData = ... # any type we can perform validation for AnyOutputType = ... class PydanticValidator : def __init__ ( self , output_type : AnyOutputType , config : Config ): ... def validate ( self , input_data : Any ) -> ValidData : ... def validate_json ( self , input_data : str | bytes | bytearray ) -> ValidData : ... def is_instance ( self , input_data : Any ) -> bool : ... def is_instance_json ( self , input_data : str | bytes | bytearray ) -> bool : ... def json_schema ( self ) -> dict : ... def dump ( self , data : ValidData , include : ... = None , exclude : ... = None , by_alias : bool = False , exclude_unset : bool = False , exclude_defaults : bool = False , exclude_none : bool = False , mode : Literal [ 'unchanged' , 'dicts' , 'json-compliant' ] = 'unchanged' , converter : Callable [[ Any ], Any ] | None = None ) -> Any : ... def dump_json ( self , ... ) -> str : ... This could be used directly, but more commonly will be used by the following: BaseModel the validate decorator described above pydantic.dataclasses.dataclass (which might be an alias of validate ) generics The aim will be to get pydantic V2 to a place were the vast majority of tests continue to pass unchanged. Thereby guaranteeing (as much as possible) that the external interface to pydantic and its behaviour are unchanged.","title":"Implementation Details"},{"location":"blog/pydantic-v2/#conversion-table","text":"The table below provisionally defines what input value types are allowed to which field types. An updated and complete version of this table will be included in the docs for V2. Note Some type conversion shown here is a significant departure from existing behavior, we may have to provide a config flag for backwards compatibility for a few of them, however pydantic V2 cannot be entirely backward compatible, see pydantic-core#152 . Field Type Input Mode Input Source Conditions str str both python, JSON - str bytes lax python assumes UTF-8, error on unicode decoding error str bytearray lax python assumes UTF-8, error on unicode decoding error bytes bytes both python - bytes str both JSON - bytes str lax python - bytes bytearray lax python - int int strict python, JSON max abs value 2^64 - i64 is used internally, bool explicitly forbidden int int lax python, JSON i64 int float lax python, JSON i64 , must be exact int, e.g. f % 1 == 0 , nan , inf raise errors int Decimal lax python, JSON i64 , must be exact int, e.g. f % 1 == 0 int bool lax python, JSON - int str lax python, JSON i64 , must be numeric only, e.g. [0-9]+ float float strict python, JSON bool explicitly forbidden float float lax python, JSON - float int lax python, JSON - float str lax python, JSON must match [0-9]+(\\.[0-9]+)? float Decimal lax python - float bool lax python, JSON - bool bool both python, JSON - bool int lax python, JSON allowed: 0, 1 bool float lax python, JSON allowed: 0, 1 bool Decimal lax python, JSON allowed: 0, 1 bool str lax python, JSON allowed: 'f', 'n', 'no', 'off', 'false', 't', 'y', 'on', 'yes', 'true' None None both python, JSON - date date both python - date datetime lax python must be exact date, eg. no H, M, S, f date str both JSON format YYYY-MM-DD date str lax python format YYYY-MM-DD date bytes lax python format YYYY-MM-DD (UTF-8) date int lax python, JSON interpreted as seconds or ms from epoch, see speedate , must be exact date date float lax python, JSON interpreted as seconds or ms from epoch, see speedate , must be exact date datetime datetime both python - datetime date lax python - datetime str both JSON format YYYY-MM-DDTHH:MM:SS.f etc. see speedate datetime str lax python format YYYY-MM-DDTHH:MM:SS.f etc. see speedate datetime bytes lax python format YYYY-MM-DDTHH:MM:SS.f etc. see speedate , (UTF-8) datetime int lax python, JSON interpreted as seconds or ms from epoch, see speedate datetime float lax python, JSON interpreted as seconds or ms from epoch, see speedate time time both python - time str both JSON format HH:MM:SS.FFFFFF etc. see speedate time str lax python format HH:MM:SS.FFFFFF etc. see speedate time bytes lax python format HH:MM:SS.FFFFFF etc. see speedate , (UTF-8) time int lax python, JSON interpreted as seconds, range 0 - 86399 time float lax python, JSON interpreted as seconds, range 0 - 86399.9* time Decimal lax python, JSON interpreted as seconds, range 0 - 86399.9* timedelta timedelta both python - timedelta str both JSON format ISO8601 etc. see speedate timedelta str lax python format ISO8601 etc. see speedate timedelta bytes lax python format ISO8601 etc. see speedate , (UTF-8) timedelta int lax python, JSON interpreted as seconds timedelta float lax python, JSON interpreted as seconds timedelta Decimal lax python, JSON interpreted as seconds dict dict both python - dict Object both JSON - dict mapping lax python must implement the mapping interface and have an items() method TypedDict dict both python - TypedDict Object both JSON - TypedDict Any both python builtins not allowed, uses getattr , requires from_attributes=True TypedDict mapping lax python must implement the mapping interface and have an items() method list list both python - list Array both JSON - list tuple lax python - list set lax python - list frozenset lax python - list dict_keys lax python - tuple tuple both python - tuple Array both JSON - tuple list lax python - tuple set lax python - tuple frozenset lax python - tuple dict_keys lax python - set set both python - set Array both JSON - set list lax python - set tuple lax python - set frozenset lax python - set dict_keys lax python - frozenset frozenset both python - frozenset Array both JSON - frozenset list lax python - frozenset tuple lax python - frozenset set lax python - frozenset dict_keys lax python - is_instance Any both python isinstance() check returns True is_instance - both JSON never valid callable Any both python callable() check returns True callable - both JSON never valid The ModelClass validator (use to create instances of a class) uses the TypedDict validator, then creates an instance with __dict__ and __fields_set__ set, so same rules apply as TypedDict .","title":"Conversion Table "},{"location":"types/boolean/","text":"Boolean Types \u00b6 IsTrueLike \u00b6 Bases: DirtyEquals [ bool ] Check if the value is True like. IsTrueLike allows comparison to anything and effectively uses just return bool(other) . Example of basic usage: IsTrueLike from dirty_equals import IsTrueLike assert True == IsTrueLike assert 1 == IsTrueLike assert 'true' == IsTrueLike assert 'foobar' == IsTrueLike # any non-empty string is \"True\" assert '' != IsTrueLike assert [ 1 ] == IsTrueLike assert {} != IsTrueLike assert None != IsTrueLike IsFalseLike \u00b6 IsFalseLike ( * , allow_strings : bool = False ) Bases: DirtyEquals [ bool ] Check if the value is False like. IsFalseLike allows comparison to anything and effectively uses return not bool(other) (with string checks if allow_strings=True is set). Parameters: Name Type Description Default allow_strings bool if True , allow comparisons to False like strings, case-insensitive, allows '' , 'false' and any string where float(other) == 0 (e.g. '0' ). False Example of basic usage: IsFalseLike from dirty_equals import IsFalseLike assert False == IsFalseLike assert 0 == IsFalseLike assert 'false' == IsFalseLike ( allow_strings = True ) assert '0' == IsFalseLike ( allow_strings = True ) assert 'foobar' != IsFalseLike ( allow_strings = True ) assert 'false' != IsFalseLike assert 'True' != IsFalseLike ( allow_strings = True ) assert [ 1 ] != IsFalseLike assert {} == IsFalseLike assert None == IsFalseLike assert '' == IsFalseLike ( allow_strings = True ) assert '' == IsFalseLike","title":"Boolean Types"},{"location":"types/boolean/#boolean-types","text":"","title":"Boolean Types"},{"location":"types/boolean/#dirty_equals.IsTrueLike","text":"Bases: DirtyEquals [ bool ] Check if the value is True like. IsTrueLike allows comparison to anything and effectively uses just return bool(other) . Example of basic usage: IsTrueLike from dirty_equals import IsTrueLike assert True == IsTrueLike assert 1 == IsTrueLike assert 'true' == IsTrueLike assert 'foobar' == IsTrueLike # any non-empty string is \"True\" assert '' != IsTrueLike assert [ 1 ] == IsTrueLike assert {} != IsTrueLike assert None != IsTrueLike","title":"IsTrueLike"},{"location":"types/boolean/#dirty_equals.IsFalseLike","text":"IsFalseLike ( * , allow_strings : bool = False ) Bases: DirtyEquals [ bool ] Check if the value is False like. IsFalseLike allows comparison to anything and effectively uses return not bool(other) (with string checks if allow_strings=True is set). Parameters: Name Type Description Default allow_strings bool if True , allow comparisons to False like strings, case-insensitive, allows '' , 'false' and any string where float(other) == 0 (e.g. '0' ). False Example of basic usage: IsFalseLike from dirty_equals import IsFalseLike assert False == IsFalseLike assert 0 == IsFalseLike assert 'false' == IsFalseLike ( allow_strings = True ) assert '0' == IsFalseLike ( allow_strings = True ) assert 'foobar' != IsFalseLike ( allow_strings = True ) assert 'false' != IsFalseLike assert 'True' != IsFalseLike ( allow_strings = True ) assert [ 1 ] != IsFalseLike assert {} == IsFalseLike assert None == IsFalseLike assert '' == IsFalseLike ( allow_strings = True ) assert '' == IsFalseLike","title":"IsFalseLike"},{"location":"types/custom/","text":"Custom Types \u00b6 DirtyEquals \u00b6 Bases: Generic [ T ] Base type for all dirty-equals types. __init__ \u00b6 __init__ ( * repr_args : Any , ** repr_kwargs : Any ) Parameters: Name Type Description Default *repr_args Any unnamed args to be used in __repr__ () **repr_kwargs Any named args to be used in __repr__ {} equals \u00b6 equals ( other : Any ) -> bool Abstract method, must be implemented by subclasses. TypeError and ValueError are caught in __eq__ and indicate other is not equals to this type. value property \u00b6 value () -> T Property to get the value last successfully compared to this object. This is seldom very useful, put it's provided for completeness. Example of usage: .values from dirty_equals import IsStr token_is_str = IsStr ( regex = r 't-.+' ) assert 't-123' == token_is_str print ( token_is_str . value ) #> 't-123' Custom Type Example \u00b6 To demonstrate the use of custom types, we'll create a custom type that matches any even number. We won't inherit from IsNumeric in this case to keep the example simple. IsEven from decimal import Decimal from typing import Any , Union from dirty_equals import IsOneOf from dirty_equals import DirtyEquals class IsEven ( DirtyEquals [ Union [ int , float , Decimal ]]): def equals ( self , other : Any ) -> bool : return other % 2 == 0 assert 2 == IsEven assert 3 != IsEven assert 'foobar' != IsEven assert 3 == IsEven | IsOneOf ( 3 ) There are a few advantages of inheriting from DirtyEquals compared to just implementing your own class with an __eq__ method: TypeError and ValueError in equals are caught and result in a not-equals result. A useful __repr__ is generated, and modified if the == operation returns True , see pytest compatibility boolean logic works out of the box Uninitialised usage ( IsEven rather than IsEven() ) works out of the box","title":"Custom Types"},{"location":"types/custom/#custom-types","text":"","title":"Custom Types"},{"location":"types/custom/#dirty_equals._base.DirtyEquals","text":"Bases: Generic [ T ] Base type for all dirty-equals types.","title":"DirtyEquals"},{"location":"types/custom/#dirty_equals._base.DirtyEquals.__init__","text":"__init__ ( * repr_args : Any , ** repr_kwargs : Any ) Parameters: Name Type Description Default *repr_args Any unnamed args to be used in __repr__ () **repr_kwargs Any named args to be used in __repr__ {}","title":"__init__()"},{"location":"types/custom/#dirty_equals._base.DirtyEquals.equals","text":"equals ( other : Any ) -> bool Abstract method, must be implemented by subclasses. TypeError and ValueError are caught in __eq__ and indicate other is not equals to this type.","title":"equals()"},{"location":"types/custom/#dirty_equals._base.DirtyEquals.value","text":"value () -> T Property to get the value last successfully compared to this object. This is seldom very useful, put it's provided for completeness. Example of usage: .values from dirty_equals import IsStr token_is_str = IsStr ( regex = r 't-.+' ) assert 't-123' == token_is_str print ( token_is_str . value ) #> 't-123'","title":"value()"},{"location":"types/custom/#custom-type-example","text":"To demonstrate the use of custom types, we'll create a custom type that matches any even number. We won't inherit from IsNumeric in this case to keep the example simple. IsEven from decimal import Decimal from typing import Any , Union from dirty_equals import IsOneOf from dirty_equals import DirtyEquals class IsEven ( DirtyEquals [ Union [ int , float , Decimal ]]): def equals ( self , other : Any ) -> bool : return other % 2 == 0 assert 2 == IsEven assert 3 != IsEven assert 'foobar' != IsEven assert 3 == IsEven | IsOneOf ( 3 ) There are a few advantages of inheriting from DirtyEquals compared to just implementing your own class with an __eq__ method: TypeError and ValueError in equals are caught and result in a not-equals result. A useful __repr__ is generated, and modified if the == operation returns True , see pytest compatibility boolean logic works out of the box Uninitialised usage ( IsEven rather than IsEven() ) works out of the box","title":"Custom Type Example"},{"location":"types/datetime/","text":"Date and Time Types \u00b6 IsDatetime \u00b6 IsDatetime ( * , approx : Optional [ datetime ] = None , delta : Optional [ Union [ timedelta , int , float ]] = None , gt : Optional [ datetime ] = None , lt : Optional [ datetime ] = None , ge : Optional [ datetime ] = None , le : Optional [ datetime ] = None , unix_number : bool = False , iso_string : bool = False , format_string : Optional [ str ] = None , enforce_tz : bool = True ) Bases: IsNumeric [ datetime ] Check if the value is a datetime, and matches the given conditions. Parameters: Name Type Description Default approx Optional [ datetime ] A value to approximately compare to. None delta Optional [ Union [ timedelta , int , float ]] The allowable different when comparing to the value to approx , if omitted 2 seconds is used, ints and floats are assumed to represent seconds and converted to timedelta s. None gt Optional [ datetime ] Value which the compared value should be greater than (after). None lt Optional [ datetime ] Value which the compared value should be less than (before). None ge Optional [ datetime ] Value which the compared value should be greater than (after) or equal to. None le Optional [ datetime ] Value which the compared value should be less than (before) or equal to. None unix_number bool whether to allow unix timestamp numbers in comparison False iso_string bool whether to allow iso formatted strings in comparison False format_string Optional [ str ] if provided, format_string is used with datetime.strptime to parse strings None enforce_tz bool whether timezone should be enforced in comparison, see below for more details True Examples of basic usage: IsDatetime from dirty_equals import IsDatetime from datetime import datetime y2k = datetime ( 2000 , 1 , 1 ) assert datetime ( 2000 , 1 , 1 ) == IsDatetime ( approx = y2k ) # Note: this requires the system timezone to be UTC assert 946684800.123 == IsDatetime ( approx = y2k , unix_number = True ) assert datetime ( 2000 , 1 , 1 , 0 , 0 , 9 ) == IsDatetime ( approx = y2k , delta = 10 ) assert '2000-01-01T00:00' == IsDatetime ( approx = y2k , iso_string = True ) assert datetime ( 2000 , 1 , 2 ) == IsDatetime ( gt = y2k ) assert datetime ( 1999 , 1 , 2 ) != IsDatetime ( gt = y2k ) Timezones \u00b6 Timezones are hard, anyone who claims otherwise is either a genius, a liar, or an idiot. IsDatetime and its subtypes (e.g. IsNow ) can be used in two modes, based on the enforce_tz parameter: enforce_tz=True (the default): * if the datetime wrapped by IsDatetime is timezone naive, the compared value must also be timezone naive. * if the datetime wrapped by IsDatetime has a timezone, the compared value must have a timezone with the same offset. enforce_tz=False : * if the datetime wrapped by IsDatetime is timezone naive, the compared value can either be naive or have a timezone all that matters is the datetime values match. * if the datetime wrapped by IsDatetime has a timezone, the compared value needs to represent the same point in time - either way it must have a timezone. Example IsDatetime & timezones from datetime import datetime from dirty_equals import IsDatetime import pytz tz_london = pytz . timezone ( 'Europe/London' ) new_year_london = tz_london . localize ( datetime ( 2000 , 1 , 1 )) tz_nyc = pytz . timezone ( 'America/New_York' ) new_year_eve_nyc = tz_nyc . localize ( datetime ( 1999 , 12 , 31 , 19 , 0 , 0 )) assert new_year_eve_nyc == IsDatetime ( approx = new_year_london , enforce_tz = False ) assert new_year_eve_nyc != IsDatetime ( approx = new_year_london , enforce_tz = True ) new_year_naive = datetime ( 2000 , 1 , 1 ) assert new_year_naive != IsDatetime ( approx = new_year_london , enforce_tz = False ) assert new_year_naive != IsDatetime ( approx = new_year_eve_nyc , enforce_tz = False ) assert new_year_london == IsDatetime ( approx = new_year_naive , enforce_tz = False ) assert new_year_eve_nyc != IsDatetime ( approx = new_year_naive , enforce_tz = False ) IsNow \u00b6 IsNow ( * , delta : Union [ timedelta , int , float ] = 2 , unix_number : bool = False , iso_string : bool = False , format_string : Optional [ str ] = None , enforce_tz : bool = True , tz : Union [ None , str , tzinfo ] = None ) Bases: IsDatetime Check if a datetime is close to now, this is similar to IsDatetime(approx=datetime.now()) , but slightly more powerful. Parameters: Name Type Description Default delta Union [ timedelta , int , float ] The allowable different when comparing to the value to now, if omitted 2 seconds is used, ints and floats are assumed to represent seconds and converted to timedelta s. 2 unix_number bool whether to allow unix timestamp numbers in comparison False iso_string bool whether to allow iso formatted strings in comparison False format_string Optional [ str ] if provided, format_string is used with datetime.strptime to parse strings None enforce_tz bool whether timezone should be enforced in comparison, see below for more details True tz Union [None, str , tzinfo ] either a pytz.timezone , a datetime.timezone or a string which will be passed to pytz.timezone , if provided now will be converted to this timezone. None IsNow from dirty_equals import IsNow from datetime import datetime , timezone now = datetime . now () assert now == IsNow assert now . timestamp () == IsNow ( unix_number = True ) assert now . timestamp () != IsNow assert now . isoformat () == IsNow ( iso_string = True ) assert now . isoformat () != IsNow utc_now = datetime . utcnow () . replace ( tzinfo = timezone . utc ) assert utc_now == IsNow ( tz = timezone . utc ) IsDate \u00b6 IsDate ( * , approx : Optional [ date ] = None , delta : Optional [ Union [ timedelta , int , float ]] = None , gt : Optional [ date ] = None , lt : Optional [ date ] = None , ge : Optional [ date ] = None , le : Optional [ date ] = None , iso_string : bool = False , format_string : Optional [ str ] = None ) Bases: IsNumeric [ date ] Check if the value is a date, and matches the given conditions. Parameters: Name Type Description Default approx Optional [ date ] A value to approximately compare to. None delta Optional [ Union [ timedelta , int , float ]] The allowable different when comparing to the value to now, if omitted 2 seconds is used, ints and floats are assumed to represent seconds and converted to timedelta s. None gt Optional [ date ] Value which the compared value should be greater than (after). None lt Optional [ date ] Value which the compared value should be less than (before). None ge Optional [ date ] Value which the compared value should be greater than (after) or equal to. None le Optional [ date ] Value which the compared value should be less than (before) or equal to. None iso_string bool whether to allow iso formatted strings in comparison False format_string Optional [ str ] if provided, format_string is used with datetime.strptime to parse strings None Examples of basic usage: IsDate from dirty_equals import IsDate from datetime import date y2k = date ( 2000 , 1 , 1 ) assert date ( 2000 , 1 , 1 ) == IsDate ( approx = y2k ) assert '2000-01-01' == IsDate ( approx = y2k , iso_string = True ) assert date ( 2000 , 1 , 2 ) == IsDate ( gt = y2k ) assert date ( 1999 , 1 , 2 ) != IsDate ( gt = y2k ) IsToday \u00b6 IsToday ( * , iso_string : bool = False , format_string : Optional [ str ] = None ) Bases: IsDate Check if a date is today, this is similar to IsDate(approx=date.today()) , but slightly more powerful. Parameters: Name Type Description Default iso_string bool whether to allow iso formatted strings in comparison False format_string Optional [ str ] if provided, format_string is used with datetime.strptime to parse strings None IsToday from dirty_equals import IsToday from datetime import date , timedelta today = date . today () assert today == IsToday assert today . isoformat () == IsToday ( iso_string = True ) assert today . isoformat () != IsToday assert today + timedelta ( days = 1 ) != IsToday assert today . strftime ( '%Y/%m/ %d ' ) == IsToday ( format_string = '%Y/%m/ %d ' ) assert today . strftime ( '%Y/%m/ %d ' ) != IsToday ()","title":"Date and Time Types"},{"location":"types/datetime/#date-and-time-types","text":"","title":"Date and Time Types"},{"location":"types/datetime/#dirty_equals.IsDatetime","text":"IsDatetime ( * , approx : Optional [ datetime ] = None , delta : Optional [ Union [ timedelta , int , float ]] = None , gt : Optional [ datetime ] = None , lt : Optional [ datetime ] = None , ge : Optional [ datetime ] = None , le : Optional [ datetime ] = None , unix_number : bool = False , iso_string : bool = False , format_string : Optional [ str ] = None , enforce_tz : bool = True ) Bases: IsNumeric [ datetime ] Check if the value is a datetime, and matches the given conditions. Parameters: Name Type Description Default approx Optional [ datetime ] A value to approximately compare to. None delta Optional [ Union [ timedelta , int , float ]] The allowable different when comparing to the value to approx , if omitted 2 seconds is used, ints and floats are assumed to represent seconds and converted to timedelta s. None gt Optional [ datetime ] Value which the compared value should be greater than (after). None lt Optional [ datetime ] Value which the compared value should be less than (before). None ge Optional [ datetime ] Value which the compared value should be greater than (after) or equal to. None le Optional [ datetime ] Value which the compared value should be less than (before) or equal to. None unix_number bool whether to allow unix timestamp numbers in comparison False iso_string bool whether to allow iso formatted strings in comparison False format_string Optional [ str ] if provided, format_string is used with datetime.strptime to parse strings None enforce_tz bool whether timezone should be enforced in comparison, see below for more details True Examples of basic usage: IsDatetime from dirty_equals import IsDatetime from datetime import datetime y2k = datetime ( 2000 , 1 , 1 ) assert datetime ( 2000 , 1 , 1 ) == IsDatetime ( approx = y2k ) # Note: this requires the system timezone to be UTC assert 946684800.123 == IsDatetime ( approx = y2k , unix_number = True ) assert datetime ( 2000 , 1 , 1 , 0 , 0 , 9 ) == IsDatetime ( approx = y2k , delta = 10 ) assert '2000-01-01T00:00' == IsDatetime ( approx = y2k , iso_string = True ) assert datetime ( 2000 , 1 , 2 ) == IsDatetime ( gt = y2k ) assert datetime ( 1999 , 1 , 2 ) != IsDatetime ( gt = y2k )","title":"IsDatetime"},{"location":"types/datetime/#timezones","text":"Timezones are hard, anyone who claims otherwise is either a genius, a liar, or an idiot. IsDatetime and its subtypes (e.g. IsNow ) can be used in two modes, based on the enforce_tz parameter: enforce_tz=True (the default): * if the datetime wrapped by IsDatetime is timezone naive, the compared value must also be timezone naive. * if the datetime wrapped by IsDatetime has a timezone, the compared value must have a timezone with the same offset. enforce_tz=False : * if the datetime wrapped by IsDatetime is timezone naive, the compared value can either be naive or have a timezone all that matters is the datetime values match. * if the datetime wrapped by IsDatetime has a timezone, the compared value needs to represent the same point in time - either way it must have a timezone. Example IsDatetime & timezones from datetime import datetime from dirty_equals import IsDatetime import pytz tz_london = pytz . timezone ( 'Europe/London' ) new_year_london = tz_london . localize ( datetime ( 2000 , 1 , 1 )) tz_nyc = pytz . timezone ( 'America/New_York' ) new_year_eve_nyc = tz_nyc . localize ( datetime ( 1999 , 12 , 31 , 19 , 0 , 0 )) assert new_year_eve_nyc == IsDatetime ( approx = new_year_london , enforce_tz = False ) assert new_year_eve_nyc != IsDatetime ( approx = new_year_london , enforce_tz = True ) new_year_naive = datetime ( 2000 , 1 , 1 ) assert new_year_naive != IsDatetime ( approx = new_year_london , enforce_tz = False ) assert new_year_naive != IsDatetime ( approx = new_year_eve_nyc , enforce_tz = False ) assert new_year_london == IsDatetime ( approx = new_year_naive , enforce_tz = False ) assert new_year_eve_nyc != IsDatetime ( approx = new_year_naive , enforce_tz = False )","title":"Timezones"},{"location":"types/datetime/#dirty_equals.IsNow","text":"IsNow ( * , delta : Union [ timedelta , int , float ] = 2 , unix_number : bool = False , iso_string : bool = False , format_string : Optional [ str ] = None , enforce_tz : bool = True , tz : Union [ None , str , tzinfo ] = None ) Bases: IsDatetime Check if a datetime is close to now, this is similar to IsDatetime(approx=datetime.now()) , but slightly more powerful. Parameters: Name Type Description Default delta Union [ timedelta , int , float ] The allowable different when comparing to the value to now, if omitted 2 seconds is used, ints and floats are assumed to represent seconds and converted to timedelta s. 2 unix_number bool whether to allow unix timestamp numbers in comparison False iso_string bool whether to allow iso formatted strings in comparison False format_string Optional [ str ] if provided, format_string is used with datetime.strptime to parse strings None enforce_tz bool whether timezone should be enforced in comparison, see below for more details True tz Union [None, str , tzinfo ] either a pytz.timezone , a datetime.timezone or a string which will be passed to pytz.timezone , if provided now will be converted to this timezone. None IsNow from dirty_equals import IsNow from datetime import datetime , timezone now = datetime . now () assert now == IsNow assert now . timestamp () == IsNow ( unix_number = True ) assert now . timestamp () != IsNow assert now . isoformat () == IsNow ( iso_string = True ) assert now . isoformat () != IsNow utc_now = datetime . utcnow () . replace ( tzinfo = timezone . utc ) assert utc_now == IsNow ( tz = timezone . utc )","title":"IsNow"},{"location":"types/datetime/#dirty_equals.IsDate","text":"IsDate ( * , approx : Optional [ date ] = None , delta : Optional [ Union [ timedelta , int , float ]] = None , gt : Optional [ date ] = None , lt : Optional [ date ] = None , ge : Optional [ date ] = None , le : Optional [ date ] = None , iso_string : bool = False , format_string : Optional [ str ] = None ) Bases: IsNumeric [ date ] Check if the value is a date, and matches the given conditions. Parameters: Name Type Description Default approx Optional [ date ] A value to approximately compare to. None delta Optional [ Union [ timedelta , int , float ]] The allowable different when comparing to the value to now, if omitted 2 seconds is used, ints and floats are assumed to represent seconds and converted to timedelta s. None gt Optional [ date ] Value which the compared value should be greater than (after). None lt Optional [ date ] Value which the compared value should be less than (before). None ge Optional [ date ] Value which the compared value should be greater than (after) or equal to. None le Optional [ date ] Value which the compared value should be less than (before) or equal to. None iso_string bool whether to allow iso formatted strings in comparison False format_string Optional [ str ] if provided, format_string is used with datetime.strptime to parse strings None Examples of basic usage: IsDate from dirty_equals import IsDate from datetime import date y2k = date ( 2000 , 1 , 1 ) assert date ( 2000 , 1 , 1 ) == IsDate ( approx = y2k ) assert '2000-01-01' == IsDate ( approx = y2k , iso_string = True ) assert date ( 2000 , 1 , 2 ) == IsDate ( gt = y2k ) assert date ( 1999 , 1 , 2 ) != IsDate ( gt = y2k )","title":"IsDate"},{"location":"types/datetime/#dirty_equals.IsToday","text":"IsToday ( * , iso_string : bool = False , format_string : Optional [ str ] = None ) Bases: IsDate Check if a date is today, this is similar to IsDate(approx=date.today()) , but slightly more powerful. Parameters: Name Type Description Default iso_string bool whether to allow iso formatted strings in comparison False format_string Optional [ str ] if provided, format_string is used with datetime.strptime to parse strings None IsToday from dirty_equals import IsToday from datetime import date , timedelta today = date . today () assert today == IsToday assert today . isoformat () == IsToday ( iso_string = True ) assert today . isoformat () != IsToday assert today + timedelta ( days = 1 ) != IsToday assert today . strftime ( '%Y/%m/ %d ' ) == IsToday ( format_string = '%Y/%m/ %d ' ) assert today . strftime ( '%Y/%m/ %d ' ) != IsToday ()","title":"IsToday"},{"location":"types/dict/","text":"Dictionary Types \u00b6 IsDict \u00b6 IsDict ( * expected_args : Dict [ Any , Any ], ** expected_kwargs : Any ) Bases: DirtyEquals [ Dict [ Any , Any ]] Base class for comparing dictionaries. By default, IsDict isn't particularly useful on its own (it behaves pretty much like a normal dict ), but it can be subclassed (see IsPartialDict and IsStrictDict ) or modified with .settings(...) to powerful things. Can be created from either keyword arguments or an existing dictionary (same as dict() ). IsDict is not particularly useful on its own, but it can be subclassed or modified with .settings(...) to facilitate powerful comparison of dictionaries. IsDict from dirty_equals import IsDict assert { 'a' : 1 , 'b' : 2 } == IsDict ( a = 1 , b = 2 ) assert { 1 : 2 , 3 : 4 } == IsDict ({ 1 : 2 , 3 : 4 }) settings \u00b6 settings ( * , strict : Optional [ bool ] = None , partial : Optional [ bool ] = None , ignore : Union [ None , Container [ Any ], Callable [[ Any ], bool ] ] = NotGiven ) -> IsDict Allows you to customise the behaviour of IsDict , technically a new IsDict is required to allow chaining. Parameters: Name Type Description Default strict bool If True , the order of key/value pairs must match. None partial bool If True , only keys include in the wrapped dict are checked. None ignore Union [None, Container [ Any ], Callable [[ Any ], bool ]] Values to omit from comparison. Can be either a Container (e.g. set or list ) of values to ignore, or a function that takes a value and should return True if the value should be ignored. NotGiven IsDict.settings(...) from dirty_equals import IsDict assert { 'a' : 1 , 'b' : 2 , 'c' : None } != IsDict ( a = 1 , b = 2 ) assert { 'a' : 1 , 'b' : 2 , 'c' : None } == IsDict ( a = 1 , b = 2 ) . settings ( partial = True ) #(1)! assert { 'b' : 2 , 'a' : 1 } == IsDict ( a = 1 , b = 2 ) assert { 'b' : 2 , 'a' : 1 } != IsDict ( a = 1 , b = 2 ) . settings ( strict = True ) #(2)! # combining partial and strict assert { 'a' : 1 , 'b' : None , 'c' : 3 } == IsDict ( a = 1 , c = 3 ) . settings ( strict = True , partial = True ) assert { 'b' : None , 'c' : 3 , 'a' : 1 } != IsDict ( a = 1 , c = 3 ) . settings ( strict = True , partial = True ) This is the same as IsPartialDict(a=1, b=2) This is the same as IsStrictDict(a=1, b=2) IsPartialDict \u00b6 Bases: IsDict Partial dictionary comparison, this is the same as IsDict(...).settings(partial=True) . IsPartialDict from dirty_equals import IsPartialDict assert { 'a' : 1 , 'b' : 2 , 'c' : 3 } == IsPartialDict ( a = 1 , b = 2 ) assert { 'a' : 1 , 'b' : 2 , 'c' : 3 } != IsPartialDict ( a = 1 , b = 3 ) assert { 'a' : 1 , 'b' : 2 , 'd' : 3 } != IsPartialDict ( a = 1 , b = 2 , c = 3 ) # combining partial and strict assert { 'a' : 1 , 'b' : None , 'c' : 3 } == IsPartialDict ( a = 1 , c = 3 ) . settings ( strict = True ) assert { 'b' : None , 'c' : 3 , 'a' : 1 } != IsPartialDict ( a = 1 , c = 3 ) . settings ( strict = True ) IsIgnoreDict \u00b6 Bases: IsDict Dictionary comparison with None values ignored, this is the same as IsDict(...).settings(ignore={None}) . .settings(...) can be used to customise the behaviour of IsIgnoreDict , in particular changing which values are ignored. IsIgnoreDict from dirty_equals import IsIgnoreDict assert { 'a' : 1 , 'b' : 2 , 'c' : None } == IsIgnoreDict ( a = 1 , b = 2 ) assert { 'a' : 1 , 'b' : 2 , 'c' : None , 'c' : 'ignore' } == ( IsIgnoreDict ( a = 1 , b = 2 ) . settings ( ignore = { None , 'ignore' }) ) def is_even ( v : int ) -> bool : return v % 2 == 0 assert { 'a' : 1 , 'b' : 2 , 'c' : 3 , 'd' : 4 } == ( IsIgnoreDict ( a = 1 , c = 3 ) . settings ( ignore = is_even ) ) # combining partial and strict assert { 'a' : 1 , 'b' : None , 'c' : 3 } == IsIgnoreDict ( a = 1 , c = 3 ) . settings ( strict = True ) assert { 'b' : None , 'c' : 3 , 'a' : 1 } != IsIgnoreDict ( a = 1 , c = 3 ) . settings ( strict = True ) IsStrictDict \u00b6 Bases: IsDict Dictionary comparison with order enforced, this is the same as IsDict(...).settings(strict=True) . IsDict.settings(...) from dirty_equals import IsStrictDict assert { 'a' : 1 , 'b' : 2 } == IsStrictDict ( a = 1 , b = 2 ) assert { 'a' : 1 , 'b' : 2 , 'c' : 3 } != IsStrictDict ( a = 1 , b = 2 ) assert { 'b' : 2 , 'a' : 1 } != IsStrictDict ( a = 1 , b = 2 ) # combining partial and strict assert { 'a' : 1 , 'b' : None , 'c' : 3 } == IsStrictDict ( a = 1 , c = 3 ) . settings ( partial = True ) assert { 'b' : None , 'c' : 3 , 'a' : 1 } != IsStrictDict ( a = 1 , c = 3 ) . settings ( partial = True )","title":"Dictionary Types"},{"location":"types/dict/#dictionary-types","text":"","title":"Dictionary Types"},{"location":"types/dict/#dirty_equals.IsDict","text":"IsDict ( * expected_args : Dict [ Any , Any ], ** expected_kwargs : Any ) Bases: DirtyEquals [ Dict [ Any , Any ]] Base class for comparing dictionaries. By default, IsDict isn't particularly useful on its own (it behaves pretty much like a normal dict ), but it can be subclassed (see IsPartialDict and IsStrictDict ) or modified with .settings(...) to powerful things. Can be created from either keyword arguments or an existing dictionary (same as dict() ). IsDict is not particularly useful on its own, but it can be subclassed or modified with .settings(...) to facilitate powerful comparison of dictionaries. IsDict from dirty_equals import IsDict assert { 'a' : 1 , 'b' : 2 } == IsDict ( a = 1 , b = 2 ) assert { 1 : 2 , 3 : 4 } == IsDict ({ 1 : 2 , 3 : 4 })","title":"IsDict"},{"location":"types/dict/#dirty_equals._dict.IsDict.settings","text":"settings ( * , strict : Optional [ bool ] = None , partial : Optional [ bool ] = None , ignore : Union [ None , Container [ Any ], Callable [[ Any ], bool ] ] = NotGiven ) -> IsDict Allows you to customise the behaviour of IsDict , technically a new IsDict is required to allow chaining. Parameters: Name Type Description Default strict bool If True , the order of key/value pairs must match. None partial bool If True , only keys include in the wrapped dict are checked. None ignore Union [None, Container [ Any ], Callable [[ Any ], bool ]] Values to omit from comparison. Can be either a Container (e.g. set or list ) of values to ignore, or a function that takes a value and should return True if the value should be ignored. NotGiven IsDict.settings(...) from dirty_equals import IsDict assert { 'a' : 1 , 'b' : 2 , 'c' : None } != IsDict ( a = 1 , b = 2 ) assert { 'a' : 1 , 'b' : 2 , 'c' : None } == IsDict ( a = 1 , b = 2 ) . settings ( partial = True ) #(1)! assert { 'b' : 2 , 'a' : 1 } == IsDict ( a = 1 , b = 2 ) assert { 'b' : 2 , 'a' : 1 } != IsDict ( a = 1 , b = 2 ) . settings ( strict = True ) #(2)! # combining partial and strict assert { 'a' : 1 , 'b' : None , 'c' : 3 } == IsDict ( a = 1 , c = 3 ) . settings ( strict = True , partial = True ) assert { 'b' : None , 'c' : 3 , 'a' : 1 } != IsDict ( a = 1 , c = 3 ) . settings ( strict = True , partial = True ) This is the same as IsPartialDict(a=1, b=2) This is the same as IsStrictDict(a=1, b=2)","title":"settings()"},{"location":"types/dict/#dirty_equals.IsPartialDict","text":"Bases: IsDict Partial dictionary comparison, this is the same as IsDict(...).settings(partial=True) . IsPartialDict from dirty_equals import IsPartialDict assert { 'a' : 1 , 'b' : 2 , 'c' : 3 } == IsPartialDict ( a = 1 , b = 2 ) assert { 'a' : 1 , 'b' : 2 , 'c' : 3 } != IsPartialDict ( a = 1 , b = 3 ) assert { 'a' : 1 , 'b' : 2 , 'd' : 3 } != IsPartialDict ( a = 1 , b = 2 , c = 3 ) # combining partial and strict assert { 'a' : 1 , 'b' : None , 'c' : 3 } == IsPartialDict ( a = 1 , c = 3 ) . settings ( strict = True ) assert { 'b' : None , 'c' : 3 , 'a' : 1 } != IsPartialDict ( a = 1 , c = 3 ) . settings ( strict = True )","title":"IsPartialDict"},{"location":"types/dict/#dirty_equals.IsIgnoreDict","text":"Bases: IsDict Dictionary comparison with None values ignored, this is the same as IsDict(...).settings(ignore={None}) . .settings(...) can be used to customise the behaviour of IsIgnoreDict , in particular changing which values are ignored. IsIgnoreDict from dirty_equals import IsIgnoreDict assert { 'a' : 1 , 'b' : 2 , 'c' : None } == IsIgnoreDict ( a = 1 , b = 2 ) assert { 'a' : 1 , 'b' : 2 , 'c' : None , 'c' : 'ignore' } == ( IsIgnoreDict ( a = 1 , b = 2 ) . settings ( ignore = { None , 'ignore' }) ) def is_even ( v : int ) -> bool : return v % 2 == 0 assert { 'a' : 1 , 'b' : 2 , 'c' : 3 , 'd' : 4 } == ( IsIgnoreDict ( a = 1 , c = 3 ) . settings ( ignore = is_even ) ) # combining partial and strict assert { 'a' : 1 , 'b' : None , 'c' : 3 } == IsIgnoreDict ( a = 1 , c = 3 ) . settings ( strict = True ) assert { 'b' : None , 'c' : 3 , 'a' : 1 } != IsIgnoreDict ( a = 1 , c = 3 ) . settings ( strict = True )","title":"IsIgnoreDict"},{"location":"types/dict/#dirty_equals.IsStrictDict","text":"Bases: IsDict Dictionary comparison with order enforced, this is the same as IsDict(...).settings(strict=True) . IsDict.settings(...) from dirty_equals import IsStrictDict assert { 'a' : 1 , 'b' : 2 } == IsStrictDict ( a = 1 , b = 2 ) assert { 'a' : 1 , 'b' : 2 , 'c' : 3 } != IsStrictDict ( a = 1 , b = 2 ) assert { 'b' : 2 , 'a' : 1 } != IsStrictDict ( a = 1 , b = 2 ) # combining partial and strict assert { 'a' : 1 , 'b' : None , 'c' : 3 } == IsStrictDict ( a = 1 , c = 3 ) . settings ( partial = True ) assert { 'b' : None , 'c' : 3 , 'a' : 1 } != IsStrictDict ( a = 1 , c = 3 ) . settings ( partial = True )","title":"IsStrictDict"},{"location":"types/inspection/","text":"Type Inspection \u00b6 IsInstance \u00b6 IsInstance ( expected_type : ExpectedType , * , only_direct_instance : bool = False ) Bases: DirtyEquals [ ExpectedType ] A type which checks that the value is an instance of the expected type. Parameters: Name Type Description Default expected_type ExpectedType The type to check against. required only_direct_instance bool whether instances of subclasses of expected_type should be considered equal. False Note IsInstance can be parameterized or initialised with a type - IsInstance[Foo] is exactly equivalent to IsInstance(Foo) . This allows usage to be analogous to type hints. Example: IsInstance from dirty_equals import IsInstance class Foo : pass class Bar ( Foo ): pass assert Foo () == IsInstance [ Foo ] assert Foo () == IsInstance ( Foo ) assert Foo != IsInstance [ Bar ] assert Bar () == IsInstance [ Foo ] assert Foo () == IsInstance ( Foo , only_direct_instance = True ) assert Bar () != IsInstance ( Foo , only_direct_instance = True ) HasName \u00b6 HasName ( expected_name : Union [ IsStr , str ], * , allow_instances : bool = True ) Bases: DirtyEquals [ T ] A type which checks that the value has the given __name__ attribute. Parameters: Name Type Description Default expected_name Union [ IsStr , str ] The name to check against. required allow_instances bool whether instances of classes with the given name should be considered equal, (e.g. whether other.__class__.__name__ == expected_name should be checked). True Example: HasName from dirty_equals import HasName , IsStr class Foo : pass assert Foo == HasName ( 'Foo' ) assert Foo == HasName [ 'Foo' ] assert Foo () == HasName ( 'Foo' ) assert Foo () != HasName ( 'Foo' , allow_instances = False ) assert Foo == HasName ( IsStr ( regex = 'F..' )) assert Foo != HasName ( 'Bar' ) assert int == HasName ( 'int' ) assert int == HasName ( 'int' ) HasRepr \u00b6 HasRepr ( expected_repr : Union [ IsStr , str ]) Bases: DirtyEquals [ T ] A type which checks that the value has the given repr() value. Parameters: Name Type Description Default expected_repr Union [ IsStr , str ] The expected repr value. required Example: HasRepr from dirty_equals import HasRepr , IsStr class Foo : def __repr__ ( self ): return 'This is a Foo' assert Foo () == HasRepr ( 'This is a Foo' ) assert Foo () == HasRepr [ 'This is a Foo' ] assert Foo == HasRepr ( IsStr ( regex = '<class.+' )) assert 42 == HasRepr ( '42' ) assert 43 != HasRepr ( '42' ) HasAttributes \u00b6 HasAttributes ( * expected_args : Dict [ Any , Any ], ** expected_kwargs : Any ) Bases: DirtyEquals [ Any ] A type which checks that the value has the given attributes. This is a partial check - e.g. the attributes provided to check do not need to be exhaustive. Can be created from either keyword arguments or an existing dictionary (same as dict() ). Example: HasAttributes from dirty_equals import HasAttributes , IsInt , IsStr , AnyThing class Foo : def __init__ ( self , a , b ): self . a = a self . b = b def spam ( self ): pass assert Foo ( 1 , 2 ) == HasAttributes ( a = 1 , b = 2 ) assert Foo ( 1 , 2 ) == HasAttributes ( a = 1 ) assert Foo ( 1 , 's' ) == HasAttributes ( a = IsInt , b = IsStr ) assert Foo ( 1 , 2 ) != HasAttributes ( a = IsInt , b = IsStr ) assert Foo ( 1 , 2 ) != HasAttributes ( a = 1 , b = 2 , c = 3 ) assert Foo ( 1 , 2 ) == HasAttributes ( a = 1 , b = 2 , spam = AnyThing )","title":"Type Inspection"},{"location":"types/inspection/#type-inspection","text":"","title":"Type Inspection"},{"location":"types/inspection/#dirty_equals.IsInstance","text":"IsInstance ( expected_type : ExpectedType , * , only_direct_instance : bool = False ) Bases: DirtyEquals [ ExpectedType ] A type which checks that the value is an instance of the expected type. Parameters: Name Type Description Default expected_type ExpectedType The type to check against. required only_direct_instance bool whether instances of subclasses of expected_type should be considered equal. False Note IsInstance can be parameterized or initialised with a type - IsInstance[Foo] is exactly equivalent to IsInstance(Foo) . This allows usage to be analogous to type hints. Example: IsInstance from dirty_equals import IsInstance class Foo : pass class Bar ( Foo ): pass assert Foo () == IsInstance [ Foo ] assert Foo () == IsInstance ( Foo ) assert Foo != IsInstance [ Bar ] assert Bar () == IsInstance [ Foo ] assert Foo () == IsInstance ( Foo , only_direct_instance = True ) assert Bar () != IsInstance ( Foo , only_direct_instance = True )","title":"IsInstance"},{"location":"types/inspection/#dirty_equals.HasName","text":"HasName ( expected_name : Union [ IsStr , str ], * , allow_instances : bool = True ) Bases: DirtyEquals [ T ] A type which checks that the value has the given __name__ attribute. Parameters: Name Type Description Default expected_name Union [ IsStr , str ] The name to check against. required allow_instances bool whether instances of classes with the given name should be considered equal, (e.g. whether other.__class__.__name__ == expected_name should be checked). True Example: HasName from dirty_equals import HasName , IsStr class Foo : pass assert Foo == HasName ( 'Foo' ) assert Foo == HasName [ 'Foo' ] assert Foo () == HasName ( 'Foo' ) assert Foo () != HasName ( 'Foo' , allow_instances = False ) assert Foo == HasName ( IsStr ( regex = 'F..' )) assert Foo != HasName ( 'Bar' ) assert int == HasName ( 'int' ) assert int == HasName ( 'int' )","title":"HasName"},{"location":"types/inspection/#dirty_equals.HasRepr","text":"HasRepr ( expected_repr : Union [ IsStr , str ]) Bases: DirtyEquals [ T ] A type which checks that the value has the given repr() value. Parameters: Name Type Description Default expected_repr Union [ IsStr , str ] The expected repr value. required Example: HasRepr from dirty_equals import HasRepr , IsStr class Foo : def __repr__ ( self ): return 'This is a Foo' assert Foo () == HasRepr ( 'This is a Foo' ) assert Foo () == HasRepr [ 'This is a Foo' ] assert Foo == HasRepr ( IsStr ( regex = '<class.+' )) assert 42 == HasRepr ( '42' ) assert 43 != HasRepr ( '42' )","title":"HasRepr"},{"location":"types/inspection/#dirty_equals.HasAttributes","text":"HasAttributes ( * expected_args : Dict [ Any , Any ], ** expected_kwargs : Any ) Bases: DirtyEquals [ Any ] A type which checks that the value has the given attributes. This is a partial check - e.g. the attributes provided to check do not need to be exhaustive. Can be created from either keyword arguments or an existing dictionary (same as dict() ). Example: HasAttributes from dirty_equals import HasAttributes , IsInt , IsStr , AnyThing class Foo : def __init__ ( self , a , b ): self . a = a self . b = b def spam ( self ): pass assert Foo ( 1 , 2 ) == HasAttributes ( a = 1 , b = 2 ) assert Foo ( 1 , 2 ) == HasAttributes ( a = 1 ) assert Foo ( 1 , 's' ) == HasAttributes ( a = IsInt , b = IsStr ) assert Foo ( 1 , 2 ) != HasAttributes ( a = IsInt , b = IsStr ) assert Foo ( 1 , 2 ) != HasAttributes ( a = 1 , b = 2 , c = 3 ) assert Foo ( 1 , 2 ) == HasAttributes ( a = 1 , b = 2 , spam = AnyThing )","title":"HasAttributes"},{"location":"types/numeric/","text":"Numeric Types \u00b6 IsInt \u00b6 Bases: IsNumeric [ int ] Checks that a value is an integer. Inherits from IsNumeric and can therefore be initialised with any of its arguments. IsInt from dirty_equals import IsInt assert 1 == IsInt assert - 2 == IsInt assert 1.0 != IsInt assert 'foobar' != IsInt assert True != IsInt allowed_types = int class-attribute \u00b6 As the name suggests, only integers are allowed, booleans ( True are False ) are explicitly excluded although technically they are sub-types of int . IsFloat \u00b6 Bases: IsNumeric [ float ] Checks that a value is a float. Inherits from IsNumeric and can therefore be initialised with any of its arguments. IsFloat from dirty_equals import IsFloat assert 1.0 == IsFloat assert 1 != IsFloat allowed_types = float class-attribute \u00b6 As the name suggests, only floats are allowed. IsPositive \u00b6 Bases: IsNumber Check that a value is positive ( > 0 ), can be an int , a float or a Decimal (or indeed any value which implements __gt__ for 0 ). IsPositive from decimal import Decimal from dirty_equals import IsPositive assert 1.0 == IsPositive assert 1 == IsPositive assert Decimal ( '3.14' ) == IsPositive assert 0 != IsPositive assert - 1 != IsPositive IsNegative \u00b6 Bases: IsNumber Check that a value is negative ( < 0 ), can be an int , a float or a Decimal (or indeed any value which implements __lt__ for 0 ). IsNegative from decimal import Decimal from dirty_equals import IsNegative assert - 1.0 == IsNegative assert - 1 == IsNegative assert Decimal ( '-3.14' ) == IsNegative assert 0 != IsNegative assert 1 != IsNegative IsNonNegative \u00b6 Bases: IsNumber Check that a value is positive or zero ( >= 0 ), can be an int , a float or a Decimal (or indeed any value which implements __ge__ for 0 ). IsNonNegative from decimal import Decimal from dirty_equals import IsNonNegative assert 1.0 == IsNonNegative assert 1 == IsNonNegative assert Decimal ( '3.14' ) == IsNonNegative assert 0 == IsNonNegative assert - 1 != IsNonNegative assert Decimal ( '0' ) == IsNonNegative IsNonPositive \u00b6 Bases: IsNumber Check that a value is negative or zero ( <=0 ), can be an int , a float or a Decimal (or indeed any value which implements __le__ for 0 ). IsNonPositive from decimal import Decimal from dirty_equals import IsNonPositive assert - 1.0 == IsNonPositive assert - 1 == IsNonPositive assert Decimal ( '-3.14' ) == IsNonPositive assert 0 == IsNonPositive assert 1 != IsNonPositive assert Decimal ( '-0' ) == IsNonPositive assert Decimal ( '0' ) == IsNonPositive IsPositiveInt \u00b6 Bases: IsInt Like IsPositive but only for int s. IsPositiveInt from decimal import Decimal from dirty_equals import IsPositiveInt assert 1 == IsPositiveInt assert 1.0 != IsPositiveInt assert Decimal ( '3.14' ) != IsPositiveInt assert 0 != IsPositiveInt assert - 1 != IsPositiveInt IsNegativeInt \u00b6 Bases: IsInt Like IsNegative but only for int s. IsNegativeInt from decimal import Decimal from dirty_equals import IsNegativeInt assert - 1 == IsNegativeInt assert - 1.0 != IsNegativeInt assert Decimal ( '-3.14' ) != IsNegativeInt assert 0 != IsNegativeInt assert 1 != IsNegativeInt IsPositiveFloat \u00b6 Bases: IsFloat Like IsPositive but only for float s. IsPositiveFloat from decimal import Decimal from dirty_equals import IsPositiveFloat assert 1.0 == IsPositiveFloat assert 1 != IsPositiveFloat assert Decimal ( '3.14' ) != IsPositiveFloat assert 0.0 != IsPositiveFloat assert - 1.0 != IsPositiveFloat IsNegativeFloat \u00b6 Bases: IsFloat Like IsNegative but only for float s. IsNegativeFloat from decimal import Decimal from dirty_equals import IsNegativeFloat assert - 1.0 == IsNegativeFloat assert - 1 != IsNegativeFloat assert Decimal ( '-3.14' ) != IsNegativeFloat assert 0.0 != IsNegativeFloat assert 1.0 != IsNegativeFloat IsApprox \u00b6 IsApprox ( approx : Num , * , delta : Optional [ Num ] = None ) Bases: IsNumber Simplified subclass of IsNumber that only allows approximate comparisons. Parameters: Name Type Description Default approx Num A value to approximately compare to. required delta Optional [ Num ] The allowable different when comparing to the value to approx , if omitted value / 100 is used. None IsApprox from dirty_equals import IsApprox assert 1.0 == IsApprox ( 1 ) assert 123 == IsApprox ( 120 , delta = 4 ) assert 201 == IsApprox ( 200 ) assert 201 != IsApprox ( 200 , delta = 0.1 ) IsNumber \u00b6 Bases: IsNumeric [ AnyNumber ] Base class for all types that can be used with all number types, e.g. numeric but not date or datetime . Inherits from IsNumeric and can therefore be initialised with any of its arguments. allowed_types class-attribute \u00b6 allowed_types = ( int , float , Decimal ) It allows any of the number types. IsNumeric \u00b6 IsNumeric ( * , approx : Optional [ N ] = None , delta : Optional [ N ] = None , gt : Optional [ N ] = None , lt : Optional [ N ] = None , ge : Optional [ N ] = None , le : Optional [ N ] = None ) Bases: DirtyEquals [ N ] Base class for all numeric types, IsNumeric implements approximate and inequality comparisons, as well as the type checks. This class can be used directly or via any of its subclasses. Parameters: Name Type Description Default approx Optional [ N ] A value to approximately compare to. None delta Optional [ N ] The allowable different when comparing to the value to approx , if omitted value / 100 is used except for datetimes where 2 seconds is used. None gt Optional [ N ] Value which the compared value should be greater than. None lt Optional [ N ] Value which the compared value should be less than. None ge Optional [ N ] Value which the compared value should be greater than or equal to. None le Optional [ N ] Value which the compared value should be less than or equal to. None If not values are provided, only the type is checked. If approx is provided as well a gt , lt , ge , or le , a TypeError is raised. Example of direct usage: IsNumeric from dirty_equals import IsNumeric from datetime import datetime assert 1.0 == IsNumeric assert 4 == IsNumeric ( gt = 3 ) d = datetime ( 2020 , 1 , 1 , 12 , 0 , 0 ) assert d == IsNumeric ( approx = datetime ( 2020 , 1 , 1 , 12 , 0 , 1 )) allowed_types class-attribute \u00b6 allowed_types : Union [ Type [ N ], Tuple [ type , ... ]] = ( int , float , Decimal , date , datetime , ) It allows any of the types supported in its subclasses.","title":"Numeric Types"},{"location":"types/numeric/#numeric-types","text":"","title":"Numeric Types"},{"location":"types/numeric/#dirty_equals.IsInt","text":"Bases: IsNumeric [ int ] Checks that a value is an integer. Inherits from IsNumeric and can therefore be initialised with any of its arguments. IsInt from dirty_equals import IsInt assert 1 == IsInt assert - 2 == IsInt assert 1.0 != IsInt assert 'foobar' != IsInt assert True != IsInt","title":"IsInt"},{"location":"types/numeric/#dirty_equals._numeric.IsInt.allowed_types","text":"As the name suggests, only integers are allowed, booleans ( True are False ) are explicitly excluded although technically they are sub-types of int .","title":"allowed_types"},{"location":"types/numeric/#dirty_equals.IsFloat","text":"Bases: IsNumeric [ float ] Checks that a value is a float. Inherits from IsNumeric and can therefore be initialised with any of its arguments. IsFloat from dirty_equals import IsFloat assert 1.0 == IsFloat assert 1 != IsFloat","title":"IsFloat"},{"location":"types/numeric/#dirty_equals._numeric.IsFloat.allowed_types","text":"As the name suggests, only floats are allowed.","title":"allowed_types"},{"location":"types/numeric/#dirty_equals.IsPositive","text":"Bases: IsNumber Check that a value is positive ( > 0 ), can be an int , a float or a Decimal (or indeed any value which implements __gt__ for 0 ). IsPositive from decimal import Decimal from dirty_equals import IsPositive assert 1.0 == IsPositive assert 1 == IsPositive assert Decimal ( '3.14' ) == IsPositive assert 0 != IsPositive assert - 1 != IsPositive","title":"IsPositive"},{"location":"types/numeric/#dirty_equals.IsNegative","text":"Bases: IsNumber Check that a value is negative ( < 0 ), can be an int , a float or a Decimal (or indeed any value which implements __lt__ for 0 ). IsNegative from decimal import Decimal from dirty_equals import IsNegative assert - 1.0 == IsNegative assert - 1 == IsNegative assert Decimal ( '-3.14' ) == IsNegative assert 0 != IsNegative assert 1 != IsNegative","title":"IsNegative"},{"location":"types/numeric/#dirty_equals.IsNonNegative","text":"Bases: IsNumber Check that a value is positive or zero ( >= 0 ), can be an int , a float or a Decimal (or indeed any value which implements __ge__ for 0 ). IsNonNegative from decimal import Decimal from dirty_equals import IsNonNegative assert 1.0 == IsNonNegative assert 1 == IsNonNegative assert Decimal ( '3.14' ) == IsNonNegative assert 0 == IsNonNegative assert - 1 != IsNonNegative assert Decimal ( '0' ) == IsNonNegative","title":"IsNonNegative"},{"location":"types/numeric/#dirty_equals.IsNonPositive","text":"Bases: IsNumber Check that a value is negative or zero ( <=0 ), can be an int , a float or a Decimal (or indeed any value which implements __le__ for 0 ). IsNonPositive from decimal import Decimal from dirty_equals import IsNonPositive assert - 1.0 == IsNonPositive assert - 1 == IsNonPositive assert Decimal ( '-3.14' ) == IsNonPositive assert 0 == IsNonPositive assert 1 != IsNonPositive assert Decimal ( '-0' ) == IsNonPositive assert Decimal ( '0' ) == IsNonPositive","title":"IsNonPositive"},{"location":"types/numeric/#dirty_equals.IsPositiveInt","text":"Bases: IsInt Like IsPositive but only for int s. IsPositiveInt from decimal import Decimal from dirty_equals import IsPositiveInt assert 1 == IsPositiveInt assert 1.0 != IsPositiveInt assert Decimal ( '3.14' ) != IsPositiveInt assert 0 != IsPositiveInt assert - 1 != IsPositiveInt","title":"IsPositiveInt"},{"location":"types/numeric/#dirty_equals.IsNegativeInt","text":"Bases: IsInt Like IsNegative but only for int s. IsNegativeInt from decimal import Decimal from dirty_equals import IsNegativeInt assert - 1 == IsNegativeInt assert - 1.0 != IsNegativeInt assert Decimal ( '-3.14' ) != IsNegativeInt assert 0 != IsNegativeInt assert 1 != IsNegativeInt","title":"IsNegativeInt"},{"location":"types/numeric/#dirty_equals.IsPositiveFloat","text":"Bases: IsFloat Like IsPositive but only for float s. IsPositiveFloat from decimal import Decimal from dirty_equals import IsPositiveFloat assert 1.0 == IsPositiveFloat assert 1 != IsPositiveFloat assert Decimal ( '3.14' ) != IsPositiveFloat assert 0.0 != IsPositiveFloat assert - 1.0 != IsPositiveFloat","title":"IsPositiveFloat"},{"location":"types/numeric/#dirty_equals.IsNegativeFloat","text":"Bases: IsFloat Like IsNegative but only for float s. IsNegativeFloat from decimal import Decimal from dirty_equals import IsNegativeFloat assert - 1.0 == IsNegativeFloat assert - 1 != IsNegativeFloat assert Decimal ( '-3.14' ) != IsNegativeFloat assert 0.0 != IsNegativeFloat assert 1.0 != IsNegativeFloat","title":"IsNegativeFloat"},{"location":"types/numeric/#dirty_equals.IsApprox","text":"IsApprox ( approx : Num , * , delta : Optional [ Num ] = None ) Bases: IsNumber Simplified subclass of IsNumber that only allows approximate comparisons. Parameters: Name Type Description Default approx Num A value to approximately compare to. required delta Optional [ Num ] The allowable different when comparing to the value to approx , if omitted value / 100 is used. None IsApprox from dirty_equals import IsApprox assert 1.0 == IsApprox ( 1 ) assert 123 == IsApprox ( 120 , delta = 4 ) assert 201 == IsApprox ( 200 ) assert 201 != IsApprox ( 200 , delta = 0.1 )","title":"IsApprox"},{"location":"types/numeric/#dirty_equals.IsNumber","text":"Bases: IsNumeric [ AnyNumber ] Base class for all types that can be used with all number types, e.g. numeric but not date or datetime . Inherits from IsNumeric and can therefore be initialised with any of its arguments.","title":"IsNumber"},{"location":"types/numeric/#dirty_equals._numeric.IsNumber.allowed_types","text":"allowed_types = ( int , float , Decimal ) It allows any of the number types.","title":"allowed_types"},{"location":"types/numeric/#dirty_equals.IsNumeric","text":"IsNumeric ( * , approx : Optional [ N ] = None , delta : Optional [ N ] = None , gt : Optional [ N ] = None , lt : Optional [ N ] = None , ge : Optional [ N ] = None , le : Optional [ N ] = None ) Bases: DirtyEquals [ N ] Base class for all numeric types, IsNumeric implements approximate and inequality comparisons, as well as the type checks. This class can be used directly or via any of its subclasses. Parameters: Name Type Description Default approx Optional [ N ] A value to approximately compare to. None delta Optional [ N ] The allowable different when comparing to the value to approx , if omitted value / 100 is used except for datetimes where 2 seconds is used. None gt Optional [ N ] Value which the compared value should be greater than. None lt Optional [ N ] Value which the compared value should be less than. None ge Optional [ N ] Value which the compared value should be greater than or equal to. None le Optional [ N ] Value which the compared value should be less than or equal to. None If not values are provided, only the type is checked. If approx is provided as well a gt , lt , ge , or le , a TypeError is raised. Example of direct usage: IsNumeric from dirty_equals import IsNumeric from datetime import datetime assert 1.0 == IsNumeric assert 4 == IsNumeric ( gt = 3 ) d = datetime ( 2020 , 1 , 1 , 12 , 0 , 0 ) assert d == IsNumeric ( approx = datetime ( 2020 , 1 , 1 , 12 , 0 , 1 ))","title":"IsNumeric"},{"location":"types/numeric/#dirty_equals._numeric.IsNumeric.allowed_types","text":"allowed_types : Union [ Type [ N ], Tuple [ type , ... ]] = ( int , float , Decimal , date , datetime , ) It allows any of the types supported in its subclasses.","title":"allowed_types"},{"location":"types/other/","text":"Other Types \u00b6 FunctionCheck \u00b6 FunctionCheck ( func : Callable [[ Any ], bool ]) Bases: DirtyEquals [ Any ] Use a function to check if a value \"equals\" whatever you want to check Parameters: Name Type Description Default func Callable [[ Any ], bool ] callable that takes a value and returns a bool. required FunctionCheck from dirty_equals import FunctionCheck def is_even ( x ): return x % 2 == 0 assert 2 == FunctionCheck ( is_even ) assert 3 != FunctionCheck ( is_even ) IsInstance \u00b6 IsInstance ( expected_type : ExpectedType , * , only_direct_instance : bool = False ) Bases: DirtyEquals [ ExpectedType ] A type which checks that the value is an instance of the expected type. Parameters: Name Type Description Default expected_type ExpectedType The type to check against. required only_direct_instance bool whether instances of subclasses of expected_type should be considered equal. False Note IsInstance can be parameterized or initialised with a type - IsInstance[Foo] is exactly equivalent to IsInstance(Foo) . This allows usage to be analogous to type hints. Example: IsInstance from dirty_equals import IsInstance class Foo : pass class Bar ( Foo ): pass assert Foo () == IsInstance [ Foo ] assert Foo () == IsInstance ( Foo ) assert Foo != IsInstance [ Bar ] assert Bar () == IsInstance [ Foo ] assert Foo () == IsInstance ( Foo , only_direct_instance = True ) assert Bar () != IsInstance ( Foo , only_direct_instance = True ) IsJson \u00b6 IsJson ( expected_value : JsonType = AnyJson , ** expected_kwargs : Any ) Bases: DirtyEquals [ JsonType ] A class that checks if a value is a JSON object, and check the contents of the JSON. Parameters: Name Type Description Default expected_value JsonType Value to compare the JSON to, if omitted, any JSON is accepted. AnyJson **expected_kwargs Any Keyword arguments forming a dict to compare the JSON to, expected_value and expected_kwargs may not be combined. {} As with any dirty_equals type, types can be nested to provide more complex checks. Note Like IsInstance , IsJson can be parameterized or initialised with a value - IsJson[xyz] is exactly equivalent to IsJson(xyz) . This allows usage to be analogous to type hints. IsJson from dirty_equals import IsJson , IsStrictDict , IsPositiveInt assert '{\"a\": 1, \"b\": 2}' == IsJson assert '{\"a\": 1, \"b\": 2}' == IsJson ( a = 1 , b = 2 ) assert '{\"a\": 1}' != IsJson ( a = 2 ) assert 'invalid json' != IsJson assert '{\"a\": 1}' == IsJson ( a = IsPositiveInt ) assert '\"just a quoted string\"' == IsJson ( 'just a quoted string' ) assert '{\"a\": 1, \"b\": 2}' == IsJson [ IsStrictDict ( a = 1 , b = 2 )] assert '{\"b\": 2, \"a\": 1}' != IsJson [ IsStrictDict ( a = 1 , b = 2 )] IsUUID \u00b6 IsUUID ( version : Literal [ None , 1 , 2 , 3 , 4 , 5 ] = None ) Bases: DirtyEquals [ UUID ] A class that checks if a value is a valid UUID, optionally checking UUID version. Parameters: Name Type Description Default version Literal [None, 1, 2, 3, 4, 5] The version of the UUID to check, if omitted, all versions are accepted. None IsUUID import uuid from dirty_equals import IsUUID assert 'edf9f29e-45c7-431c-99db-28ea44df9785' == IsUUID assert 'edf9f29e-45c7-431c-99db-28ea44df9785' == IsUUID ( 4 ) assert 'edf9f29e45c7431c99db28ea44df9785' == IsUUID ( 4 ) assert 'edf9f29e-45c7-431c-99db-28ea44df9785' != IsUUID ( 5 ) assert uuid . uuid4 () == IsUUID ( 4 ) AnyThing \u00b6 Bases: DirtyEquals [ Any ] A type which matches any value. AnyThing isn't generally very useful on its own, but can be used within other comparisons. AnyThing from dirty_equals import AnyThing , IsList , IsStrictDict assert 1 == AnyThing assert 'foobar' == AnyThing assert [ 1 , 2 , 3 ] == AnyThing assert [ 1 , 2 , 3 ] == IsList ( AnyThing , 2 , 3 ) assert { 'a' : 1 , 'b' : 2 , 'c' : 3 } == IsStrictDict ( a = 1 , b = AnyThing , c = 3 ) IsOneOf \u00b6 IsOneOf ( expected_value : Any , * more_expected_values : Any ) Bases: DirtyEquals [ Any ] A type which checks that the value is equal to one of the given values. Can be useful with boolean operators. Parameters: Name Type Description Default expected_value Any Expected value for equals to return true. required *more_expected_values Any More expected values for equals to return true. () IsOneOf from dirty_equals import IsOneOf , Contains assert 1 == IsOneOf ( 1 , 2 , 3 ) assert 4 != IsOneOf ( 1 , 2 , 3 ) # check that a list either contain 1 or is empty assert [ 1 , 2 , 3 ] == Contains ( 1 ) | IsOneOf ([]) assert [] == Contains ( 1 ) | IsOneOf ([]) IsUrl \u00b6 IsUrl ( any_url : bool = False , any_http_url : bool = False , http_url : bool = False , file_url : bool = False , postgres_dsn : bool = False , ampqp_dsn : bool = False , redis_dsn : bool = False , ** expected_attributes : Any ) Bases: DirtyEquals [ str ] A class that checks if a value is a valid URL, optionally checking different URL types and attributes with Pydantic . Parameters: Name Type Description Default any_url bool any scheme allowed, TLD not required, host required False any_http_url bool scheme http or https, TLD not required, host required False http_url bool scheme http or https, TLD required, host required, max length 2083 False file_url bool scheme file, host not required False postgres_dsn bool user info required, TLD not required False ampqp_dsn bool schema amqp or amqps, user info not required, TLD not required, host not required False redis_dsn bool scheme redis or rediss, user info not required, tld not required, host not required False **expected_attributes Any Expected values for url attributes {} IsUrl from dirty_equals import IsUrl assert 'https://example.com' == IsUrl assert 'https://example.com' == IsUrl ( tld = 'com' ) assert 'https://example.com' == IsUrl ( scheme = 'https' ) assert 'https://example.com' != IsUrl ( scheme = 'http' ) assert 'postgres://user:pass@localhost:5432/app' == IsUrl ( postgres_dsn = True ) assert 'postgres://user:pass@localhost:5432/app' != IsUrl ( http_url = True ) IsHash \u00b6 IsHash ( hash_type : HashTypes ) Bases: DirtyEquals [ str ] A class that checks if a value is a valid common hash type, using a simple length and allowed characters regex. Parameters: Name Type Description Default hash_type HashTypes The hash type to check. Must be specified. required IsHash from dirty_equals import IsHash assert 'f1e069787ece74531d112559945c6871' == IsHash ( 'md5' ) assert b 'f1e069787ece74531d112559945c6871' == IsHash ( 'md5' ) assert 'f1e069787ece74531d112559945c6871' != IsHash ( 'sha-256' ) assert 'F1E069787ECE74531D112559945C6871' == IsHash ( 'md5' ) assert '40bd001563085fc35165329ea1ff5c5ecbdbbeef' == IsHash ( 'sha-1' ) assert 'a665a45920422f9d417e4867efdc4fb8a04a1f3fff1fa07e998e86f7f7a27ae3' == IsHash ( 'sha-256' ) IsIP \u00b6 IsIP ( * , version : Literal [ None , 4 , 6 ] = None , netmask : Optional [ str ] = None ) Bases: DirtyEquals [ IP ] A class that checks if a value is a valid IP address, optionally checking IP version, netmask. Parameters: Name Type Description Default version Literal [None, 4, 6] The version of the IP to check, if omitted, versions 4 and 6 are both accepted. None netmask Optional [ str ] The netmask of the IP to check, if omitted, any netmask is accepted. Requires version. None IsIP from ipaddress import IPv4Address , IPv6Address , IPv4Network from dirty_equals import IsIP assert '179.27.154.96' == IsIP assert '179.27.154.96' == IsIP ( version = 4 ) assert '2001:0db8:0a0b:12f0:0000:0000:0000:0001' == IsIP ( version = 6 ) assert IPv4Address ( '127.0.0.1' ) == IsIP assert IPv4Network ( '43.48.0.0/12' ) == IsIP assert IPv6Address ( '::eeff:ae3f:d473' ) == IsIP assert '54.43.53.219/10' == IsIP ( version = 4 , netmask = '255.192.0.0' ) assert '54.43.53.219/10' == IsIP ( version = 4 , netmask = 4290772992 ) assert '::ffff:aebf:d473/12' == IsIP ( version = 6 , netmask = 'fff0::' ) assert 3232235521 == IsIP","title":"Other Types"},{"location":"types/other/#other-types","text":"","title":"Other Types"},{"location":"types/other/#dirty_equals.FunctionCheck","text":"FunctionCheck ( func : Callable [[ Any ], bool ]) Bases: DirtyEquals [ Any ] Use a function to check if a value \"equals\" whatever you want to check Parameters: Name Type Description Default func Callable [[ Any ], bool ] callable that takes a value and returns a bool. required FunctionCheck from dirty_equals import FunctionCheck def is_even ( x ): return x % 2 == 0 assert 2 == FunctionCheck ( is_even ) assert 3 != FunctionCheck ( is_even )","title":"FunctionCheck"},{"location":"types/other/#dirty_equals.IsInstance","text":"IsInstance ( expected_type : ExpectedType , * , only_direct_instance : bool = False ) Bases: DirtyEquals [ ExpectedType ] A type which checks that the value is an instance of the expected type. Parameters: Name Type Description Default expected_type ExpectedType The type to check against. required only_direct_instance bool whether instances of subclasses of expected_type should be considered equal. False Note IsInstance can be parameterized or initialised with a type - IsInstance[Foo] is exactly equivalent to IsInstance(Foo) . This allows usage to be analogous to type hints. Example: IsInstance from dirty_equals import IsInstance class Foo : pass class Bar ( Foo ): pass assert Foo () == IsInstance [ Foo ] assert Foo () == IsInstance ( Foo ) assert Foo != IsInstance [ Bar ] assert Bar () == IsInstance [ Foo ] assert Foo () == IsInstance ( Foo , only_direct_instance = True ) assert Bar () != IsInstance ( Foo , only_direct_instance = True )","title":"IsInstance"},{"location":"types/other/#dirty_equals.IsJson","text":"IsJson ( expected_value : JsonType = AnyJson , ** expected_kwargs : Any ) Bases: DirtyEquals [ JsonType ] A class that checks if a value is a JSON object, and check the contents of the JSON. Parameters: Name Type Description Default expected_value JsonType Value to compare the JSON to, if omitted, any JSON is accepted. AnyJson **expected_kwargs Any Keyword arguments forming a dict to compare the JSON to, expected_value and expected_kwargs may not be combined. {} As with any dirty_equals type, types can be nested to provide more complex checks. Note Like IsInstance , IsJson can be parameterized or initialised with a value - IsJson[xyz] is exactly equivalent to IsJson(xyz) . This allows usage to be analogous to type hints. IsJson from dirty_equals import IsJson , IsStrictDict , IsPositiveInt assert '{\"a\": 1, \"b\": 2}' == IsJson assert '{\"a\": 1, \"b\": 2}' == IsJson ( a = 1 , b = 2 ) assert '{\"a\": 1}' != IsJson ( a = 2 ) assert 'invalid json' != IsJson assert '{\"a\": 1}' == IsJson ( a = IsPositiveInt ) assert '\"just a quoted string\"' == IsJson ( 'just a quoted string' ) assert '{\"a\": 1, \"b\": 2}' == IsJson [ IsStrictDict ( a = 1 , b = 2 )] assert '{\"b\": 2, \"a\": 1}' != IsJson [ IsStrictDict ( a = 1 , b = 2 )]","title":"IsJson"},{"location":"types/other/#dirty_equals.IsUUID","text":"IsUUID ( version : Literal [ None , 1 , 2 , 3 , 4 , 5 ] = None ) Bases: DirtyEquals [ UUID ] A class that checks if a value is a valid UUID, optionally checking UUID version. Parameters: Name Type Description Default version Literal [None, 1, 2, 3, 4, 5] The version of the UUID to check, if omitted, all versions are accepted. None IsUUID import uuid from dirty_equals import IsUUID assert 'edf9f29e-45c7-431c-99db-28ea44df9785' == IsUUID assert 'edf9f29e-45c7-431c-99db-28ea44df9785' == IsUUID ( 4 ) assert 'edf9f29e45c7431c99db28ea44df9785' == IsUUID ( 4 ) assert 'edf9f29e-45c7-431c-99db-28ea44df9785' != IsUUID ( 5 ) assert uuid . uuid4 () == IsUUID ( 4 )","title":"IsUUID"},{"location":"types/other/#dirty_equals.AnyThing","text":"Bases: DirtyEquals [ Any ] A type which matches any value. AnyThing isn't generally very useful on its own, but can be used within other comparisons. AnyThing from dirty_equals import AnyThing , IsList , IsStrictDict assert 1 == AnyThing assert 'foobar' == AnyThing assert [ 1 , 2 , 3 ] == AnyThing assert [ 1 , 2 , 3 ] == IsList ( AnyThing , 2 , 3 ) assert { 'a' : 1 , 'b' : 2 , 'c' : 3 } == IsStrictDict ( a = 1 , b = AnyThing , c = 3 )","title":"AnyThing"},{"location":"types/other/#dirty_equals.IsOneOf","text":"IsOneOf ( expected_value : Any , * more_expected_values : Any ) Bases: DirtyEquals [ Any ] A type which checks that the value is equal to one of the given values. Can be useful with boolean operators. Parameters: Name Type Description Default expected_value Any Expected value for equals to return true. required *more_expected_values Any More expected values for equals to return true. () IsOneOf from dirty_equals import IsOneOf , Contains assert 1 == IsOneOf ( 1 , 2 , 3 ) assert 4 != IsOneOf ( 1 , 2 , 3 ) # check that a list either contain 1 or is empty assert [ 1 , 2 , 3 ] == Contains ( 1 ) | IsOneOf ([]) assert [] == Contains ( 1 ) | IsOneOf ([])","title":"IsOneOf"},{"location":"types/other/#dirty_equals.IsUrl","text":"IsUrl ( any_url : bool = False , any_http_url : bool = False , http_url : bool = False , file_url : bool = False , postgres_dsn : bool = False , ampqp_dsn : bool = False , redis_dsn : bool = False , ** expected_attributes : Any ) Bases: DirtyEquals [ str ] A class that checks if a value is a valid URL, optionally checking different URL types and attributes with Pydantic . Parameters: Name Type Description Default any_url bool any scheme allowed, TLD not required, host required False any_http_url bool scheme http or https, TLD not required, host required False http_url bool scheme http or https, TLD required, host required, max length 2083 False file_url bool scheme file, host not required False postgres_dsn bool user info required, TLD not required False ampqp_dsn bool schema amqp or amqps, user info not required, TLD not required, host not required False redis_dsn bool scheme redis or rediss, user info not required, tld not required, host not required False **expected_attributes Any Expected values for url attributes {} IsUrl from dirty_equals import IsUrl assert 'https://example.com' == IsUrl assert 'https://example.com' == IsUrl ( tld = 'com' ) assert 'https://example.com' == IsUrl ( scheme = 'https' ) assert 'https://example.com' != IsUrl ( scheme = 'http' ) assert 'postgres://user:pass@localhost:5432/app' == IsUrl ( postgres_dsn = True ) assert 'postgres://user:pass@localhost:5432/app' != IsUrl ( http_url = True )","title":"IsUrl"},{"location":"types/other/#dirty_equals.IsHash","text":"IsHash ( hash_type : HashTypes ) Bases: DirtyEquals [ str ] A class that checks if a value is a valid common hash type, using a simple length and allowed characters regex. Parameters: Name Type Description Default hash_type HashTypes The hash type to check. Must be specified. required IsHash from dirty_equals import IsHash assert 'f1e069787ece74531d112559945c6871' == IsHash ( 'md5' ) assert b 'f1e069787ece74531d112559945c6871' == IsHash ( 'md5' ) assert 'f1e069787ece74531d112559945c6871' != IsHash ( 'sha-256' ) assert 'F1E069787ECE74531D112559945C6871' == IsHash ( 'md5' ) assert '40bd001563085fc35165329ea1ff5c5ecbdbbeef' == IsHash ( 'sha-1' ) assert 'a665a45920422f9d417e4867efdc4fb8a04a1f3fff1fa07e998e86f7f7a27ae3' == IsHash ( 'sha-256' )","title":"IsHash"},{"location":"types/other/#dirty_equals.IsIP","text":"IsIP ( * , version : Literal [ None , 4 , 6 ] = None , netmask : Optional [ str ] = None ) Bases: DirtyEquals [ IP ] A class that checks if a value is a valid IP address, optionally checking IP version, netmask. Parameters: Name Type Description Default version Literal [None, 4, 6] The version of the IP to check, if omitted, versions 4 and 6 are both accepted. None netmask Optional [ str ] The netmask of the IP to check, if omitted, any netmask is accepted. Requires version. None IsIP from ipaddress import IPv4Address , IPv6Address , IPv4Network from dirty_equals import IsIP assert '179.27.154.96' == IsIP assert '179.27.154.96' == IsIP ( version = 4 ) assert '2001:0db8:0a0b:12f0:0000:0000:0000:0001' == IsIP ( version = 6 ) assert IPv4Address ( '127.0.0.1' ) == IsIP assert IPv4Network ( '43.48.0.0/12' ) == IsIP assert IPv6Address ( '::eeff:ae3f:d473' ) == IsIP assert '54.43.53.219/10' == IsIP ( version = 4 , netmask = '255.192.0.0' ) assert '54.43.53.219/10' == IsIP ( version = 4 , netmask = 4290772992 ) assert '::ffff:aebf:d473/12' == IsIP ( version = 6 , netmask = 'fff0::' ) assert 3232235521 == IsIP","title":"IsIP"},{"location":"types/sequence/","text":"Sequence Types \u00b6 IsListOrTuple \u00b6 IsListOrTuple ( * items : Any , positions : Optional [ Dict [ int , Any ]] = None , check_order : bool = True , length : LengthType = None ) Bases: DirtyEquals [ T ] Check that some object is a list or tuple and optionally its values match some constraints. IsListOrTuple and its subclasses can be initialised in two ways: Parameters: Name Type Description Default *items Any Positional members of an object to check. These must start from the zeroth position, but (depending on the value of length ) may not include all values of the list/tuple being checked. () check_order bool Whether to enforce the order of the items. True length Union [ int , Tuple [ int , Union [ int , Any ]]] length constraints, int or tuple matching the arguments of HasLen . None or, Parameters: Name Type Description Default positions Dict [ int , Any ] Instead of *items , a dictionary of positions and values to check and be provided. None length Union [ int , Tuple [ int , Union [ int , Any ]]] length constraints, int or tuple matching the arguments of HasLen . None IsListOrTuple from dirty_equals import IsListOrTuple , AnyThing assert [ 1 , 2 , 3 ] == IsListOrTuple ( 1 , 2 , 3 ) assert ( 1 , 3 , 2 ) == IsListOrTuple ( 1 , 2 , 3 , check_order = False ) assert [{ 'a' : 1 }, { 'a' : 2 }] == ( IsListOrTuple ({ 'a' : 2 }, { 'a' : 1 }, check_order = False ) #(1)! ) assert [ 1 , 2 , 3 , 3 ] != IsListOrTuple ( 1 , 2 , 3 , check_order = False ) #(2)! assert [ 1 , 2 , 3 , 4 , 5 ] == IsListOrTuple ( 1 , 2 , 3 , length =... ) #(3)! assert [ 1 , 2 , 3 , 4 , 5 ] != IsListOrTuple ( 1 , 2 , 3 , length = ( 8 , 10 )) #(4)! assert [ 'a' , 'b' , 'c' , 'd' ] == ( IsListOrTuple ( positions = { 2 : 'c' , 3 : 'd' }) #(5)! ) assert [ 'a' , 'b' , 'c' , 'd' ] == ( IsListOrTuple ( positions = { 2 : 'c' , 3 : 'd' }, length = 4 ) #(6)! ) assert [ 1 , 2 , 3 , 4 ] == IsListOrTuple ( 3 , check_order = False , length = ( 0 , ... )) #(7)! assert [ 1 , 2 , 3 ] == IsListOrTuple ( AnyThing , AnyThing , 3 ) #(8)! Unlike using sets for comparison, we can do order-insensitive comparisons on objects that are not hashable. And we won't get caught out by duplicate values Here we're just checking the first 3 items, the compared list or tuple can be of any length Compared list is not long enough Compare using positions , here no length if enforced Compare using positions but with a length constraint Here we're just confirming that the value 3 is in the list If you don't care about the first few values of a list or tuple, you can use AnyThing in your arguments. IsList \u00b6 Bases: IsListOrTuple [ List [ Any ]] All the same functionality as IsListOrTuple , but the compared value must be a list. IsList from dirty_equals import IsList assert [ 1 , 2 , 3 ] == IsList ( 1 , 2 , 3 ) assert [ 1 , 2 , 3 ] == IsList ( positions = { 2 : 3 }) assert [ 1 , 2 , 3 ] == IsList ( 1 , 2 , 3 , check_order = False ) assert [ 1 , 2 , 3 , 4 ] == IsList ( 1 , 2 , 3 , length = 4 ) assert [ 1 , 2 , 3 , 4 ] == IsList ( 1 , 2 , 3 , length = ( 4 , 5 )) assert [ 1 , 2 , 3 , 4 ] == IsList ( 1 , 2 , 3 , length =... ) assert ( 1 , 2 , 3 ) != IsList ( 1 , 2 , 3 ) IsTuple \u00b6 Bases: IsListOrTuple [ Tuple [ Any , ...]] All the same functionality as IsListOrTuple , but the compared value must be a tuple. IsTuple from dirty_equals import IsTuple assert ( 1 , 2 , 3 ) == IsTuple ( 1 , 2 , 3 ) assert ( 1 , 2 , 3 ) == IsTuple ( positions = { 2 : 3 }) assert ( 1 , 2 , 3 ) == IsTuple ( 1 , 2 , 3 , check_order = False ) assert ( 1 , 2 , 3 , 4 ) == IsTuple ( 1 , 2 , 3 , length = 4 ) assert ( 1 , 2 , 3 , 4 ) == IsTuple ( 1 , 2 , 3 , length = ( 4 , 5 )) assert ( 1 , 2 , 3 , 4 ) == IsTuple ( 1 , 2 , 3 , length =... ) assert [ 1 , 2 , 3 ] != IsTuple ( 1 , 2 , 3 ) HasLen \u00b6 HasLen ( min_length : int , max_length : Union [ None , int , Any ] = None , ) Bases: DirtyEquals [ Sized ] Check that some has a given length, or length in a given range. Parameters: Name Type Description Default min_length int Expected length if max_length is not given, else minimum length. required max_length Union [None, int , Any ] Expected maximum length, use an ellipsis ... to indicate that there's no maximum. None HasLen from dirty_equals import HasLen assert [ 1 , 2 , 3 ] == HasLen ( 3 ) #(1)! assert '123' == HasLen ( 3 , ... ) #(2)! assert ( 1 , 2 , 3 ) == HasLen ( 3 , 5 ) #(3)! assert ( 1 , 2 , 3 ) == HasLen ( 0 , ... ) #(4)! Length must be 3. Length must be 3 or higher. Length must be between 3 and 5 inclusive. Length is required but can take any value. Contains \u00b6 Contains ( contained_value : Any , * more_contained_values : Any ) Bases: DirtyEquals [ Container [ Any ]] Check that an object contains one or more values. Parameters: Name Type Description Default contained_value Any value that must be contained in the compared object. required *more_contained_values Any more values that must be contained in the compared object. () Contains from dirty_equals import Contains assert [ 1 , 2 , 3 ] == Contains ( 1 ) assert [ 1 , 2 , 3 ] == Contains ( 1 , 2 ) assert ( 1 , 2 , 3 ) == Contains ( 1 ) assert 'abc' == Contains ( 'b' ) assert { 'a' : 1 , 'b' : 2 } == Contains ( 'a' ) assert [ 1 , 2 , 3 ] != Contains ( 10 )","title":"Sequence Types"},{"location":"types/sequence/#sequence-types","text":"","title":"Sequence Types"},{"location":"types/sequence/#dirty_equals.IsListOrTuple","text":"IsListOrTuple ( * items : Any , positions : Optional [ Dict [ int , Any ]] = None , check_order : bool = True , length : LengthType = None ) Bases: DirtyEquals [ T ] Check that some object is a list or tuple and optionally its values match some constraints. IsListOrTuple and its subclasses can be initialised in two ways: Parameters: Name Type Description Default *items Any Positional members of an object to check. These must start from the zeroth position, but (depending on the value of length ) may not include all values of the list/tuple being checked. () check_order bool Whether to enforce the order of the items. True length Union [ int , Tuple [ int , Union [ int , Any ]]] length constraints, int or tuple matching the arguments of HasLen . None or, Parameters: Name Type Description Default positions Dict [ int , Any ] Instead of *items , a dictionary of positions and values to check and be provided. None length Union [ int , Tuple [ int , Union [ int , Any ]]] length constraints, int or tuple matching the arguments of HasLen . None IsListOrTuple from dirty_equals import IsListOrTuple , AnyThing assert [ 1 , 2 , 3 ] == IsListOrTuple ( 1 , 2 , 3 ) assert ( 1 , 3 , 2 ) == IsListOrTuple ( 1 , 2 , 3 , check_order = False ) assert [{ 'a' : 1 }, { 'a' : 2 }] == ( IsListOrTuple ({ 'a' : 2 }, { 'a' : 1 }, check_order = False ) #(1)! ) assert [ 1 , 2 , 3 , 3 ] != IsListOrTuple ( 1 , 2 , 3 , check_order = False ) #(2)! assert [ 1 , 2 , 3 , 4 , 5 ] == IsListOrTuple ( 1 , 2 , 3 , length =... ) #(3)! assert [ 1 , 2 , 3 , 4 , 5 ] != IsListOrTuple ( 1 , 2 , 3 , length = ( 8 , 10 )) #(4)! assert [ 'a' , 'b' , 'c' , 'd' ] == ( IsListOrTuple ( positions = { 2 : 'c' , 3 : 'd' }) #(5)! ) assert [ 'a' , 'b' , 'c' , 'd' ] == ( IsListOrTuple ( positions = { 2 : 'c' , 3 : 'd' }, length = 4 ) #(6)! ) assert [ 1 , 2 , 3 , 4 ] == IsListOrTuple ( 3 , check_order = False , length = ( 0 , ... )) #(7)! assert [ 1 , 2 , 3 ] == IsListOrTuple ( AnyThing , AnyThing , 3 ) #(8)! Unlike using sets for comparison, we can do order-insensitive comparisons on objects that are not hashable. And we won't get caught out by duplicate values Here we're just checking the first 3 items, the compared list or tuple can be of any length Compared list is not long enough Compare using positions , here no length if enforced Compare using positions but with a length constraint Here we're just confirming that the value 3 is in the list If you don't care about the first few values of a list or tuple, you can use AnyThing in your arguments.","title":"IsListOrTuple"},{"location":"types/sequence/#dirty_equals.IsList","text":"Bases: IsListOrTuple [ List [ Any ]] All the same functionality as IsListOrTuple , but the compared value must be a list. IsList from dirty_equals import IsList assert [ 1 , 2 , 3 ] == IsList ( 1 , 2 , 3 ) assert [ 1 , 2 , 3 ] == IsList ( positions = { 2 : 3 }) assert [ 1 , 2 , 3 ] == IsList ( 1 , 2 , 3 , check_order = False ) assert [ 1 , 2 , 3 , 4 ] == IsList ( 1 , 2 , 3 , length = 4 ) assert [ 1 , 2 , 3 , 4 ] == IsList ( 1 , 2 , 3 , length = ( 4 , 5 )) assert [ 1 , 2 , 3 , 4 ] == IsList ( 1 , 2 , 3 , length =... ) assert ( 1 , 2 , 3 ) != IsList ( 1 , 2 , 3 )","title":"IsList"},{"location":"types/sequence/#dirty_equals.IsTuple","text":"Bases: IsListOrTuple [ Tuple [ Any , ...]] All the same functionality as IsListOrTuple , but the compared value must be a tuple. IsTuple from dirty_equals import IsTuple assert ( 1 , 2 , 3 ) == IsTuple ( 1 , 2 , 3 ) assert ( 1 , 2 , 3 ) == IsTuple ( positions = { 2 : 3 }) assert ( 1 , 2 , 3 ) == IsTuple ( 1 , 2 , 3 , check_order = False ) assert ( 1 , 2 , 3 , 4 ) == IsTuple ( 1 , 2 , 3 , length = 4 ) assert ( 1 , 2 , 3 , 4 ) == IsTuple ( 1 , 2 , 3 , length = ( 4 , 5 )) assert ( 1 , 2 , 3 , 4 ) == IsTuple ( 1 , 2 , 3 , length =... ) assert [ 1 , 2 , 3 ] != IsTuple ( 1 , 2 , 3 )","title":"IsTuple"},{"location":"types/sequence/#dirty_equals.HasLen","text":"HasLen ( min_length : int , max_length : Union [ None , int , Any ] = None , ) Bases: DirtyEquals [ Sized ] Check that some has a given length, or length in a given range. Parameters: Name Type Description Default min_length int Expected length if max_length is not given, else minimum length. required max_length Union [None, int , Any ] Expected maximum length, use an ellipsis ... to indicate that there's no maximum. None HasLen from dirty_equals import HasLen assert [ 1 , 2 , 3 ] == HasLen ( 3 ) #(1)! assert '123' == HasLen ( 3 , ... ) #(2)! assert ( 1 , 2 , 3 ) == HasLen ( 3 , 5 ) #(3)! assert ( 1 , 2 , 3 ) == HasLen ( 0 , ... ) #(4)! Length must be 3. Length must be 3 or higher. Length must be between 3 and 5 inclusive. Length is required but can take any value.","title":"HasLen"},{"location":"types/sequence/#dirty_equals.Contains","text":"Contains ( contained_value : Any , * more_contained_values : Any ) Bases: DirtyEquals [ Container [ Any ]] Check that an object contains one or more values. Parameters: Name Type Description Default contained_value Any value that must be contained in the compared object. required *more_contained_values Any more values that must be contained in the compared object. () Contains from dirty_equals import Contains assert [ 1 , 2 , 3 ] == Contains ( 1 ) assert [ 1 , 2 , 3 ] == Contains ( 1 , 2 ) assert ( 1 , 2 , 3 ) == Contains ( 1 ) assert 'abc' == Contains ( 'b' ) assert { 'a' : 1 , 'b' : 2 } == Contains ( 'a' ) assert [ 1 , 2 , 3 ] != Contains ( 10 )","title":"Contains"},{"location":"types/string/","text":"String Types \u00b6 IsAnyStr \u00b6 IsAnyStr ( * , min_length : Optional [ int ] = None , max_length : Optional [ int ] = None , case : Literal [ upper , lower , None ] = None , regex : Union [ None , T , Pattern [ T ]] = None , regex_flags : int = 0 ) Bases: DirtyEquals [ T ] Comparison of str or bytes objects. This class allow comparison with both str and bytes but is subclassed by IsStr and IsBytes which restrict comparison to str or bytes respectively. Parameters: Name Type Description Default min_length Optional [ int ] minimum length of the string/bytes None max_length Optional [ int ] maximum length of the string/bytes None case Literal [ upper , lower , None] check case of the string/bytes None regex Union [None, T , Pattern [ T ]] regular expression to match the string/bytes with, re.fullmatch is used. This can be a compiled regex, or a string or bytes. None regex_flags int optional flags for the regular expression 0 Examples: IsAnyStr from dirty_equals import IsAnyStr assert 'foobar' == IsAnyStr () assert b 'foobar' == IsAnyStr () assert 123 != IsAnyStr () assert 'foobar' == IsAnyStr ( regex = 'foo...' ) assert 'foobar' == IsAnyStr ( regex = b 'foo...' ) #(1)! assert 'foobar' == IsAnyStr ( min_length = 6 ) assert 'foobar' != IsAnyStr ( min_length = 8 ) assert 'foobar' == IsAnyStr ( case = 'lower' ) assert 'Foobar' != IsAnyStr ( case = 'lower' ) regex can be either a string or bytes, IsAnyStr will take care of conversion so checks work. IsStr \u00b6 Bases: IsAnyStr [ str ] Checks if the value is a string, and optionally meets some constraints. IsStr is a subclass of IsAnyStr and therefore allows all the same arguments. Examples: IsStr from dirty_equals import IsStr assert 'foobar' == IsStr () assert b 'foobar' != IsStr () assert 'foobar' == IsStr ( regex = 'foo...' ) assert 'FOOBAR' == IsStr ( min_length = 5 , max_length = 10 , case = 'upper' ) IsBytes \u00b6 Bases: IsAnyStr [ bytes ] Checks if the value is a bytes object, and optionally meets some constraints. IsBytes is a subclass of IsAnyStr and therefore allows all the same arguments. Examples: IsBytes from dirty_equals import IsBytes assert b 'foobar' == IsBytes () assert 'foobar' != IsBytes () assert b 'foobar' == IsBytes ( regex = b 'foo...' ) assert b 'FOOBAR' == IsBytes ( min_length = 5 , max_length = 10 , case = 'upper' )","title":"String Types"},{"location":"types/string/#string-types","text":"","title":"String Types"},{"location":"types/string/#dirty_equals.IsAnyStr","text":"IsAnyStr ( * , min_length : Optional [ int ] = None , max_length : Optional [ int ] = None , case : Literal [ upper , lower , None ] = None , regex : Union [ None , T , Pattern [ T ]] = None , regex_flags : int = 0 ) Bases: DirtyEquals [ T ] Comparison of str or bytes objects. This class allow comparison with both str and bytes but is subclassed by IsStr and IsBytes which restrict comparison to str or bytes respectively. Parameters: Name Type Description Default min_length Optional [ int ] minimum length of the string/bytes None max_length Optional [ int ] maximum length of the string/bytes None case Literal [ upper , lower , None] check case of the string/bytes None regex Union [None, T , Pattern [ T ]] regular expression to match the string/bytes with, re.fullmatch is used. This can be a compiled regex, or a string or bytes. None regex_flags int optional flags for the regular expression 0 Examples: IsAnyStr from dirty_equals import IsAnyStr assert 'foobar' == IsAnyStr () assert b 'foobar' == IsAnyStr () assert 123 != IsAnyStr () assert 'foobar' == IsAnyStr ( regex = 'foo...' ) assert 'foobar' == IsAnyStr ( regex = b 'foo...' ) #(1)! assert 'foobar' == IsAnyStr ( min_length = 6 ) assert 'foobar' != IsAnyStr ( min_length = 8 ) assert 'foobar' == IsAnyStr ( case = 'lower' ) assert 'Foobar' != IsAnyStr ( case = 'lower' ) regex can be either a string or bytes, IsAnyStr will take care of conversion so checks work.","title":"IsAnyStr"},{"location":"types/string/#dirty_equals.IsStr","text":"Bases: IsAnyStr [ str ] Checks if the value is a string, and optionally meets some constraints. IsStr is a subclass of IsAnyStr and therefore allows all the same arguments. Examples: IsStr from dirty_equals import IsStr assert 'foobar' == IsStr () assert b 'foobar' != IsStr () assert 'foobar' == IsStr ( regex = 'foo...' ) assert 'FOOBAR' == IsStr ( min_length = 5 , max_length = 10 , case = 'upper' )","title":"IsStr"},{"location":"types/string/#dirty_equals.IsBytes","text":"Bases: IsAnyStr [ bytes ] Checks if the value is a bytes object, and optionally meets some constraints. IsBytes is a subclass of IsAnyStr and therefore allows all the same arguments. Examples: IsBytes from dirty_equals import IsBytes assert b 'foobar' == IsBytes () assert 'foobar' != IsBytes () assert b 'foobar' == IsBytes ( regex = b 'foo...' ) assert b 'FOOBAR' == IsBytes ( min_length = 5 , max_length = 10 , case = 'upper' )","title":"IsBytes"},{"location":"usage/dataclasses/","text":"If you don't want to use pydantic 's BaseModel you can instead get the same data validation on standard dataclasses (introduced in Python 3.7). Note Keep in mind that pydantic.dataclasses.dataclass is a drop-in replacement for dataclasses.dataclass with validation, not a replacement for pydantic.BaseModel (with a small difference in how initialization hooks work). There are cases where subclassing pydantic.BaseModel is the better choice. For more information and discussion see pydantic/pydantic#710 . You can use all the standard pydantic field types, and the resulting dataclass will be identical to the one created by the standard library dataclass decorator. The underlying model and its schema can be accessed through __pydantic_model__ . Also, fields that require a default_factory can be specified by either a pydantic.Field or a dataclasses.field . pydantic.dataclasses.dataclass 's arguments are the same as the standard decorator, except one extra keyword argument config which has the same meaning as Config . Warning After v1.2, The Mypy plugin must be installed to type check pydantic dataclasses. For more information about combining validators with dataclasses, see dataclass validators . Dataclass Config \u00b6 If you want to modify the Config like you would with a BaseModel , you have three options: Warning After v1.10, pydantic dataclasses support Config.extra but some default behaviour of stdlib dataclasses may prevail. For example, when print ing a pydantic dataclass with allowed extra fields, it will still use the __str__ method of stdlib dataclass and show only the required fields. This may be improved further in the future. Nested dataclasses \u00b6 Nested dataclasses are supported both in dataclasses and normal models. Dataclasses attributes can be populated by tuples, dictionaries or instances of the dataclass itself. Stdlib dataclasses and pydantic dataclasses \u00b6 Convert stdlib dataclasses into pydantic dataclasses \u00b6 Stdlib dataclasses (nested or not) can be easily converted into pydantic dataclasses by just decorating them with pydantic.dataclasses.dataclass . Pydantic will enhance the given stdlib dataclass but won't alter the default behaviour (i.e. without validation). It will instead create a wrapper around it to trigger validation that will act like a plain proxy. The stdlib dataclass can still be accessed via the __dataclass__ attribute (see example below). Choose when to trigger validation \u00b6 As soon as your stdlib dataclass has been decorated with pydantic dataclass decorator, magic methods have been added to validate input data. If you want, you can still keep using your dataclass and choose when to trigger it. Inherit from stdlib dataclasses \u00b6 Stdlib dataclasses (nested or not) can also be inherited and pydantic will automatically validate all the inherited fields. Use of stdlib dataclasses with BaseModel \u00b6 Bear in mind that stdlib dataclasses (nested or not) are automatically converted into pydantic dataclasses when mixed with BaseModel ! Furthermore the generated pydantic dataclass will have the exact same configuration ( order , frozen , ...) as the original one. Use custom types \u00b6 Since stdlib dataclasses are automatically converted to add validation using custom types may cause some unexpected behaviour. In this case you can simply add arbitrary_types_allowed in the config! Initialize hooks \u00b6 When you initialize a dataclass, it is possible to execute code after validation with the help of __post_init_post_parse__ . This is not the same as __post_init__ , which executes code before validation. Tip If you use a stdlib dataclass , you may only have __post_init__ available and wish the validation to be done before. In this case you can set Config.post_init_call = 'after_validation' Since version v1.0 , any fields annotated with dataclasses.InitVar are passed to both __post_init__ and __post_init_post_parse__ . Difference with stdlib dataclasses \u00b6 Note that the dataclasses.dataclass from Python stdlib implements only the __post_init__ method since it doesn't run a validation step. When substituting usage of dataclasses.dataclass with pydantic.dataclasses.dataclass , it is recommended to move the code executed in the __post_init__ method to the __post_init_post_parse__ method, and only leave behind part of code which needs to be executed before validation. JSON Dumping \u00b6 Pydantic dataclasses do not feature a .json() function. To dump them as JSON, you will need to make use of the pydantic_encoder as follows:","title":"Dataclasses"},{"location":"usage/dataclasses/#dataclass-config","text":"If you want to modify the Config like you would with a BaseModel , you have three options: Warning After v1.10, pydantic dataclasses support Config.extra but some default behaviour of stdlib dataclasses may prevail. For example, when print ing a pydantic dataclass with allowed extra fields, it will still use the __str__ method of stdlib dataclass and show only the required fields. This may be improved further in the future.","title":"Dataclass Config"},{"location":"usage/dataclasses/#nested-dataclasses","text":"Nested dataclasses are supported both in dataclasses and normal models. Dataclasses attributes can be populated by tuples, dictionaries or instances of the dataclass itself.","title":"Nested dataclasses"},{"location":"usage/dataclasses/#stdlib-dataclasses-and-pydantic-dataclasses","text":"","title":"Stdlib dataclasses and pydantic dataclasses"},{"location":"usage/dataclasses/#convert-stdlib-dataclasses-into-pydantic-dataclasses","text":"Stdlib dataclasses (nested or not) can be easily converted into pydantic dataclasses by just decorating them with pydantic.dataclasses.dataclass . Pydantic will enhance the given stdlib dataclass but won't alter the default behaviour (i.e. without validation). It will instead create a wrapper around it to trigger validation that will act like a plain proxy. The stdlib dataclass can still be accessed via the __dataclass__ attribute (see example below).","title":"Convert stdlib dataclasses into pydantic dataclasses"},{"location":"usage/dataclasses/#choose-when-to-trigger-validation","text":"As soon as your stdlib dataclass has been decorated with pydantic dataclass decorator, magic methods have been added to validate input data. If you want, you can still keep using your dataclass and choose when to trigger it.","title":"Choose when to trigger validation"},{"location":"usage/dataclasses/#inherit-from-stdlib-dataclasses","text":"Stdlib dataclasses (nested or not) can also be inherited and pydantic will automatically validate all the inherited fields.","title":"Inherit from stdlib dataclasses"},{"location":"usage/dataclasses/#use-of-stdlib-dataclasses-with-basemodel","text":"Bear in mind that stdlib dataclasses (nested or not) are automatically converted into pydantic dataclasses when mixed with BaseModel ! Furthermore the generated pydantic dataclass will have the exact same configuration ( order , frozen , ...) as the original one.","title":"Use of stdlib dataclasses with BaseModel"},{"location":"usage/dataclasses/#use-custom-types","text":"Since stdlib dataclasses are automatically converted to add validation using custom types may cause some unexpected behaviour. In this case you can simply add arbitrary_types_allowed in the config!","title":"Use custom types"},{"location":"usage/dataclasses/#initialize-hooks","text":"When you initialize a dataclass, it is possible to execute code after validation with the help of __post_init_post_parse__ . This is not the same as __post_init__ , which executes code before validation. Tip If you use a stdlib dataclass , you may only have __post_init__ available and wish the validation to be done before. In this case you can set Config.post_init_call = 'after_validation' Since version v1.0 , any fields annotated with dataclasses.InitVar are passed to both __post_init__ and __post_init_post_parse__ .","title":"Initialize hooks"},{"location":"usage/dataclasses/#difference-with-stdlib-dataclasses","text":"Note that the dataclasses.dataclass from Python stdlib implements only the __post_init__ method since it doesn't run a validation step. When substituting usage of dataclasses.dataclass with pydantic.dataclasses.dataclass , it is recommended to move the code executed in the __post_init__ method to the __post_init_post_parse__ method, and only leave behind part of code which needs to be executed before validation.","title":"Difference with stdlib dataclasses"},{"location":"usage/dataclasses/#json-dumping","text":"Pydantic dataclasses do not feature a .json() function. To dump them as JSON, you will need to make use of the pydantic_encoder as follows:","title":"JSON Dumping"},{"location":"usage/devtools/","text":"Note Admission: I (the primary developer of pydantic ) also develop python-devtools. python-devtools ( pip install devtools ) provides a number of tools which are useful during Python development, including debug() an alternative to print() which formats output in a way which should be easier to read than print as well as giving information about which file/line the print statement is on and what value was printed. pydantic integrates with devtools by implementing the __pretty__ method on most public classes. In particular debug() is useful when inspecting models: Will output in your terminal:","title":"Usage with devtools"},{"location":"usage/exporting_models/","text":"As well as accessing model attributes directly via their names (e.g. model.foobar ), models can be converted and exported in a number of ways: model.model_dump(...) \u00b6 This is the primary way of converting a model to a dictionary. Sub-models will be recursively converted to dictionaries. Arguments: include : fields to include in the returned dictionary; see below exclude : fields to exclude from the returned dictionary; see below by_alias : whether field aliases should be used as keys in the returned dictionary; default False exclude_unset : whether fields which were not explicitly set when creating the model should be excluded from the returned dictionary; default False . Prior to v1.0 , exclude_unset was known as skip_defaults ; use of skip_defaults is now deprecated exclude_defaults : whether fields which are equal to their default values (whether set or otherwise) should be excluded from the returned dictionary; default False exclude_none : whether fields which are equal to None should be excluded from the returned dictionary; default False Example: dict(model) and iteration \u00b6 pydantic models can also be converted to dictionaries using dict(model) , and you can also iterate over a model's field using for field_name, value in model: . With this approach the raw field values are returned, so sub-models will not be converted to dictionaries. Example: model.copy(...) \u00b6 copy() allows models to be duplicated, which is particularly useful for immutable models. Arguments: include : fields to include in the returned dictionary; see below exclude : fields to exclude from the returned dictionary; see below update : a dictionary of values to change when creating the copied model deep : whether to make a deep copy of the new model; default False Example: model.model_dump_json(...) \u00b6 The .model_dump_json() method will serialise a model to JSON. (For models with a custom root type , only the value for the __root__ key is serialised) Arguments: include : fields to include in the returned dictionary; see below exclude : fields to exclude from the returned dictionary; see below by_alias : whether field aliases should be used as keys in the returned dictionary; default False exclude_unset : whether fields which were not set when creating the model and have their default values should be excluded from the returned dictionary; default False . Prior to v1.0 , exclude_unset was known as skip_defaults ; use of skip_defaults is now deprecated exclude_defaults : whether fields which are equal to their default values (whether set or otherwise) should be excluded from the returned dictionary; default False exclude_none : whether fields which are equal to None should be excluded from the returned dictionary; default False encoder : a custom encoder function passed to the default argument of json.dumps() ; defaults to a custom encoder designed to take care of all common types **dumps_kwargs : any other keyword arguments are passed to json.dumps() , e.g. indent . pydantic can serialise many commonly used types to JSON (e.g. datetime , date or UUID ) which would normally fail with a simple json.dumps(foobar) . json_encoders \u00b6 Serialisation can be customised on a model using the json_encoders config property; the keys should be types (or names of types for forward references), and the values should be functions which serialise that type (see the example below): By default, timedelta is encoded as a simple float of total seconds. The timedelta_isoformat is provided as an optional alternative which implements ISO 8601 time diff encoding. The json_encoders are also merged during the models inheritance with the child encoders taking precedence over the parent one. Serialising self-reference or other models \u00b6 By default, models are serialised as dictionaries. If you want to serialise them differently, you can add models_as_dict=False when calling json() method and add the classes of the model in json_encoders . In case of forward references, you can use a string with the class name instead of the class itself Serialising subclasses \u00b6 Note New in version v1.5 . Subclasses of common types were not automatically serialised to JSON before v1.5 . Subclasses of common types are automatically encoded like their super-classes: Custom JSON (de)serialisation \u00b6 To improve the performance of encoding and decoding JSON, alternative JSON implementations (e.g. ujson ) can be used via the json_loads and json_dumps properties of Config . ujson generally cannot be used to dump JSON since it doesn't support encoding of objects like datetimes and does not accept a default fallback function argument. To do this, you may use another library like orjson . Note that orjson takes care of datetime encoding natively, making it faster than json.dumps but meaning you cannot always customise the encoding using Config.json_encoders . pickle.dumps(model) \u00b6 Using the same plumbing as copy() , pydantic models support efficient pickling and unpickling. Advanced include and exclude \u00b6 The dict , json , and copy methods support include and exclude arguments which can either be sets or dictionaries. This allows nested selection of which fields to export: The True indicates that we want to exclude or include an entire key, just as if we included it in a set. Of course, the same can be done at any depth level. Special care must be taken when including or excluding fields from a list or tuple of submodels or dictionaries. In this scenario, dict and related methods expect integer keys for element-wise inclusion or exclusion. To exclude a field from every member of a list or tuple, the dictionary key '__all__' can be used as follows: The same holds for the json and copy methods. Model and field level include and exclude \u00b6 In addition to the explicit arguments exclude and include passed to dict , json and copy methods, we can also pass the include / exclude arguments directly to the Field constructor or the equivalent field entry in the models Config class: In the case where multiple strategies are used, exclude / include fields are merged according to the following rules: First, model config level settings (via \"fields\" entry) are merged per field with the field constructor settings (i.e. Field(..., exclude=True) ), with the field constructor taking priority. The resulting settings are merged per class with the explicit settings on dict , json , copy calls with the explicit settings taking priority. Note that while merging settings, exclude entries are merged by computing the \"union\" of keys, while include entries are merged by computing the \"intersection\" of keys. The resulting merged exclude settings: are the same as using merged include settings as follows:","title":"Exporting models"},{"location":"usage/exporting_models/#modelmodel_dump","text":"This is the primary way of converting a model to a dictionary. Sub-models will be recursively converted to dictionaries. Arguments: include : fields to include in the returned dictionary; see below exclude : fields to exclude from the returned dictionary; see below by_alias : whether field aliases should be used as keys in the returned dictionary; default False exclude_unset : whether fields which were not explicitly set when creating the model should be excluded from the returned dictionary; default False . Prior to v1.0 , exclude_unset was known as skip_defaults ; use of skip_defaults is now deprecated exclude_defaults : whether fields which are equal to their default values (whether set or otherwise) should be excluded from the returned dictionary; default False exclude_none : whether fields which are equal to None should be excluded from the returned dictionary; default False Example:","title":"model.model_dump(...)"},{"location":"usage/exporting_models/#dictmodel-and-iteration","text":"pydantic models can also be converted to dictionaries using dict(model) , and you can also iterate over a model's field using for field_name, value in model: . With this approach the raw field values are returned, so sub-models will not be converted to dictionaries. Example:","title":"dict(model) and iteration"},{"location":"usage/exporting_models/#modelcopy","text":"copy() allows models to be duplicated, which is particularly useful for immutable models. Arguments: include : fields to include in the returned dictionary; see below exclude : fields to exclude from the returned dictionary; see below update : a dictionary of values to change when creating the copied model deep : whether to make a deep copy of the new model; default False Example:","title":"model.copy(...)"},{"location":"usage/exporting_models/#modelmodel_dump_json","text":"The .model_dump_json() method will serialise a model to JSON. (For models with a custom root type , only the value for the __root__ key is serialised) Arguments: include : fields to include in the returned dictionary; see below exclude : fields to exclude from the returned dictionary; see below by_alias : whether field aliases should be used as keys in the returned dictionary; default False exclude_unset : whether fields which were not set when creating the model and have their default values should be excluded from the returned dictionary; default False . Prior to v1.0 , exclude_unset was known as skip_defaults ; use of skip_defaults is now deprecated exclude_defaults : whether fields which are equal to their default values (whether set or otherwise) should be excluded from the returned dictionary; default False exclude_none : whether fields which are equal to None should be excluded from the returned dictionary; default False encoder : a custom encoder function passed to the default argument of json.dumps() ; defaults to a custom encoder designed to take care of all common types **dumps_kwargs : any other keyword arguments are passed to json.dumps() , e.g. indent . pydantic can serialise many commonly used types to JSON (e.g. datetime , date or UUID ) which would normally fail with a simple json.dumps(foobar) .","title":"model.model_dump_json(...)"},{"location":"usage/exporting_models/#json_encoders","text":"Serialisation can be customised on a model using the json_encoders config property; the keys should be types (or names of types for forward references), and the values should be functions which serialise that type (see the example below): By default, timedelta is encoded as a simple float of total seconds. The timedelta_isoformat is provided as an optional alternative which implements ISO 8601 time diff encoding. The json_encoders are also merged during the models inheritance with the child encoders taking precedence over the parent one.","title":"json_encoders"},{"location":"usage/exporting_models/#serialising-self-reference-or-other-models","text":"By default, models are serialised as dictionaries. If you want to serialise them differently, you can add models_as_dict=False when calling json() method and add the classes of the model in json_encoders . In case of forward references, you can use a string with the class name instead of the class itself","title":"Serialising self-reference or other models"},{"location":"usage/exporting_models/#serialising-subclasses","text":"Note New in version v1.5 . Subclasses of common types were not automatically serialised to JSON before v1.5 . Subclasses of common types are automatically encoded like their super-classes:","title":"Serialising subclasses"},{"location":"usage/exporting_models/#custom-json-deserialisation","text":"To improve the performance of encoding and decoding JSON, alternative JSON implementations (e.g. ujson ) can be used via the json_loads and json_dumps properties of Config . ujson generally cannot be used to dump JSON since it doesn't support encoding of objects like datetimes and does not accept a default fallback function argument. To do this, you may use another library like orjson . Note that orjson takes care of datetime encoding natively, making it faster than json.dumps but meaning you cannot always customise the encoding using Config.json_encoders .","title":"Custom JSON (de)serialisation"},{"location":"usage/exporting_models/#pickledumpsmodel","text":"Using the same plumbing as copy() , pydantic models support efficient pickling and unpickling.","title":"pickle.dumps(model)"},{"location":"usage/exporting_models/#advanced-include-and-exclude","text":"The dict , json , and copy methods support include and exclude arguments which can either be sets or dictionaries. This allows nested selection of which fields to export: The True indicates that we want to exclude or include an entire key, just as if we included it in a set. Of course, the same can be done at any depth level. Special care must be taken when including or excluding fields from a list or tuple of submodels or dictionaries. In this scenario, dict and related methods expect integer keys for element-wise inclusion or exclusion. To exclude a field from every member of a list or tuple, the dictionary key '__all__' can be used as follows: The same holds for the json and copy methods.","title":"Advanced include and exclude"},{"location":"usage/exporting_models/#model-and-field-level-include-and-exclude","text":"In addition to the explicit arguments exclude and include passed to dict , json and copy methods, we can also pass the include / exclude arguments directly to the Field constructor or the equivalent field entry in the models Config class: In the case where multiple strategies are used, exclude / include fields are merged according to the following rules: First, model config level settings (via \"fields\" entry) are merged per field with the field constructor settings (i.e. Field(..., exclude=True) ), with the field constructor taking priority. The resulting settings are merged per class with the explicit settings on dict , json , copy calls with the explicit settings taking priority. Note that while merging settings, exclude entries are merged by computing the \"union\" of keys, while include entries are merged by computing the \"intersection\" of keys. The resulting merged exclude settings: are the same as using merged include settings as follows:","title":"Model and field level include and exclude"},{"location":"usage/model_config/","text":"Behaviour of pydantic can be controlled via the Config class on a model or a pydantic dataclass. Also, you can specify config options as model class kwargs: Similarly, if using the @dataclass decorator: Options \u00b6 title the title for the generated JSON Schema anystr_strip_whitespace whether to strip leading and trailing whitespace for str & byte types (default: False ) anystr_upper whether to make all characters uppercase for str & byte types (default: False ) anystr_lower whether to make all characters lowercase for str & byte types (default: False ) min_anystr_length the min length for str & byte types (default: 0 ) max_anystr_length the max length for str & byte types (default: None ) validate_all whether to validate field defaults (default: False ) extra whether to ignore, allow, or forbid extra attributes during model initialization. Accepts the string values of 'ignore' , 'allow' , or 'forbid' , or values of the Extra enum (default: Extra.ignore ). 'forbid' will cause validation to fail if extra attributes are included, 'ignore' will silently ignore any extra attributes, and 'allow' will assign the attributes to the model. allow_mutation whether or not models are faux-immutable, i.e. whether __setattr__ is allowed (default: True ) frozen Warning This parameter is in beta setting frozen=True does everything that allow_mutation=False does, and also generates a __hash__() method for the model. This makes instances of the model potentially hashable if all the attributes are hashable. (default: False ) use_enum_values whether to populate models with the value property of enums, rather than the raw enum. This may be useful if you want to serialise model.model_dump() later (default: False ) fields a dict containing schema information for each field; this is equivalent to using the Field class , except when a field is already defined through annotation or the Field class, in which case only alias , include , exclude , min_length , max_length , regex , gt , lt , gt , le , multiple_of , max_digits , decimal_places , min_items , max_items , unique_items and allow_mutation can be set (for example you cannot set default of default_factory ) (default: None ) validate_assignment whether to perform validation on assignment to attributes (default: False ) allow_population_by_field_name whether an aliased field may be populated by its name as given by the model attribute, as well as the alias (default: False ) Note The name of this configuration setting was changed in v1.0 from allow_population_by_alias to allow_population_by_field_name . error_msg_templates a dict used to override the default error message templates. Pass in a dictionary with keys matching the error messages you want to override (default: {} ) arbitrary_types_allowed whether to allow arbitrary user types for fields (they are validated simply by checking if the value is an instance of the type). If False , RuntimeError will be raised on model declaration (default: False ). See an example in Field Types . undefined_types_warning whether to raise a warning if a type is undefined when a model is declared. This occurs when a type is defined in another model declared elsewhere in code which has not yet executed. If True , UserWarning will be raised on model declaration (default: True ). See an example in Field Types . orm_mode whether to allow usage of ORM mode getter_dict a custom class (which should inherit from GetterDict ) to use when decomposing arbitrary classes for validation, for use with orm_mode ; see Data binding . alias_generator a callable that takes a field name and returns an alias for it; see the dedicated section keep_untouched a tuple of types (e.g. descriptors) for a model's default values that should not be changed during model creation and will not be included in the model schemas. Note : this means that attributes on the model with defaults of this type , not annotations of this type , will be left alone. schema_extra a dict used to extend/update the generated JSON Schema, or a callable to post-process it; see schema customization json_loads a custom function for decoding JSON; see custom JSON (de)serialisation json_dumps a custom function for encoding JSON; see custom JSON (de)serialisation json_encoders a dict used to customise the way types are encoded to JSON; see JSON Serialisation underscore_attrs_are_private whether to treat any underscore non-class var attrs as private, or leave them as is; see Private model attributes copy_on_model_validation string literal to control how models instances are processed during validation, with the following means (see #4093 for a full discussion of the changes to this field): 'none' - models are not copied on validation, they're simply kept \"untouched\" 'shallow' - models are shallow copied, this is the default 'deep' - models are deep copied smart_union whether pydantic should try to check all types inside Union to prevent undesired coercion; see the dedicated section post_init_call whether stdlib dataclasses __post_init__ should be run before (default behaviour with value 'before_validation' ) or after (value 'after_validation' ) parsing and validation when they are converted . allow_inf_nan whether to allow infinity ( +inf an -inf ) and NaN values to float fields, defaults to True , set to False for compatibility with JSON , see #3994 for more details, added in V1.10 Change behaviour globally \u00b6 If you wish to change the behaviour of pydantic globally, you can create your own custom BaseModel with custom Config since the config is inherited Alias Generator \u00b6 If data source field names do not match your code style (e. g. CamelCase fields), you can automatically generate aliases using alias_generator : Here camel case refers to \"upper camel case\" aka pascal case e.g. CamelCase . If you'd like instead to use lower camel case e.g. camelCase , instead use the to_lower_camel function. Alias Precedence \u00b6 Warning Alias priority logic changed in v1.4 to resolve buggy and unexpected behaviour in previous versions. In some circumstances this may represent a breaking change , see #1178 and the precedence order below for details. In the case where a field's alias may be defined in multiple places, the selected value is determined as follows (in descending order of priority): Set via Field(..., alias=<alias>) , directly on the model Defined in Config.fields , directly on the model Set via Field(..., alias=<alias>) , on a parent model Defined in Config.fields , on a parent model Generated by alias_generator , regardless of whether it's on the model or a parent Note This means an alias_generator defined on a child model does not take priority over an alias defined on a field in a parent model. For example: Smart Union \u00b6 By default, as explained here , pydantic tries to validate (and coerce if it can) in the order of the Union . So sometimes you may have unexpected coerced data. To prevent this, you can enable Config.smart_union . Pydantic will then check all allowed types before even trying to coerce. Know that this is of course slower, especially if your Union is quite big. Warning Note that this option does not support compound types yet (e.g. differentiate List[int] and List[str] ). This option will be improved further once a strict mode is added in pydantic and will probably be the default behaviour in v2!","title":"Model Config"},{"location":"usage/model_config/#options","text":"title the title for the generated JSON Schema anystr_strip_whitespace whether to strip leading and trailing whitespace for str & byte types (default: False ) anystr_upper whether to make all characters uppercase for str & byte types (default: False ) anystr_lower whether to make all characters lowercase for str & byte types (default: False ) min_anystr_length the min length for str & byte types (default: 0 ) max_anystr_length the max length for str & byte types (default: None ) validate_all whether to validate field defaults (default: False ) extra whether to ignore, allow, or forbid extra attributes during model initialization. Accepts the string values of 'ignore' , 'allow' , or 'forbid' , or values of the Extra enum (default: Extra.ignore ). 'forbid' will cause validation to fail if extra attributes are included, 'ignore' will silently ignore any extra attributes, and 'allow' will assign the attributes to the model. allow_mutation whether or not models are faux-immutable, i.e. whether __setattr__ is allowed (default: True ) frozen Warning This parameter is in beta setting frozen=True does everything that allow_mutation=False does, and also generates a __hash__() method for the model. This makes instances of the model potentially hashable if all the attributes are hashable. (default: False ) use_enum_values whether to populate models with the value property of enums, rather than the raw enum. This may be useful if you want to serialise model.model_dump() later (default: False ) fields a dict containing schema information for each field; this is equivalent to using the Field class , except when a field is already defined through annotation or the Field class, in which case only alias , include , exclude , min_length , max_length , regex , gt , lt , gt , le , multiple_of , max_digits , decimal_places , min_items , max_items , unique_items and allow_mutation can be set (for example you cannot set default of default_factory ) (default: None ) validate_assignment whether to perform validation on assignment to attributes (default: False ) allow_population_by_field_name whether an aliased field may be populated by its name as given by the model attribute, as well as the alias (default: False ) Note The name of this configuration setting was changed in v1.0 from allow_population_by_alias to allow_population_by_field_name . error_msg_templates a dict used to override the default error message templates. Pass in a dictionary with keys matching the error messages you want to override (default: {} ) arbitrary_types_allowed whether to allow arbitrary user types for fields (they are validated simply by checking if the value is an instance of the type). If False , RuntimeError will be raised on model declaration (default: False ). See an example in Field Types . undefined_types_warning whether to raise a warning if a type is undefined when a model is declared. This occurs when a type is defined in another model declared elsewhere in code which has not yet executed. If True , UserWarning will be raised on model declaration (default: True ). See an example in Field Types . orm_mode whether to allow usage of ORM mode getter_dict a custom class (which should inherit from GetterDict ) to use when decomposing arbitrary classes for validation, for use with orm_mode ; see Data binding . alias_generator a callable that takes a field name and returns an alias for it; see the dedicated section keep_untouched a tuple of types (e.g. descriptors) for a model's default values that should not be changed during model creation and will not be included in the model schemas. Note : this means that attributes on the model with defaults of this type , not annotations of this type , will be left alone. schema_extra a dict used to extend/update the generated JSON Schema, or a callable to post-process it; see schema customization json_loads a custom function for decoding JSON; see custom JSON (de)serialisation json_dumps a custom function for encoding JSON; see custom JSON (de)serialisation json_encoders a dict used to customise the way types are encoded to JSON; see JSON Serialisation underscore_attrs_are_private whether to treat any underscore non-class var attrs as private, or leave them as is; see Private model attributes copy_on_model_validation string literal to control how models instances are processed during validation, with the following means (see #4093 for a full discussion of the changes to this field): 'none' - models are not copied on validation, they're simply kept \"untouched\" 'shallow' - models are shallow copied, this is the default 'deep' - models are deep copied smart_union whether pydantic should try to check all types inside Union to prevent undesired coercion; see the dedicated section post_init_call whether stdlib dataclasses __post_init__ should be run before (default behaviour with value 'before_validation' ) or after (value 'after_validation' ) parsing and validation when they are converted . allow_inf_nan whether to allow infinity ( +inf an -inf ) and NaN values to float fields, defaults to True , set to False for compatibility with JSON , see #3994 for more details, added in V1.10","title":"Options"},{"location":"usage/model_config/#change-behaviour-globally","text":"If you wish to change the behaviour of pydantic globally, you can create your own custom BaseModel with custom Config since the config is inherited","title":"Change behaviour globally"},{"location":"usage/model_config/#alias-generator","text":"If data source field names do not match your code style (e. g. CamelCase fields), you can automatically generate aliases using alias_generator : Here camel case refers to \"upper camel case\" aka pascal case e.g. CamelCase . If you'd like instead to use lower camel case e.g. camelCase , instead use the to_lower_camel function.","title":"Alias Generator"},{"location":"usage/model_config/#alias-precedence","text":"Warning Alias priority logic changed in v1.4 to resolve buggy and unexpected behaviour in previous versions. In some circumstances this may represent a breaking change , see #1178 and the precedence order below for details. In the case where a field's alias may be defined in multiple places, the selected value is determined as follows (in descending order of priority): Set via Field(..., alias=<alias>) , directly on the model Defined in Config.fields , directly on the model Set via Field(..., alias=<alias>) , on a parent model Defined in Config.fields , on a parent model Generated by alias_generator , regardless of whether it's on the model or a parent Note This means an alias_generator defined on a child model does not take priority over an alias defined on a field in a parent model. For example:","title":"Alias Precedence"},{"location":"usage/model_config/#smart-union","text":"By default, as explained here , pydantic tries to validate (and coerce if it can) in the order of the Union . So sometimes you may have unexpected coerced data. To prevent this, you can enable Config.smart_union . Pydantic will then check all allowed types before even trying to coerce. Know that this is of course slower, especially if your Union is quite big. Warning Note that this option does not support compound types yet (e.g. differentiate List[int] and List[str] ). This option will be improved further once a strict mode is added in pydantic and will probably be the default behaviour in v2!","title":"Smart Union"},{"location":"usage/models/","text":"The primary means of defining objects in pydantic is via models (models are simply classes which inherit from BaseModel ). You can think of models as similar to types in strictly typed languages, or as the requirements of a single endpoint in an API. Untrusted data can be passed to a model, and after parsing and validation pydantic guarantees that the fields of the resultant model instance will conform to the field types defined on the model. Note pydantic is primarily a parsing library, not a validation library . Validation is a means to an end: building a model which conforms to the types and constraints provided. In other words, pydantic guarantees the types and constraints of the output model, not the input data. This might sound like an esoteric distinction, but it is not. If you're unsure what this means or how it might affect your usage you should read the section about Data Conversion below. Although validation is not the main purpose of pydantic , you can use this library for custom validation . Basic model usage \u00b6 from pydantic import BaseModel class User ( BaseModel ): id : int name = 'Jane Doe' User here is a model with two fields id which is an integer and is required, and name which is a string and is not required (it has a default value). The type of name is inferred from the default value, and so a type annotation is not required (however note this warning about field order when some fields do not have type annotations). user = User ( id = '123' ) user_x = User ( id = '123.45' ) user here is an instance of User . Initialisation of the object will perform all parsing and validation, if no ValidationError is raised, you know the resulting model instance is valid. assert user . id == 123 assert user_x . id == 123 assert isinstance ( user_x . id , int ) # Note that 123.45 was casted to an int and its value is 123 More details on the casting in the case of user_x can be found in Data Conversion . Fields of a model can be accessed as normal attributes of the user object. The string '123' has been cast to an int as per the field type assert user . name == 'Jane Doe' name wasn't set when user was initialised, so it has the default value assert user . __fields_set__ == { 'id' } The fields which were supplied when user was initialised. assert user . model_dump () == dict ( user ) == { 'id' : 123 , 'name' : 'Jane Doe' } Either .model_dump() or dict(user) will provide a dict of fields, but .model_dump() can take numerous other arguments. user . id = 321 assert user . id == 321 This model is mutable so field values can be changed. Model properties \u00b6 The example above only shows the tip of the iceberg of what models can do. Models possess the following methods and attributes: model_dump() returns a dictionary of the model's fields and values; cf. exporting models model_dump_json() returns a JSON string representation model_dump() ; cf. exporting models copy() returns a copy (by default, shallow copy) of the model; cf. exporting models model_validate() a utility for loading any object into a model with error handling if the object is not a dictionary; cf. helper functions parse_raw() a utility for loading strings of numerous formats; cf. helper functions parse_file() like parse_raw() but for file paths; cf. helper functions from_orm() loads data into a model from an arbitrary class; cf. ORM mode model_json_schema() returns a dictionary representing the model as JSON Schema; cf. schema schema_json() returns a JSON string representation of schema() ; cf. schema model_construct() a class method for creating models without running validation; cf. Creating models without validation __fields_set__ Set of names of fields which were set when the model instance was initialised model_fields a dictionary of the model's fields __config__ the configuration class for the model, cf. model config Recursive Models \u00b6 More complex hierarchical data structures can be defined using models themselves as types in annotations. For self-referencing models, see postponed annotations . ORM Mode (aka Arbitrary Class Instances) \u00b6 Pydantic models can be created from arbitrary class instances to support models that map to ORM objects. To do this: The Config property orm_mode must be set to True . The special constructor from_orm must be used to create the model instance. The example here uses SQLAlchemy, but the same approach should work for any ORM. Reserved names \u00b6 You may want to name a Column after a reserved SQLAlchemy field. In that case, Field aliases will be convenient: Note The example above works because aliases have priority over field names for field population. Accessing SQLModel 's metadata attribute would lead to a ValidationError . Recursive ORM models \u00b6 ORM instances will be parsed with from_orm recursively as well as at the top level. Here a vanilla class is used to demonstrate the principle, but any ORM class could be used instead. Data binding \u00b6 Arbitrary classes are processed by pydantic using the GetterDict class (see utils.py ), which attempts to provide a dictionary-like interface to any class. You can customise how this works by setting your own sub-class of GetterDict as the value of Config.getter_dict (see config ). You can also customise class validation using root_validators with pre=True . In this case your validator function will be passed a GetterDict instance which you may copy and modify. The GetterDict instance will be called for each field with a sentinel as a fallback (if no other default value is set). Returning this sentinel means that the field is missing. Any other value will be interpreted as the value of the field. Error Handling \u00b6 pydantic will raise ValidationError whenever it finds an error in the data it's validating. Note Validation code should not raise ValidationError itself, but rather raise ValueError , TypeError or AssertionError (or subclasses of ValueError or TypeError ) which will be caught and used to populate ValidationError . One exception will be raised regardless of the number of errors found, that ValidationError will contain information about all the errors and how they happened. You can access these errors in several ways: e.errors() method will return list of errors found in the input data. e.json() method will return a JSON representation of errors . str(e) method will return a human readable representation of the errors. Each error object contains: loc the error's location as a list. The first item in the list will be the field where the error occurred, and if the field is a sub-model , subsequent items will be present to indicate the nested location of the error. type a computer-readable identifier of the error type. msg a human readable explanation of the error. ctx an optional object which contains values required to render the error message. As a demonstration: Custom Errors \u00b6 In your custom data types or validators you should use ValueError , TypeError or AssertionError to raise errors. See validators for more details on use of the @validator decorator. You can also define your own error classes, which can specify a custom error code, message template, and context: Helper Functions \u00b6 Pydantic provides three classmethod helper functions on models for parsing data: model_validate : this is very similar to the __init__ method of the model, except it takes a dict rather than keyword arguments. If the object passed is not a dict a ValidationError will be raised. parse_raw : this takes a str or bytes and parses it as json , then passes the result to model_validate . Parsing pickle data is also supported by setting the content_type argument appropriately. parse_file : this takes in a file path, reads the file and passes the contents to parse_raw . If content_type is omitted, it is inferred from the file's extension. Warning To quote the official pickle docs , \"The pickle module is not secure against erroneous or maliciously constructed data. Never unpickle data received from an untrusted or unauthenticated source.\" Info Because it can result in arbitrary code execution, as a security measure, you need to explicitly pass allow_pickle to the parsing function in order to load pickle data. Creating models without validation \u00b6 pydantic also provides the model_construct() method which allows models to be created without validation this can be useful when data has already been validated or comes from a trusted source and you want to create a model as efficiently as possible ( model_construct() is generally around 30x faster than creating a model with full validation). Warning model_construct() does not do any validation, meaning it can create models which are invalid. You should only ever use the model_construct() method with data which has already been validated, or you trust. The _fields_set keyword argument to model_construct() is optional, but allows you to be more precise about which fields were originally set and which weren't. If it's omitted __fields_set__ will just be the keys of the data provided. For example, in the example above, if _fields_set was not provided, new_user.__fields_set__ would be {'id', 'age', 'name'} . Generic Models \u00b6 Pydantic supports the creation of generic models to make it easier to reuse a common model structure. In order to declare a generic model, you perform the following steps: Declare one or more typing.TypeVar instances to use to parameterize your model. Declare a pydantic model that inherits from pydantic.generics.GenericModel and typing.Generic , where you pass the TypeVar instances as parameters to typing.Generic . Use the TypeVar instances as annotations where you will want to replace them with other types or pydantic models. Here is an example using GenericModel to create an easily-reused HTTP response payload wrapper: If you set Config or make use of validator in your generic model definition, it is applied to concrete subclasses in the same way as when inheriting from BaseModel . Any methods defined on your generic class will also be inherited. Pydantic's generics also integrate properly with mypy, so you get all the type checking you would expect mypy to provide if you were to declare the type without using GenericModel . Note Internally, pydantic uses create_model to generate a (cached) concrete BaseModel at runtime, so there is essentially zero overhead introduced by making use of GenericModel . To inherit from a GenericModel without replacing the TypeVar instance, a class must also inherit from typing.Generic : You can also create a generic subclass of a GenericModel that partially or fully replaces the type parameters in the superclass. If the name of the concrete subclasses is important, you can also override the default behavior: Using the same TypeVar in nested models allows you to enforce typing relationships at different points in your model: Pydantic also treats GenericModel similarly to how it treats built-in generic types like List and Dict when it comes to leaving them unparameterized, or using bounded TypeVar instances: If you don't specify parameters before instantiating the generic model, they will be treated as Any You can parametrize models with one or more bounded parameters to add subclass checks Also, like List and Dict , any parameters specified using a TypeVar can later be substituted with concrete types. Dynamic model creation \u00b6 There are some occasions where the shape of a model is not known until runtime. For this pydantic provides the create_model method to allow models to be created on the fly. Here StaticFoobarModel and DynamicFoobarModel are identical. Warning See the note in Required Optional Fields for the distinction between an ellipsis as a field default and annotation-only fields. See pydantic/pydantic#1047 for more details. Fields are defined by either a tuple of the form (<type>, <default value>) or just a default value. The special key word arguments __config__ and __base__ can be used to customise the new model. This includes extending a base model with extra fields. You can also add validators by passing a dict to the __validators__ argument. Model creation from NamedTuple or TypedDict \u00b6 Sometimes you already use in your application classes that inherit from NamedTuple or TypedDict and you don't want to duplicate all your information to have a BaseModel . For this pydantic provides create_model_from_namedtuple and create_model_from_typeddict methods. Those methods have the exact same keyword arguments as create_model . Custom Root Types \u00b6 Pydantic models can be defined with a custom root type by declaring the __root__ field. The root type can be any type supported by pydantic, and is specified by the type hint on the __root__ field. The root value can be passed to the model __init__ via the __root__ keyword argument, or as the first and only argument to model_validate . If you call the model_validate method for a model with a custom root type with a dict as the first argument, the following logic is used: If the custom root type is a mapping type (eg., Dict or Mapping ), the argument itself is always validated against the custom root type. For other custom root types, if the dict has precisely one key with the value __root__ , the corresponding value will be validated against the custom root type. Otherwise, the dict itself is validated against the custom root type. This is demonstrated in the following example: Warning Calling the model_validate method on a dict with the single key \"__root__\" for non-mapping custom root types is currently supported for backwards compatibility, but is not recommended and may be dropped in a future version. If you want to access items in the __root__ field directly or to iterate over the items, you can implement custom __iter__ and __getitem__ functions, as shown in the following example. Faux Immutability \u00b6 Models can be configured to be immutable via allow_mutation = False . When this is set, attempting to change the values of instance attributes will raise errors. See model config for more details on Config . Warning Immutability in Python is never strict. If developers are determined/stupid they can always modify a so-called \"immutable\" object. Trying to change a caused an error, and a remains unchanged. However, the dict b is mutable, and the immutability of foobar doesn't stop b from being changed. Abstract Base Classes \u00b6 Pydantic models can be used alongside Python's Abstract Base Classes (ABCs). Field Ordering \u00b6 Field order is important in models for the following reasons: validation is performed in the order fields are defined; fields validators can access the values of earlier fields, but not later ones field order is preserved in the model schema field order is preserved in validation errors field order is preserved by .model_dump() and .model_dump_json() etc. As of v1.0 all fields with annotations (whether annotation-only or with a default value) will precede all fields without an annotation. Within their respective groups, fields remain in the order they were defined. Warning As demonstrated by the example above, combining the use of annotated and non-annotated fields in the same model can result in surprising field orderings. (This is due to limitations of Python) Therefore, we recommend adding type annotations to all fields , even when a default value would determine the type by itself to guarantee field order is preserved. Required fields \u00b6 To declare a field as required, you may declare it using just an annotation, or you may use an ellipsis ( ... ) as the value: Where Field refers to the field function . Here a , b and c are all required. However, use of the ellipses in b will not work well with mypy , and as of v1.0 should be avoided in most cases. Required Optional fields \u00b6 Warning Since version v1.2 annotation only nullable ( Optional[...] , Union[None, ...] and Any ) fields and nullable fields with an ellipsis ( ... ) as the default value, no longer mean the same thing. In some situations this may cause v1.2 to not be entirely backwards compatible with earlier v1. * releases. If you want to specify a field that can take a None value while still being required, you can use Optional with ... : In this model, a , b , and c can take None as a value. But a is optional, while b and c are required. b and c require a value, even if the value is None . Field with dynamic default value \u00b6 When declaring a field with a default value, you may want it to be dynamic (i.e. different for each model). To do this, you may want to use a default_factory . In Beta The default_factory argument is in beta , it has been added to pydantic in v1.5 on a provisional basis . It may change significantly in future releases and its signature or behaviour will not be concrete until v2 . Feedback from the community while it's still provisional would be extremely useful; either comment on #866 or create a new issue. Example of usage: Where Field refers to the field function . Warning The default_factory expects the field type to be set. Automatically excluded attributes \u00b6 Class variables which begin with an underscore and attributes annotated with typing.ClassVar will be automatically excluded from the model. Private model attributes \u00b6 If you need to vary or manipulate internal attributes on instances of the model, you can declare them using PrivateAttr : Private attribute names must start with underscore to prevent conflicts with model fields: both _attr and __attr__ are supported. If Config.underscore_attrs_are_private is True , any non-ClassVar underscore attribute will be treated as private: Upon class creation pydantic constructs __slots__ filled with private attributes. Parsing data into a specified type \u00b6 Pydantic includes a standalone utility function parse_obj_as that can be used to apply the parsing logic used to populate pydantic models in a more ad-hoc way. This function behaves similarly to BaseModel.model_validate , but works with arbitrary pydantic-compatible types. This is especially useful when you want to parse results into a type that is not a direct subclass of BaseModel . For example: This function is capable of parsing data into any of the types pydantic can handle as fields of a BaseModel . Pydantic also includes two similar standalone functions called parse_file_as and parse_raw_as , which are analogous to BaseModel.parse_file and BaseModel.parse_raw . Data Conversion \u00b6 pydantic may cast input data to force it to conform to model field types, and in some cases this may result in a loss of information. For example: This is a deliberate decision of pydantic , and in general it's the most useful approach. See here for a longer discussion on the subject. Nevertheless, strict type checking is partially supported. Model signature \u00b6 All pydantic models will have their signature generated based on their fields: An accurate signature is useful for introspection purposes and libraries like FastAPI or hypothesis . The generated signature will also respect custom __init__ functions: To be included in the signature, a field's alias or name must be a valid Python identifier. pydantic prefers aliases over names, but may use field names if the alias is not a valid Python identifier. If a field's alias and name are both invalid identifiers, a **data argument will be added. In addition, the **data argument will always be present in the signature if Config.extra is Extra.allow . Note Types in the model signature are the same as declared in model annotations, not necessarily all the types that can actually be provided to that field. This may be fixed one day once #1055 is solved. Structural pattern matching \u00b6 pydantic supports structural pattern matching for models, as introduced by PEP 636 in Python 3.10. Note A match-case statement may seem as if it creates a new model, but don't be fooled; it is just syntactic sugar for getting an attribute and either comparing it or declaring and initializing it.","title":"Models"},{"location":"usage/models/#basic-model-usage","text":"from pydantic import BaseModel class User ( BaseModel ): id : int name = 'Jane Doe' User here is a model with two fields id which is an integer and is required, and name which is a string and is not required (it has a default value). The type of name is inferred from the default value, and so a type annotation is not required (however note this warning about field order when some fields do not have type annotations). user = User ( id = '123' ) user_x = User ( id = '123.45' ) user here is an instance of User . Initialisation of the object will perform all parsing and validation, if no ValidationError is raised, you know the resulting model instance is valid. assert user . id == 123 assert user_x . id == 123 assert isinstance ( user_x . id , int ) # Note that 123.45 was casted to an int and its value is 123 More details on the casting in the case of user_x can be found in Data Conversion . Fields of a model can be accessed as normal attributes of the user object. The string '123' has been cast to an int as per the field type assert user . name == 'Jane Doe' name wasn't set when user was initialised, so it has the default value assert user . __fields_set__ == { 'id' } The fields which were supplied when user was initialised. assert user . model_dump () == dict ( user ) == { 'id' : 123 , 'name' : 'Jane Doe' } Either .model_dump() or dict(user) will provide a dict of fields, but .model_dump() can take numerous other arguments. user . id = 321 assert user . id == 321 This model is mutable so field values can be changed.","title":"Basic model usage"},{"location":"usage/models/#model-properties","text":"The example above only shows the tip of the iceberg of what models can do. Models possess the following methods and attributes: model_dump() returns a dictionary of the model's fields and values; cf. exporting models model_dump_json() returns a JSON string representation model_dump() ; cf. exporting models copy() returns a copy (by default, shallow copy) of the model; cf. exporting models model_validate() a utility for loading any object into a model with error handling if the object is not a dictionary; cf. helper functions parse_raw() a utility for loading strings of numerous formats; cf. helper functions parse_file() like parse_raw() but for file paths; cf. helper functions from_orm() loads data into a model from an arbitrary class; cf. ORM mode model_json_schema() returns a dictionary representing the model as JSON Schema; cf. schema schema_json() returns a JSON string representation of schema() ; cf. schema model_construct() a class method for creating models without running validation; cf. Creating models without validation __fields_set__ Set of names of fields which were set when the model instance was initialised model_fields a dictionary of the model's fields __config__ the configuration class for the model, cf. model config","title":"Model properties"},{"location":"usage/models/#recursive-models","text":"More complex hierarchical data structures can be defined using models themselves as types in annotations. For self-referencing models, see postponed annotations .","title":"Recursive Models"},{"location":"usage/models/#orm-mode-aka-arbitrary-class-instances","text":"Pydantic models can be created from arbitrary class instances to support models that map to ORM objects. To do this: The Config property orm_mode must be set to True . The special constructor from_orm must be used to create the model instance. The example here uses SQLAlchemy, but the same approach should work for any ORM.","title":"ORM Mode (aka Arbitrary Class Instances)"},{"location":"usage/models/#reserved-names","text":"You may want to name a Column after a reserved SQLAlchemy field. In that case, Field aliases will be convenient: Note The example above works because aliases have priority over field names for field population. Accessing SQLModel 's metadata attribute would lead to a ValidationError .","title":"Reserved names"},{"location":"usage/models/#recursive-orm-models","text":"ORM instances will be parsed with from_orm recursively as well as at the top level. Here a vanilla class is used to demonstrate the principle, but any ORM class could be used instead.","title":"Recursive ORM models"},{"location":"usage/models/#data-binding","text":"Arbitrary classes are processed by pydantic using the GetterDict class (see utils.py ), which attempts to provide a dictionary-like interface to any class. You can customise how this works by setting your own sub-class of GetterDict as the value of Config.getter_dict (see config ). You can also customise class validation using root_validators with pre=True . In this case your validator function will be passed a GetterDict instance which you may copy and modify. The GetterDict instance will be called for each field with a sentinel as a fallback (if no other default value is set). Returning this sentinel means that the field is missing. Any other value will be interpreted as the value of the field.","title":"Data binding"},{"location":"usage/models/#error-handling","text":"pydantic will raise ValidationError whenever it finds an error in the data it's validating. Note Validation code should not raise ValidationError itself, but rather raise ValueError , TypeError or AssertionError (or subclasses of ValueError or TypeError ) which will be caught and used to populate ValidationError . One exception will be raised regardless of the number of errors found, that ValidationError will contain information about all the errors and how they happened. You can access these errors in several ways: e.errors() method will return list of errors found in the input data. e.json() method will return a JSON representation of errors . str(e) method will return a human readable representation of the errors. Each error object contains: loc the error's location as a list. The first item in the list will be the field where the error occurred, and if the field is a sub-model , subsequent items will be present to indicate the nested location of the error. type a computer-readable identifier of the error type. msg a human readable explanation of the error. ctx an optional object which contains values required to render the error message. As a demonstration:","title":"Error Handling"},{"location":"usage/models/#custom-errors","text":"In your custom data types or validators you should use ValueError , TypeError or AssertionError to raise errors. See validators for more details on use of the @validator decorator. You can also define your own error classes, which can specify a custom error code, message template, and context:","title":"Custom Errors"},{"location":"usage/models/#helper-functions","text":"Pydantic provides three classmethod helper functions on models for parsing data: model_validate : this is very similar to the __init__ method of the model, except it takes a dict rather than keyword arguments. If the object passed is not a dict a ValidationError will be raised. parse_raw : this takes a str or bytes and parses it as json , then passes the result to model_validate . Parsing pickle data is also supported by setting the content_type argument appropriately. parse_file : this takes in a file path, reads the file and passes the contents to parse_raw . If content_type is omitted, it is inferred from the file's extension. Warning To quote the official pickle docs , \"The pickle module is not secure against erroneous or maliciously constructed data. Never unpickle data received from an untrusted or unauthenticated source.\" Info Because it can result in arbitrary code execution, as a security measure, you need to explicitly pass allow_pickle to the parsing function in order to load pickle data.","title":"Helper Functions"},{"location":"usage/models/#creating-models-without-validation","text":"pydantic also provides the model_construct() method which allows models to be created without validation this can be useful when data has already been validated or comes from a trusted source and you want to create a model as efficiently as possible ( model_construct() is generally around 30x faster than creating a model with full validation). Warning model_construct() does not do any validation, meaning it can create models which are invalid. You should only ever use the model_construct() method with data which has already been validated, or you trust. The _fields_set keyword argument to model_construct() is optional, but allows you to be more precise about which fields were originally set and which weren't. If it's omitted __fields_set__ will just be the keys of the data provided. For example, in the example above, if _fields_set was not provided, new_user.__fields_set__ would be {'id', 'age', 'name'} .","title":"Creating models without validation"},{"location":"usage/models/#generic-models","text":"Pydantic supports the creation of generic models to make it easier to reuse a common model structure. In order to declare a generic model, you perform the following steps: Declare one or more typing.TypeVar instances to use to parameterize your model. Declare a pydantic model that inherits from pydantic.generics.GenericModel and typing.Generic , where you pass the TypeVar instances as parameters to typing.Generic . Use the TypeVar instances as annotations where you will want to replace them with other types or pydantic models. Here is an example using GenericModel to create an easily-reused HTTP response payload wrapper: If you set Config or make use of validator in your generic model definition, it is applied to concrete subclasses in the same way as when inheriting from BaseModel . Any methods defined on your generic class will also be inherited. Pydantic's generics also integrate properly with mypy, so you get all the type checking you would expect mypy to provide if you were to declare the type without using GenericModel . Note Internally, pydantic uses create_model to generate a (cached) concrete BaseModel at runtime, so there is essentially zero overhead introduced by making use of GenericModel . To inherit from a GenericModel without replacing the TypeVar instance, a class must also inherit from typing.Generic : You can also create a generic subclass of a GenericModel that partially or fully replaces the type parameters in the superclass. If the name of the concrete subclasses is important, you can also override the default behavior: Using the same TypeVar in nested models allows you to enforce typing relationships at different points in your model: Pydantic also treats GenericModel similarly to how it treats built-in generic types like List and Dict when it comes to leaving them unparameterized, or using bounded TypeVar instances: If you don't specify parameters before instantiating the generic model, they will be treated as Any You can parametrize models with one or more bounded parameters to add subclass checks Also, like List and Dict , any parameters specified using a TypeVar can later be substituted with concrete types.","title":"Generic Models"},{"location":"usage/models/#dynamic-model-creation","text":"There are some occasions where the shape of a model is not known until runtime. For this pydantic provides the create_model method to allow models to be created on the fly. Here StaticFoobarModel and DynamicFoobarModel are identical. Warning See the note in Required Optional Fields for the distinction between an ellipsis as a field default and annotation-only fields. See pydantic/pydantic#1047 for more details. Fields are defined by either a tuple of the form (<type>, <default value>) or just a default value. The special key word arguments __config__ and __base__ can be used to customise the new model. This includes extending a base model with extra fields. You can also add validators by passing a dict to the __validators__ argument.","title":"Dynamic model creation"},{"location":"usage/models/#model-creation-from-namedtuple-or-typeddict","text":"Sometimes you already use in your application classes that inherit from NamedTuple or TypedDict and you don't want to duplicate all your information to have a BaseModel . For this pydantic provides create_model_from_namedtuple and create_model_from_typeddict methods. Those methods have the exact same keyword arguments as create_model .","title":"Model creation from NamedTuple or TypedDict"},{"location":"usage/models/#custom-root-types","text":"Pydantic models can be defined with a custom root type by declaring the __root__ field. The root type can be any type supported by pydantic, and is specified by the type hint on the __root__ field. The root value can be passed to the model __init__ via the __root__ keyword argument, or as the first and only argument to model_validate . If you call the model_validate method for a model with a custom root type with a dict as the first argument, the following logic is used: If the custom root type is a mapping type (eg., Dict or Mapping ), the argument itself is always validated against the custom root type. For other custom root types, if the dict has precisely one key with the value __root__ , the corresponding value will be validated against the custom root type. Otherwise, the dict itself is validated against the custom root type. This is demonstrated in the following example: Warning Calling the model_validate method on a dict with the single key \"__root__\" for non-mapping custom root types is currently supported for backwards compatibility, but is not recommended and may be dropped in a future version. If you want to access items in the __root__ field directly or to iterate over the items, you can implement custom __iter__ and __getitem__ functions, as shown in the following example.","title":"Custom Root Types"},{"location":"usage/models/#faux-immutability","text":"Models can be configured to be immutable via allow_mutation = False . When this is set, attempting to change the values of instance attributes will raise errors. See model config for more details on Config . Warning Immutability in Python is never strict. If developers are determined/stupid they can always modify a so-called \"immutable\" object. Trying to change a caused an error, and a remains unchanged. However, the dict b is mutable, and the immutability of foobar doesn't stop b from being changed.","title":"Faux Immutability"},{"location":"usage/models/#abstract-base-classes","text":"Pydantic models can be used alongside Python's Abstract Base Classes (ABCs).","title":"Abstract Base Classes"},{"location":"usage/models/#field-ordering","text":"Field order is important in models for the following reasons: validation is performed in the order fields are defined; fields validators can access the values of earlier fields, but not later ones field order is preserved in the model schema field order is preserved in validation errors field order is preserved by .model_dump() and .model_dump_json() etc. As of v1.0 all fields with annotations (whether annotation-only or with a default value) will precede all fields without an annotation. Within their respective groups, fields remain in the order they were defined. Warning As demonstrated by the example above, combining the use of annotated and non-annotated fields in the same model can result in surprising field orderings. (This is due to limitations of Python) Therefore, we recommend adding type annotations to all fields , even when a default value would determine the type by itself to guarantee field order is preserved.","title":"Field Ordering"},{"location":"usage/models/#required-fields","text":"To declare a field as required, you may declare it using just an annotation, or you may use an ellipsis ( ... ) as the value: Where Field refers to the field function . Here a , b and c are all required. However, use of the ellipses in b will not work well with mypy , and as of v1.0 should be avoided in most cases.","title":"Required fields"},{"location":"usage/models/#required-optional-fields","text":"Warning Since version v1.2 annotation only nullable ( Optional[...] , Union[None, ...] and Any ) fields and nullable fields with an ellipsis ( ... ) as the default value, no longer mean the same thing. In some situations this may cause v1.2 to not be entirely backwards compatible with earlier v1. * releases. If you want to specify a field that can take a None value while still being required, you can use Optional with ... : In this model, a , b , and c can take None as a value. But a is optional, while b and c are required. b and c require a value, even if the value is None .","title":"Required Optional fields"},{"location":"usage/models/#field-with-dynamic-default-value","text":"When declaring a field with a default value, you may want it to be dynamic (i.e. different for each model). To do this, you may want to use a default_factory . In Beta The default_factory argument is in beta , it has been added to pydantic in v1.5 on a provisional basis . It may change significantly in future releases and its signature or behaviour will not be concrete until v2 . Feedback from the community while it's still provisional would be extremely useful; either comment on #866 or create a new issue. Example of usage: Where Field refers to the field function . Warning The default_factory expects the field type to be set.","title":"Field with dynamic default value"},{"location":"usage/models/#automatically-excluded-attributes","text":"Class variables which begin with an underscore and attributes annotated with typing.ClassVar will be automatically excluded from the model.","title":"Automatically excluded attributes"},{"location":"usage/models/#private-model-attributes","text":"If you need to vary or manipulate internal attributes on instances of the model, you can declare them using PrivateAttr : Private attribute names must start with underscore to prevent conflicts with model fields: both _attr and __attr__ are supported. If Config.underscore_attrs_are_private is True , any non-ClassVar underscore attribute will be treated as private: Upon class creation pydantic constructs __slots__ filled with private attributes.","title":"Private model attributes"},{"location":"usage/models/#parsing-data-into-a-specified-type","text":"Pydantic includes a standalone utility function parse_obj_as that can be used to apply the parsing logic used to populate pydantic models in a more ad-hoc way. This function behaves similarly to BaseModel.model_validate , but works with arbitrary pydantic-compatible types. This is especially useful when you want to parse results into a type that is not a direct subclass of BaseModel . For example: This function is capable of parsing data into any of the types pydantic can handle as fields of a BaseModel . Pydantic also includes two similar standalone functions called parse_file_as and parse_raw_as , which are analogous to BaseModel.parse_file and BaseModel.parse_raw .","title":"Parsing data into a specified type"},{"location":"usage/models/#data-conversion","text":"pydantic may cast input data to force it to conform to model field types, and in some cases this may result in a loss of information. For example: This is a deliberate decision of pydantic , and in general it's the most useful approach. See here for a longer discussion on the subject. Nevertheless, strict type checking is partially supported.","title":"Data Conversion"},{"location":"usage/models/#model-signature","text":"All pydantic models will have their signature generated based on their fields: An accurate signature is useful for introspection purposes and libraries like FastAPI or hypothesis . The generated signature will also respect custom __init__ functions: To be included in the signature, a field's alias or name must be a valid Python identifier. pydantic prefers aliases over names, but may use field names if the alias is not a valid Python identifier. If a field's alias and name are both invalid identifiers, a **data argument will be added. In addition, the **data argument will always be present in the signature if Config.extra is Extra.allow . Note Types in the model signature are the same as declared in model annotations, not necessarily all the types that can actually be provided to that field. This may be fixed one day once #1055 is solved.","title":"Model signature"},{"location":"usage/models/#structural-pattern-matching","text":"pydantic supports structural pattern matching for models, as introduced by PEP 636 in Python 3.10. Note A match-case statement may seem as if it creates a new model, but don't be fooled; it is just syntactic sugar for getting an attribute and either comparing it or declaring and initializing it.","title":"Structural pattern matching"},{"location":"usage/mypy/","text":"pydantic models work with mypy provided you use the annotation-only version of required fields: You can run your code through mypy with: mypy \\ --ignore-missing-imports \\ --follow-imports = skip \\ --strict-optional \\ pydantic_mypy_test.py If you call mypy on the example code above, you should see mypy detect the attribute access error: 13: error: \"Model\" has no attribute \"middle_name\" Strict Optional \u00b6 For your code to pass with --strict-optional , you need to to use Optional[] or an alias of Optional[] for all fields with None as the default. (This is standard with mypy.) Pydantic provides a few useful optional or union types: NoneStr aka. Optional[str] NoneBytes aka. Optional[bytes] StrBytes aka. Union[str, bytes] NoneStrBytes aka. Optional[StrBytes] If these aren't sufficient you can of course define your own. Mypy Plugin \u00b6 Pydantic ships with a mypy plugin that adds a number of important pydantic-specific features to mypy that improve its ability to type-check your code. See the pydantic mypy plugin docs for more details. Other pydantic interfaces \u00b6 Pydantic dataclasses and the validate_arguments decorator should also work well with mypy.","title":"Usage with mypy"},{"location":"usage/mypy/#strict-optional","text":"For your code to pass with --strict-optional , you need to to use Optional[] or an alias of Optional[] for all fields with None as the default. (This is standard with mypy.) Pydantic provides a few useful optional or union types: NoneStr aka. Optional[str] NoneBytes aka. Optional[bytes] StrBytes aka. Union[str, bytes] NoneStrBytes aka. Optional[StrBytes] If these aren't sufficient you can of course define your own.","title":"Strict Optional"},{"location":"usage/mypy/#mypy-plugin","text":"Pydantic ships with a mypy plugin that adds a number of important pydantic-specific features to mypy that improve its ability to type-check your code. See the pydantic mypy plugin docs for more details.","title":"Mypy Plugin"},{"location":"usage/mypy/#other-pydantic-interfaces","text":"Pydantic dataclasses and the validate_arguments decorator should also work well with mypy.","title":"Other pydantic interfaces"},{"location":"usage/postponed_annotations/","text":"Note Both postponed annotations via the future import and ForwardRef require Python 3.7+. Postponed annotations (as described in PEP563 ) \"just work\". Internally, pydantic will call a method similar to typing.get_type_hints to resolve annotations. In cases where the referenced type is not yet defined, ForwardRef can be used (although referencing the type directly or by its string is a simpler solution in the case of self-referencing models ). In some cases, a ForwardRef won't be able to be resolved during model creation. For example, this happens whenever a model references itself as a field type. When this happens, you'll need to call update_forward_refs after the model has been created before it can be used: Warning To resolve strings (type names) into annotations (types), pydantic needs a namespace dict in which to perform the lookup. For this it uses module.__dict__ , just like get_type_hints . This means pydantic may not play well with types not defined in the global scope of a module. For example, this works fine: While this will break: Resolving this is beyond the call for pydantic : either remove the future import or declare the types globally. Self-referencing Models \u00b6 Data structures with self-referencing models are also supported. Self-referencing fields will be automatically resolved after model creation. Within the model, you can refer to the not-yet-constructed model using a string: Since Python 3.7, you can also refer it by its type, provided you import annotations (see above for support depending on Python and pydantic versions).","title":"Postponed annotations"},{"location":"usage/postponed_annotations/#self-referencing-models","text":"Data structures with self-referencing models are also supported. Self-referencing fields will be automatically resolved after model creation. Within the model, you can refer to the not-yet-constructed model using a string: Since Python 3.7, you can also refer it by its type, provided you import annotations (see above for support depending on Python and pydantic versions).","title":"Self-referencing Models"},{"location":"usage/rich/","text":"Pydantic models may be printed with the Rich library which will add additional formatting and color to the output. Here's an example: See the Rich documentation on pretty printing for more information.","title":"Usage with rich"},{"location":"usage/schema/","text":"Pydantic allows auto creation of JSON Schemas from models: The generated schemas are compliant with the specifications: JSON Schema Core , JSON Schema Validation and OpenAPI . BaseModel.model_json_schema will return a dict of the schema, while BaseModel.schema_json will return a JSON string representation of that dict. Sub-models used are added to the definitions JSON attribute and referenced, as per the spec. All sub-models' (and their sub-models') schemas are put directly in a top-level definitions JSON key for easy re-use and reference. \"Sub-models\" with modifications (via the Field class) like a custom title, description or default value, are recursively included instead of referenced. The description for models is taken from either the docstring of the class or the argument description to the Field class. The schema is generated by default using aliases as keys, but it can be generated using model property names instead by calling MainModel.model_json_schema/schema_json(by_alias=False) . The format of $ref s ( \"#/definitions/FooBar\" above) can be altered by calling model_json_schema() or schema_json() with the ref_template keyword argument, e.g. ApplePie.model_json_schema(ref_template='/schemas/{model}.json#/') , here {model} will be replaced with the model naming using str.format() . Getting schema of a specified type \u00b6 Pydantic includes two standalone utility functions schema_of and schema_json_of that can be used to apply the schema generation logic used for pydantic models in a more ad-hoc way. These functions behave similarly to BaseModel.model_json_schema and BaseModel.schema_json , but work with arbitrary pydantic-compatible types. Field customization \u00b6 Optionally, the Field function can be used to provide extra information about the field and validations. It has the following arguments: default : (a positional argument) the default value of the field. Since the Field replaces the field's default, this first argument can be used to set the default. Use ellipsis ( ... ) to indicate the field is required. default_factory : a zero-argument callable that will be called when a default value is needed for this field. Among other purposes, this can be used to set dynamic default values. It is forbidden to set both default and default_factory . alias : the public name of the field title : if omitted, field_name.title() is used description : if omitted and the annotation is a sub-model, the docstring of the sub-model will be used exclude : exclude this field when dumping ( .dict and .json ) the instance. The exact syntax and configuration options are described in details in the exporting models section . include : include (only) this field when dumping ( .dict and .json ) the instance. The exact syntax and configuration options are described in details in the exporting models section . const : this argument must be the same as the field's default value if present. gt : for numeric values ( int , float , Decimal ), adds a validation of \"greater than\" and an annotation of exclusiveMinimum to the JSON Schema ge : for numeric values, this adds a validation of \"greater than or equal\" and an annotation of minimum to the JSON Schema lt : for numeric values, this adds a validation of \"less than\" and an annotation of exclusiveMaximum to the JSON Schema le : for numeric values, this adds a validation of \"less than or equal\" and an annotation of maximum to the JSON Schema multiple_of : for numeric values, this adds a validation of \"a multiple of\" and an annotation of multipleOf to the JSON Schema max_digits : for Decimal values, this adds a validation to have a maximum number of digits within the decimal. It does not include a zero before the decimal point or trailing decimal zeroes. decimal_places : for Decimal values, this adds a validation to have at most a number of decimal places allowed. It does not include trailing decimal zeroes. min_items : for list values, this adds a corresponding validation and an annotation of minItems to the JSON Schema max_items : for list values, this adds a corresponding validation and an annotation of maxItems to the JSON Schema unique_items : for list values, this adds a corresponding validation and an annotation of uniqueItems to the JSON Schema min_length : for string values, this adds a corresponding validation and an annotation of minLength to the JSON Schema max_length : for string values, this adds a corresponding validation and an annotation of maxLength to the JSON Schema allow_mutation : a boolean which defaults to True . When False, the field raises a TypeError if the field is assigned on an instance. The model config must set validate_assignment to True for this check to be performed. regex : for string values, this adds a Regular Expression validation generated from the passed string and an annotation of pattern to the JSON Schema Note pydantic validates strings using re.match , which treats regular expressions as implicitly anchored at the beginning. On the contrary, JSON Schema validators treat the pattern keyword as implicitly unanchored, more like what re.search does. For interoperability, depending on your desired behavior, either explicitly anchor your regular expressions with ^ (e.g. ^foo to match any string starting with foo ), or explicitly allow an arbitrary prefix with .*? (e.g. .*?foo to match any string containing the substring foo ). See #1631 for a discussion of possible changes to pydantic behavior in v2 . repr : a boolean which defaults to True . When False, the field shall be hidden from the object representation. ** any other keyword arguments (e.g. examples ) will be added verbatim to the field's schema Instead of using Field , the fields property of the Config class can be used to set all of the arguments above except default . Unenforced Field constraints \u00b6 If pydantic finds constraints which are not being enforced, an error will be raised. If you want to force the constraint to appear in the schema, even though it's not being checked upon parsing, you can use variadic arguments to Field() with the raw schema attribute name: typing.Annotated Fields \u00b6 Rather than assigning a Field value, it can be specified in the type hint with typing.Annotated : Field can only be supplied once per field - an error will be raised if used in Annotated and as the assigned value. Defaults can be set outside Annotated as the assigned value or with Field.default_factory inside Annotated - the Field.default argument is not supported inside Annotated . For versions of Python prior to 3.9, typing_extensions.Annotated can be used. Modifying schema in custom fields \u00b6 Custom field types can customise the schema generated for them using the __modify_schema__ class method; see Custom Data Types for more details. __modify_schema__ can also take a field argument which will have type Optional[ModelField] . pydantic will inspect the signature of __modify_schema__ to determine whether the field argument should be included. JSON Schema Types \u00b6 Types, custom field types, and constraints (like max_length ) are mapped to the corresponding spec formats in the following priority order (when there is an equivalent available): JSON Schema Core JSON Schema Validation OpenAPI Data Types The standard format JSON field is used to define pydantic extensions for more complex string sub-types. The field schema mapping from Python / pydantic to JSON Schema is done as follows: Top-level schema generation \u00b6 You can also generate a top-level JSON Schema that only includes a list of models and related sub-models in its definitions : Schema customization \u00b6 You can customize the generated $ref JSON location: the definitions are always stored under the key definitions , but a specified prefix can be used for the references. This is useful if you need to extend or modify the JSON Schema default definitions location. E.g. with OpenAPI: It's also possible to extend/override the generated JSON schema in a model. To do it, use the Config sub-class attribute schema_extra . For example, you could add examples to the JSON Schema: For more fine-grained control, you can alternatively set schema_extra to a callable and post-process the generated schema. The callable can have one or two positional arguments. The first will be the schema dictionary. The second, if accepted, will be the model class. The callable is expected to mutate the schema dictionary in-place ; the return value is not used. For example, the title key can be removed from the model's properties :","title":"Schema"},{"location":"usage/schema/#getting-schema-of-a-specified-type","text":"Pydantic includes two standalone utility functions schema_of and schema_json_of that can be used to apply the schema generation logic used for pydantic models in a more ad-hoc way. These functions behave similarly to BaseModel.model_json_schema and BaseModel.schema_json , but work with arbitrary pydantic-compatible types.","title":"Getting schema of a specified type"},{"location":"usage/schema/#field-customization","text":"Optionally, the Field function can be used to provide extra information about the field and validations. It has the following arguments: default : (a positional argument) the default value of the field. Since the Field replaces the field's default, this first argument can be used to set the default. Use ellipsis ( ... ) to indicate the field is required. default_factory : a zero-argument callable that will be called when a default value is needed for this field. Among other purposes, this can be used to set dynamic default values. It is forbidden to set both default and default_factory . alias : the public name of the field title : if omitted, field_name.title() is used description : if omitted and the annotation is a sub-model, the docstring of the sub-model will be used exclude : exclude this field when dumping ( .dict and .json ) the instance. The exact syntax and configuration options are described in details in the exporting models section . include : include (only) this field when dumping ( .dict and .json ) the instance. The exact syntax and configuration options are described in details in the exporting models section . const : this argument must be the same as the field's default value if present. gt : for numeric values ( int , float , Decimal ), adds a validation of \"greater than\" and an annotation of exclusiveMinimum to the JSON Schema ge : for numeric values, this adds a validation of \"greater than or equal\" and an annotation of minimum to the JSON Schema lt : for numeric values, this adds a validation of \"less than\" and an annotation of exclusiveMaximum to the JSON Schema le : for numeric values, this adds a validation of \"less than or equal\" and an annotation of maximum to the JSON Schema multiple_of : for numeric values, this adds a validation of \"a multiple of\" and an annotation of multipleOf to the JSON Schema max_digits : for Decimal values, this adds a validation to have a maximum number of digits within the decimal. It does not include a zero before the decimal point or trailing decimal zeroes. decimal_places : for Decimal values, this adds a validation to have at most a number of decimal places allowed. It does not include trailing decimal zeroes. min_items : for list values, this adds a corresponding validation and an annotation of minItems to the JSON Schema max_items : for list values, this adds a corresponding validation and an annotation of maxItems to the JSON Schema unique_items : for list values, this adds a corresponding validation and an annotation of uniqueItems to the JSON Schema min_length : for string values, this adds a corresponding validation and an annotation of minLength to the JSON Schema max_length : for string values, this adds a corresponding validation and an annotation of maxLength to the JSON Schema allow_mutation : a boolean which defaults to True . When False, the field raises a TypeError if the field is assigned on an instance. The model config must set validate_assignment to True for this check to be performed. regex : for string values, this adds a Regular Expression validation generated from the passed string and an annotation of pattern to the JSON Schema Note pydantic validates strings using re.match , which treats regular expressions as implicitly anchored at the beginning. On the contrary, JSON Schema validators treat the pattern keyword as implicitly unanchored, more like what re.search does. For interoperability, depending on your desired behavior, either explicitly anchor your regular expressions with ^ (e.g. ^foo to match any string starting with foo ), or explicitly allow an arbitrary prefix with .*? (e.g. .*?foo to match any string containing the substring foo ). See #1631 for a discussion of possible changes to pydantic behavior in v2 . repr : a boolean which defaults to True . When False, the field shall be hidden from the object representation. ** any other keyword arguments (e.g. examples ) will be added verbatim to the field's schema Instead of using Field , the fields property of the Config class can be used to set all of the arguments above except default .","title":"Field customization"},{"location":"usage/schema/#unenforced-field-constraints","text":"If pydantic finds constraints which are not being enforced, an error will be raised. If you want to force the constraint to appear in the schema, even though it's not being checked upon parsing, you can use variadic arguments to Field() with the raw schema attribute name:","title":"Unenforced Field constraints"},{"location":"usage/schema/#typingannotated-fields","text":"Rather than assigning a Field value, it can be specified in the type hint with typing.Annotated : Field can only be supplied once per field - an error will be raised if used in Annotated and as the assigned value. Defaults can be set outside Annotated as the assigned value or with Field.default_factory inside Annotated - the Field.default argument is not supported inside Annotated . For versions of Python prior to 3.9, typing_extensions.Annotated can be used.","title":"typing.Annotated Fields"},{"location":"usage/schema/#modifying-schema-in-custom-fields","text":"Custom field types can customise the schema generated for them using the __modify_schema__ class method; see Custom Data Types for more details. __modify_schema__ can also take a field argument which will have type Optional[ModelField] . pydantic will inspect the signature of __modify_schema__ to determine whether the field argument should be included.","title":"Modifying schema in custom fields"},{"location":"usage/schema/#json-schema-types","text":"Types, custom field types, and constraints (like max_length ) are mapped to the corresponding spec formats in the following priority order (when there is an equivalent available): JSON Schema Core JSON Schema Validation OpenAPI Data Types The standard format JSON field is used to define pydantic extensions for more complex string sub-types. The field schema mapping from Python / pydantic to JSON Schema is done as follows:","title":"JSON Schema Types"},{"location":"usage/schema/#top-level-schema-generation","text":"You can also generate a top-level JSON Schema that only includes a list of models and related sub-models in its definitions :","title":"Top-level schema generation"},{"location":"usage/schema/#schema-customization","text":"You can customize the generated $ref JSON location: the definitions are always stored under the key definitions , but a specified prefix can be used for the references. This is useful if you need to extend or modify the JSON Schema default definitions location. E.g. with OpenAPI: It's also possible to extend/override the generated JSON schema in a model. To do it, use the Config sub-class attribute schema_extra . For example, you could add examples to the JSON Schema: For more fine-grained control, you can alternatively set schema_extra to a callable and post-process the generated schema. The callable can have one or two positional arguments. The first will be the schema dictionary. The second, if accepted, will be the model class. The callable is expected to mutate the schema dictionary in-place ; the return value is not used. For example, the title key can be removed from the model's properties :","title":"Schema customization"},{"location":"usage/types/","text":"Where possible pydantic uses standard library types to define fields, thus smoothing the learning curve. For many useful applications, however, no standard library type exists, so pydantic implements many commonly used types . If no existing type suits your purpose you can also implement your own pydantic-compatible types with custom properties and validation. Standard Library Types \u00b6 pydantic supports many common types from the Python standard library. If you need stricter processing see Strict Types ; if you need to constrain the values allowed (e.g. to require a positive int) see Constrained Types . None , type(None) or Literal[None] (equivalent according to PEP 484 ) allows only None value bool see Booleans below for details on how bools are validated and what values are permitted int pydantic uses int(v) to coerce types to an int ; see this warning on loss of information during data conversion float similarly, float(v) is used to coerce values to floats str strings are accepted as-is, int float and Decimal are coerced using str(v) , bytes and bytearray are converted using v.decode() , enums inheriting from str are converted using v.value , and all other types cause an error bytes bytes are accepted as-is, bytearray is converted using bytes(v) , str are converted using v.encode() , and int , float , and Decimal are coerced using str(v).encode() list allows list , tuple , set , frozenset , deque , or generators and casts to a list; see typing.List below for sub-type constraints tuple allows list , tuple , set , frozenset , deque , or generators and casts to a tuple; see typing.Tuple below for sub-type constraints dict dict(v) is used to attempt to convert a dictionary; see typing.Dict below for sub-type constraints set allows list , tuple , set , frozenset , deque , or generators and casts to a set; see typing.Set below for sub-type constraints frozenset allows list , tuple , set , frozenset , deque , or generators and casts to a frozen set; see typing.FrozenSet below for sub-type constraints deque allows list , tuple , set , frozenset , deque , or generators and casts to a deque; see typing.Deque below for sub-type constraints datetime.date see Datetime Types below for more detail on parsing and validation datetime.time see Datetime Types below for more detail on parsing and validation datetime.datetime see Datetime Types below for more detail on parsing and validation datetime.timedelta see Datetime Types below for more detail on parsing and validation typing.Any allows any value including None , thus an Any field is optional typing.Annotated allows wrapping another type with arbitrary metadata, as per PEP-593 . The Annotated hint may contain a single call to the Field function , but otherwise the additional metadata is ignored and the root type is used. typing.TypeVar constrains the values allowed based on constraints or bound , see TypeVar typing.Union see Unions below for more detail on parsing and validation typing.Optional Optional[x] is simply short hand for Union[x, None] ; see Unions below for more detail on parsing and validation and Required Fields for details about required fields that can receive None as a value. typing.List see Typing Iterables below for more detail on parsing and validation typing.Tuple see Typing Iterables below for more detail on parsing and validation subclass of typing.NamedTuple Same as tuple but instantiates with the given namedtuple and validates fields since they are annotated. See Annotated Types below for more detail on parsing and validation subclass of collections.namedtuple Same as subclass of typing.NamedTuple but all fields will have type Any since they are not annotated typing.Dict see Typing Iterables below for more detail on parsing and validation subclass of typing.TypedDict Same as dict but pydantic will validate the dictionary since keys are annotated. See Annotated Types below for more detail on parsing and validation typing.Set see Typing Iterables below for more detail on parsing and validation typing.FrozenSet see Typing Iterables below for more detail on parsing and validation typing.Deque see Typing Iterables below for more detail on parsing and validation typing.Sequence see Typing Iterables below for more detail on parsing and validation typing.Iterable this is reserved for iterables that shouldn't be consumed. See Infinite Generators below for more detail on parsing and validation typing.Type see Type below for more detail on parsing and validation typing.Callable see Callable below for more detail on parsing and validation typing.Pattern will cause the input value to be passed to re.compile(v) to create a regex pattern ipaddress.IPv4Address simply uses the type itself for validation by passing the value to IPv4Address(v) ; see Pydantic Types for other custom IP address types ipaddress.IPv4Interface simply uses the type itself for validation by passing the value to IPv4Address(v) ; see Pydantic Types for other custom IP address types ipaddress.IPv4Network simply uses the type itself for validation by passing the value to IPv4Network(v) ; see Pydantic Types for other custom IP address types ipaddress.IPv6Address simply uses the type itself for validation by passing the value to IPv6Address(v) ; see Pydantic Types for other custom IP address types ipaddress.IPv6Interface simply uses the type itself for validation by passing the value to IPv6Interface(v) ; see Pydantic Types for other custom IP address types ipaddress.IPv6Network simply uses the type itself for validation by passing the value to IPv6Network(v) ; see Pydantic Types for other custom IP address types enum.Enum checks that the value is a valid Enum instance subclass of enum.Enum checks that the value is a valid member of the enum; see Enums and Choices for more details enum.IntEnum checks that the value is a valid IntEnum instance subclass of enum.IntEnum checks that the value is a valid member of the integer enum; see Enums and Choices for more details decimal.Decimal pydantic attempts to convert the value to a string, then passes the string to Decimal(v) pathlib.Path simply uses the type itself for validation by passing the value to Path(v) ; see Pydantic Types for other more strict path types uuid.UUID strings and bytes (converted to strings) are passed to UUID(v) , with a fallback to UUID(bytes=v) for bytes and bytearray ; see Pydantic Types for other stricter UUID types ByteSize converts a bytes string with units to bytes Typing Iterables \u00b6 pydantic uses standard library typing types as defined in PEP 484 to define complex objects. Infinite Generators \u00b6 If you have a generator you can use Sequence as described above. In that case, the generator will be consumed and stored on the model as a list and its values will be validated with the sub-type of Sequence (e.g. int in Sequence[int] ). But if you have a generator that you don't want to be consumed, e.g. an infinite generator or a remote data loader, you can define its type with Iterable : Warning Iterable fields only perform a simple check that the argument is iterable and won't be consumed. No validation of their values is performed as it cannot be done without consuming the iterable. Tip If you want to validate the values of an infinite generator you can create a separate model and use it while consuming the generator, reporting the validation errors as appropriate. pydantic can't validate the values automatically for you because it would require consuming the infinite generator. Validating the first value \u00b6 You can create a validator to validate the first value in an infinite generator and still not consume it entirely. Unions \u00b6 The Union type allows a model attribute to accept different types, e.g.: Info You may get unexpected coercion with Union ; see below. Know that you can also make the check slower but stricter by using Smart Union However, as can be seen above, pydantic will attempt to 'match' any of the types defined under Union and will use the first one that matches. In the above example the id of user_03 was defined as a uuid.UUID class (which is defined under the attribute's Union annotation) but as the uuid.UUID can be marshalled into an int it chose to match against the int type and disregarded the other types. Warning typing.Union also ignores order when defined , so Union[int, float] == Union[float, int] which can lead to unexpected behaviour when combined with matching based on the Union type order inside other type definitions, such as List and Dict types (because Python treats these definitions as singletons). For example, Dict[str, Union[int, float]] == Dict[str, Union[float, int]] with the order based on the first time it was defined. Please note that this can also be affected by third party libraries and their internal type definitions and the import orders. As such, it is recommended that, when defining Union annotations, the most specific type is included first and followed by less specific types. In the above example, the UUID class should precede the int and str classes to preclude the unexpected representation as such: Tip The type Optional[x] is a shorthand for Union[x, None] . Optional[x] can also be used to specify a required field that can take None as a value. See more details in Required Fields . Discriminated Unions (a.k.a. Tagged Unions) \u00b6 When Union is used with multiple submodels, you sometimes know exactly which submodel needs to be checked and validated and want to enforce this. To do that you can set the same field - let's call it my_discriminator - in each of the submodels with a discriminated value, which is one (or many) Literal value(s). For your Union , you can set the discriminator in its value: Field(discriminator='my_discriminator') . Setting a discriminated union has many benefits: validation is faster since it is only attempted against one model only one explicit error is raised in case of failure the generated JSON schema implements the associated OpenAPI specification Note Using the Annotated Fields syntax can be handy to regroup the Union and discriminator information. See below for an example! Warning Discriminated unions cannot be used with only a single variant, such as Union[Cat] . Python changes Union[T] into T at interpretation time, so it is not possible for pydantic to distinguish fields of Union[T] from T . Nested Discriminated Unions \u00b6 Only one discriminator can be set for a field but sometimes you want to combine multiple discriminators. In this case you can always create \"intermediate\" models with __root__ and add your discriminator. Enums and Choices \u00b6 pydantic uses Python's standard enum classes to define choices. Datetime Types \u00b6 Pydantic supports the following datetime types: datetime fields can be: datetime , existing datetime object int or float , assumed as Unix time, i.e. seconds (if >= -2e10 or <= 2e10 ) or milliseconds (if < -2e10 or > 2e10 ) since 1 January 1970 str , following formats work: YYYY-MM-DD[T]HH:MM[:SS[.ffffff]][Z or [\u00b1]HH[:]MM] int or float as a string (assumed as Unix time) date fields can be: date , existing date object int or float , see datetime str , following formats work: YYYY-MM-DD int or float , see datetime time fields can be: time , existing time object str , following formats work: HH:MM[:SS[.ffffff]][Z or [\u00b1]HH[:]MM] timedelta fields can be: timedelta , existing timedelta object int or float , assumed as seconds str , following formats work: [-][DD ][HH:MM]SS[.ffffff] [\u00b1]P[DD]DT[HH]H[MM]M[SS]S ( ISO 8601 format for timedelta) Booleans \u00b6 Warning The logic for parsing bool fields has changed as of version v1.0 . Prior to v1.0 , bool parsing never failed, leading to some unexpected results. The new logic is described below. A standard bool field will raise a ValidationError if the value is not one of the following: A valid boolean (i.e. True or False ), The integers 0 or 1 , a str which when converted to lower case is one of '0', 'off', 'f', 'false', 'n', 'no', '1', 'on', 't', 'true', 'y', 'yes' a bytes which is valid (per the previous rule) when decoded to str Note If you want stricter boolean logic (e.g. a field which only permits True and False ) you can use StrictBool . Here is a script demonstrating some of these behaviors: Callable \u00b6 Fields can also be of type Callable : Warning Callable fields only perform a simple check that the argument is callable; no validation of arguments, their types, or the return type is performed. Type \u00b6 pydantic supports the use of Type[T] to specify that a field may only accept classes (not instances) that are subclasses of T . You may also use Type to specify that any class is allowed. TypeVar \u00b6 TypeVar is supported either unconstrained, constrained or with a bound. Literal Type \u00b6 Note This is a new feature of the Python standard library as of Python 3.8; prior to Python 3.8, it requires the typing-extensions package. pydantic supports the use of typing.Literal (or typing_extensions.Literal prior to Python 3.8) as a lightweight way to specify that a field may accept only specific literal values: One benefit of this field type is that it can be used to check for equality with one or more specific values without needing to declare custom validators: With proper ordering in an annotated Union , you can use this to parse types of decreasing specificity: Annotated Types \u00b6 NamedTuple \u00b6 TypedDict \u00b6 Note This is a new feature of the Python standard library as of Python 3.8. Prior to Python 3.8, it requires the typing-extensions package. But required and optional fields are properly differentiated only since Python 3.9. We therefore recommend using typing-extensions with Python 3.8 as well. Pydantic Types \u00b6 pydantic also provides a variety of other useful types: FilePath like Path , but the path must exist and be a file DirectoryPath like Path , but the path must exist and be a directory PastDate like date , but the date should be in the past FutureDate like date , but the date should be in the future EmailStr requires email-validator to be installed; the input string must be a valid email address, and the output is a simple string NameEmail requires email-validator to be installed; the input string must be either a valid email address or in the format Fred Bloggs <fred.bloggs@example.com> , and the output is a NameEmail object which has two properties: name and email . For Fred Bloggs <fred.bloggs@example.com> the name would be \"Fred Bloggs\" ; for fred.bloggs@example.com it would be \"fred.bloggs\" . PyObject expects a string and loads the Python object importable at that dotted path; e.g. if 'math.cos' was provided, the resulting field value would be the function cos Color for parsing HTML and CSS colors; see Color Type Json a special type wrapper which loads JSON before parsing; see JSON Type PaymentCardNumber for parsing and validating payment cards; see payment cards AnyUrl any URL; see URLs AnyHttpUrl an HTTP URL; see URLs HttpUrl a stricter HTTP URL; see URLs FileUrl a file path URL; see URLs PostgresDsn a postgres DSN style URL; see URLs CockroachDsn a cockroachdb DSN style URL; see URLs AmqpDsn an AMQP DSN style URL as used by RabbitMQ, StormMQ, ActiveMQ etc.; see URLs RedisDsn a redis DSN style URL; see URLs MongoDsn a MongoDB DSN style URL; see URLs KafkaDsn a kafka DSN style URL; see URLs stricturl a type method for arbitrary URL constraints; see URLs UUID1 requires a valid UUID of type 1; see UUID above UUID3 requires a valid UUID of type 3; see UUID above UUID4 requires a valid UUID of type 4; see UUID above UUID5 requires a valid UUID of type 5; see UUID above SecretBytes bytes where the value is kept partially secret; see Secrets SecretStr string where the value is kept partially secret; see Secrets IPvAnyAddress allows either an IPv4Address or an IPv6Address IPvAnyInterface allows either an IPv4Interface or an IPv6Interface IPvAnyNetwork allows either an IPv4Network or an IPv6Network NegativeFloat allows a float which is negative; uses standard float parsing then checks the value is less than 0; see Constrained Types NegativeInt allows an int which is negative; uses standard int parsing then checks the value is less than 0; see Constrained Types PositiveFloat allows a float which is positive; uses standard float parsing then checks the value is greater than 0; see Constrained Types PositiveInt allows an int which is positive; uses standard int parsing then checks the value is greater than 0; see Constrained Types conbytes type method for constraining bytes; see Constrained Types condecimal type method for constraining Decimals; see Constrained Types confloat type method for constraining floats; see Constrained Types conint type method for constraining ints; see Constrained Types condate type method for constraining dates; see Constrained Types conlist type method for constraining lists; see Constrained Types conset type method for constraining sets; see Constrained Types confrozenset type method for constraining frozen sets; see Constrained Types constr type method for constraining strs; see Constrained Types URLs \u00b6 For URI/URL validation the following types are available: AnyUrl : any scheme allowed, TLD not required, host required AnyHttpUrl : scheme http or https , TLD not required, host required HttpUrl : scheme http or https , TLD required, host required, max length 2083 FileUrl : scheme file , host not required PostgresDsn : user info required, TLD not required, host required, as of V.10 PostgresDsn supports multiple hosts. The following schemes are supported: postgres postgresql postgresql+asyncpg postgresql+pg8000 postgresql+psycopg postgresql+psycopg2 postgresql+psycopg2cffi postgresql+py-postgresql postgresql+pygresql CockroachDsn : scheme cockroachdb , user info required, TLD not required, host required. Also, its supported DBAPI dialects: cockroachdb+asyncpg cockroachdb+psycopg2 AmqpDsn : schema amqp or amqps , user info not required, TLD not required, host not required RedisDsn : scheme redis or rediss , user info not required, tld not required, host not required (CHANGED: user info) (e.g., rediss://:pass@localhost ) MongoDsn : scheme mongodb , user info not required, database name not required, port not required from v1.6 onwards), user info may be passed without user part (e.g., mongodb://mongodb0.example.com:27017 ) stricturl : method with the following keyword arguments: - strip_whitespace: bool = True - min_length: int = 1 - max_length: int = 2 ** 16 - tld_required: bool = True - host_required: bool = True - allowed_schemes: Optional[Set[str]] = None Warning In V1.10.0 and v1.10.1 stricturl also took an optional quote_plus argument and URL components were percent encoded in some cases. This feature was removed in v1.10.2, see #4470 for explanation and more details. The above types (which all inherit from AnyUrl ) will attempt to give descriptive errors when invalid URLs are provided: If you require a custom URI/URL type, it can be created in a similar way to the types defined above. URL Properties \u00b6 Assuming an input URL of http://samuel:pass@example.com:8000/the/path/?query=here#fragment=is;this=bit , the above types export the following properties: scheme : always set - the url scheme ( http above) host : always set - the url host ( example.com above) host_type : always set - describes the type of host, either: domain : e.g. example.com , int_domain : international domain, see below , e.g. exampl\u00a3e.org , ipv4 : an IP V4 address, e.g. 127.0.0.1 , or ipv6 : an IP V6 address, e.g. 2001:db8:ff00:42 user : optional - the username if included ( samuel above) password : optional - the password if included ( pass above) tld : optional - the top level domain ( com above), Note: this will be wrong for any two-level domain, e.g. \"co.uk\". You'll need to implement your own list of TLDs if you require full TLD validation port : optional - the port ( 8000 above) path : optional - the path ( /the/path/ above) query : optional - the URL query (aka GET arguments or \"search string\") ( query=here above) fragment : optional - the fragment ( fragment=is;this=bit above) If further validation is required, these properties can be used by validators to enforce specific behaviour: International Domains \u00b6 \"International domains\" (e.g. a URL where the host or TLD includes non-ascii characters) will be encoded via punycode (see this article for a good description of why this is important): Warning Underscores in Hostnames \u00b6 In pydantic underscores are allowed in all parts of a domain except the tld. Technically this might be wrong - in theory the hostname cannot have underscores, but subdomains can. To explain this; consider the following two cases: exam_ple.co.uk : the hostname is exam_ple , which should not be allowed since it contains an underscore foo_bar.example.com the hostname is example , which should be allowed since the underscore is in the subdomain Without having an exhaustive list of TLDs, it would be impossible to differentiate between these two. Therefore underscores are allowed, but you can always do further validation in a validator if desired. Also, Chrome, Firefox, and Safari all currently accept http://exam_ple.com as a URL, so we're in good (or at least big) company. Color Type \u00b6 You can use the Color data type for storing colors as per CSS3 specification . Colors can be defined via: name (e.g. \"Black\" , \"azure\" ) hexadecimal value (e.g. \"0x000\" , \"#FFFFFF\" , \"7fffd4\" ) RGB/RGBA tuples (e.g. (255, 255, 255) , (255, 255, 255, 0.5) ) RGB/RGBA strings (e.g. \"rgb(255, 255, 255)\" , \"rgba(255, 255, 255, 0.5)\" ) HSL strings (e.g. \"hsl(270, 60%, 70%)\" , \"hsl(270, 60%, 70%, .5)\" ) Color has the following methods: original the original string or tuple passed to Color as_named returns a named CSS3 color; fails if the alpha channel is set or no such color exists unless fallback=True is supplied, in which case it falls back to as_hex as_hex returns a string in the format #fff or #ffffff ; will contain 4 (or 8) hex values if the alpha channel is set, e.g. #7f33cc26 as_rgb returns a string in the format rgb(<red>, <green>, <blue>) , or rgba(<red>, <green>, <blue>, <alpha>) if the alpha channel is set as_rgb_tuple returns a 3- or 4-tuple in RGB(a) format. The alpha keyword argument can be used to define whether the alpha channel should be included; options: True - always include, False - never include, None (default) - include if set as_hsl string in the format hsl(<hue deg>, <saturation %>, <lightness %>) or hsl(<hue deg>, <saturation %>, <lightness %>, <alpha>) if the alpha channel is set as_hsl_tuple returns a 3- or 4-tuple in HSL(a) format. The alpha keyword argument can be used to define whether the alpha channel should be included; options: True - always include, False - never include, None (the default) - include if set The __str__ method for Color returns self.as_named(fallback=True) . Note the as_hsl* refer to hue, saturation, lightness \"HSL\" as used in html and most of the world, not \"HLS\" as used in Python's colorsys . Secret Types \u00b6 You can use the SecretStr and the SecretBytes data types for storing sensitive information that you do not want to be visible in logging or tracebacks. SecretStr and SecretBytes can be initialized idempotently or by using str or bytes literals respectively. The SecretStr and SecretBytes will be formatted as either '**********' or '' on conversion to json. Json Type \u00b6 You can use Json data type to make pydantic first load a raw JSON string. It can also optionally be used to parse the loaded object into another type base on the type Json is parameterised with: Payment Card Numbers \u00b6 The PaymentCardNumber type validates payment cards (such as a debit or credit card). PaymentCardBrand can be one of the following based on the BIN: PaymentCardBrand.amex PaymentCardBrand.mastercard PaymentCardBrand.visa PaymentCardBrand.other The actual validation verifies the card number is: a str of only digits luhn valid the correct length based on the BIN, if Amex, Mastercard or Visa, and between 12 and 19 digits for all other brands Constrained Types \u00b6 The value of numerous common types can be restricted using con* type functions: Where Field refers to the field function . Arguments to conlist \u00b6 The following arguments are available when using the conlist type function item_type: Type[T] : type of the list items min_items: int = None : minimum number of items in the list max_items: int = None : maximum number of items in the list unique_items: bool = None : enforces list elements to be unique Arguments to conset \u00b6 The following arguments are available when using the conset type function item_type: Type[T] : type of the set items min_items: int = None : minimum number of items in the set max_items: int = None : maximum number of items in the set Arguments to confrozenset \u00b6 The following arguments are available when using the confrozenset type function item_type: Type[T] : type of the frozenset items min_items: int = None : minimum number of items in the frozenset max_items: int = None : maximum number of items in the frozenset Arguments to conint \u00b6 The following arguments are available when using the conint type function strict: bool = False : controls type coercion gt: int = None : enforces integer to be greater than the set value ge: int = None : enforces integer to be greater than or equal to the set value lt: int = None : enforces integer to be less than the set value le: int = None : enforces integer to be less than or equal to the set value multiple_of: int = None : enforces integer to be a multiple of the set value Arguments to confloat \u00b6 The following arguments are available when using the confloat type function strict: bool = False : controls type coercion gt: float = None : enforces float to be greater than the set value ge: float = None : enforces float to be greater than or equal to the set value lt: float = None : enforces float to be less than the set value le: float = None : enforces float to be less than or equal to the set value multiple_of: float = None : enforces float to be a multiple of the set value allow_inf_nan: bool = True : whether to allows infinity ( +inf an -inf ) and NaN values, defaults to True , set to False for compatibility with JSON , see #3994 for more details, added in V1.10 Arguments to condecimal \u00b6 The following arguments are available when using the condecimal type function gt: Decimal = None : enforces decimal to be greater than the set value ge: Decimal = None : enforces decimal to be greater than or equal to the set value lt: Decimal = None : enforces decimal to be less than the set value le: Decimal = None : enforces decimal to be less than or equal to the set value max_digits: int = None : maximum number of digits within the decimal. it does not include a zero before the decimal point or trailing decimal zeroes decimal_places: int = None : max number of decimal places allowed. it does not include trailing decimal zeroes multiple_of: Decimal = None : enforces decimal to be a multiple of the set value Arguments to constr \u00b6 The following arguments are available when using the constr type function strip_whitespace: bool = False : removes leading and trailing whitespace to_upper: bool = False : turns all characters to uppercase to_lower: bool = False : turns all characters to lowercase strict: bool = False : controls type coercion min_length: int = None : minimum length of the string max_length: int = None : maximum length of the string curtail_length: int = None : shrinks the string length to the set value when it is longer than the set value regex: str = None : regex to validate the string against Arguments to conbytes \u00b6 The following arguments are available when using the conbytes type function strip_whitespace: bool = False : removes leading and trailing whitespace to_upper: bool = False : turns all characters to uppercase to_lower: bool = False : turns all characters to lowercase min_length: int = None : minimum length of the byte string max_length: int = None : maximum length of the byte string strict: bool = False : controls type coercion Arguments to condate \u00b6 The following arguments are available when using the condate type function gt: date = None : enforces date to be greater than the set value ge: date = None : enforces date to be greater than or equal to the set value lt: date = None : enforces date to be less than the set value le: date = None : enforces date to be less than or equal to the set value Strict Types \u00b6 You can use the StrictStr , StrictBytes , StrictInt , StrictFloat , and StrictBool types to prevent coercion from compatible types. These types will only pass validation when the validated value is of the respective type or is a subtype of that type. This behavior is also exposed via the strict field of the ConstrainedStr , ConstrainedBytes , ConstrainedFloat and ConstrainedInt classes and can be combined with a multitude of complex validation rules. The following caveats apply: StrictBytes (and the strict option of ConstrainedBytes ) will accept both bytes , and bytearray types. StrictInt (and the strict option of ConstrainedInt ) will not accept bool types, even though bool is a subclass of int in Python. Other subclasses will work. StrictFloat (and the strict option of ConstrainedFloat ) will not accept int . ByteSize \u00b6 You can use the ByteSize data type to convert byte string representation to raw bytes and print out human readable versions of the bytes as well. Info Note that 1b will be parsed as \"1 byte\" and not \"1 bit\". Custom Data Types \u00b6 You can also define your own custom data types. There are several ways to achieve it. Classes with __get_validators__ \u00b6 You use a custom class with a classmethod __get_validators__ . It will be called to get validators to parse and validate the input data. Tip These validators have the same semantics as in Validators , you can declare a parameter config , field , etc. Similar validation could be achieved using constr(regex=...) except the value won't be formatted with a space, the schema would just include the full pattern and the returned value would be a vanilla string. See schema for more details on how the model's schema is generated. Arbitrary Types Allowed \u00b6 You can allow arbitrary types using the arbitrary_types_allowed config in the Model Config . Undefined Types Warning \u00b6 You can suppress the Undefined Types Warning by setting undefined_types_warning to False in the Model Config . Generic Classes as Types \u00b6 Warning This is an advanced technique that you might not need in the beginning. In most of the cases you will probably be fine with standard pydantic models. You can use Generic Classes as field types and perform custom validation based on the \"type parameters\" (or sub-types) with __get_validators__ . If the Generic class that you are using as a sub-type has a classmethod __get_validators__ you don't need to use arbitrary_types_allowed for it to work. Because you can declare validators that receive the current field , you can extract the sub_fields (from the generic class type parameters) and validate data with them.","title":"Field Types"},{"location":"usage/types/#standard-library-types","text":"pydantic supports many common types from the Python standard library. If you need stricter processing see Strict Types ; if you need to constrain the values allowed (e.g. to require a positive int) see Constrained Types . None , type(None) or Literal[None] (equivalent according to PEP 484 ) allows only None value bool see Booleans below for details on how bools are validated and what values are permitted int pydantic uses int(v) to coerce types to an int ; see this warning on loss of information during data conversion float similarly, float(v) is used to coerce values to floats str strings are accepted as-is, int float and Decimal are coerced using str(v) , bytes and bytearray are converted using v.decode() , enums inheriting from str are converted using v.value , and all other types cause an error bytes bytes are accepted as-is, bytearray is converted using bytes(v) , str are converted using v.encode() , and int , float , and Decimal are coerced using str(v).encode() list allows list , tuple , set , frozenset , deque , or generators and casts to a list; see typing.List below for sub-type constraints tuple allows list , tuple , set , frozenset , deque , or generators and casts to a tuple; see typing.Tuple below for sub-type constraints dict dict(v) is used to attempt to convert a dictionary; see typing.Dict below for sub-type constraints set allows list , tuple , set , frozenset , deque , or generators and casts to a set; see typing.Set below for sub-type constraints frozenset allows list , tuple , set , frozenset , deque , or generators and casts to a frozen set; see typing.FrozenSet below for sub-type constraints deque allows list , tuple , set , frozenset , deque , or generators and casts to a deque; see typing.Deque below for sub-type constraints datetime.date see Datetime Types below for more detail on parsing and validation datetime.time see Datetime Types below for more detail on parsing and validation datetime.datetime see Datetime Types below for more detail on parsing and validation datetime.timedelta see Datetime Types below for more detail on parsing and validation typing.Any allows any value including None , thus an Any field is optional typing.Annotated allows wrapping another type with arbitrary metadata, as per PEP-593 . The Annotated hint may contain a single call to the Field function , but otherwise the additional metadata is ignored and the root type is used. typing.TypeVar constrains the values allowed based on constraints or bound , see TypeVar typing.Union see Unions below for more detail on parsing and validation typing.Optional Optional[x] is simply short hand for Union[x, None] ; see Unions below for more detail on parsing and validation and Required Fields for details about required fields that can receive None as a value. typing.List see Typing Iterables below for more detail on parsing and validation typing.Tuple see Typing Iterables below for more detail on parsing and validation subclass of typing.NamedTuple Same as tuple but instantiates with the given namedtuple and validates fields since they are annotated. See Annotated Types below for more detail on parsing and validation subclass of collections.namedtuple Same as subclass of typing.NamedTuple but all fields will have type Any since they are not annotated typing.Dict see Typing Iterables below for more detail on parsing and validation subclass of typing.TypedDict Same as dict but pydantic will validate the dictionary since keys are annotated. See Annotated Types below for more detail on parsing and validation typing.Set see Typing Iterables below for more detail on parsing and validation typing.FrozenSet see Typing Iterables below for more detail on parsing and validation typing.Deque see Typing Iterables below for more detail on parsing and validation typing.Sequence see Typing Iterables below for more detail on parsing and validation typing.Iterable this is reserved for iterables that shouldn't be consumed. See Infinite Generators below for more detail on parsing and validation typing.Type see Type below for more detail on parsing and validation typing.Callable see Callable below for more detail on parsing and validation typing.Pattern will cause the input value to be passed to re.compile(v) to create a regex pattern ipaddress.IPv4Address simply uses the type itself for validation by passing the value to IPv4Address(v) ; see Pydantic Types for other custom IP address types ipaddress.IPv4Interface simply uses the type itself for validation by passing the value to IPv4Address(v) ; see Pydantic Types for other custom IP address types ipaddress.IPv4Network simply uses the type itself for validation by passing the value to IPv4Network(v) ; see Pydantic Types for other custom IP address types ipaddress.IPv6Address simply uses the type itself for validation by passing the value to IPv6Address(v) ; see Pydantic Types for other custom IP address types ipaddress.IPv6Interface simply uses the type itself for validation by passing the value to IPv6Interface(v) ; see Pydantic Types for other custom IP address types ipaddress.IPv6Network simply uses the type itself for validation by passing the value to IPv6Network(v) ; see Pydantic Types for other custom IP address types enum.Enum checks that the value is a valid Enum instance subclass of enum.Enum checks that the value is a valid member of the enum; see Enums and Choices for more details enum.IntEnum checks that the value is a valid IntEnum instance subclass of enum.IntEnum checks that the value is a valid member of the integer enum; see Enums and Choices for more details decimal.Decimal pydantic attempts to convert the value to a string, then passes the string to Decimal(v) pathlib.Path simply uses the type itself for validation by passing the value to Path(v) ; see Pydantic Types for other more strict path types uuid.UUID strings and bytes (converted to strings) are passed to UUID(v) , with a fallback to UUID(bytes=v) for bytes and bytearray ; see Pydantic Types for other stricter UUID types ByteSize converts a bytes string with units to bytes","title":"Standard Library Types"},{"location":"usage/types/#typing-iterables","text":"pydantic uses standard library typing types as defined in PEP 484 to define complex objects.","title":"Typing Iterables"},{"location":"usage/types/#infinite-generators","text":"If you have a generator you can use Sequence as described above. In that case, the generator will be consumed and stored on the model as a list and its values will be validated with the sub-type of Sequence (e.g. int in Sequence[int] ). But if you have a generator that you don't want to be consumed, e.g. an infinite generator or a remote data loader, you can define its type with Iterable : Warning Iterable fields only perform a simple check that the argument is iterable and won't be consumed. No validation of their values is performed as it cannot be done without consuming the iterable. Tip If you want to validate the values of an infinite generator you can create a separate model and use it while consuming the generator, reporting the validation errors as appropriate. pydantic can't validate the values automatically for you because it would require consuming the infinite generator.","title":"Infinite Generators"},{"location":"usage/types/#validating-the-first-value","text":"You can create a validator to validate the first value in an infinite generator and still not consume it entirely.","title":"Validating the first value"},{"location":"usage/types/#unions","text":"The Union type allows a model attribute to accept different types, e.g.: Info You may get unexpected coercion with Union ; see below. Know that you can also make the check slower but stricter by using Smart Union However, as can be seen above, pydantic will attempt to 'match' any of the types defined under Union and will use the first one that matches. In the above example the id of user_03 was defined as a uuid.UUID class (which is defined under the attribute's Union annotation) but as the uuid.UUID can be marshalled into an int it chose to match against the int type and disregarded the other types. Warning typing.Union also ignores order when defined , so Union[int, float] == Union[float, int] which can lead to unexpected behaviour when combined with matching based on the Union type order inside other type definitions, such as List and Dict types (because Python treats these definitions as singletons). For example, Dict[str, Union[int, float]] == Dict[str, Union[float, int]] with the order based on the first time it was defined. Please note that this can also be affected by third party libraries and their internal type definitions and the import orders. As such, it is recommended that, when defining Union annotations, the most specific type is included first and followed by less specific types. In the above example, the UUID class should precede the int and str classes to preclude the unexpected representation as such: Tip The type Optional[x] is a shorthand for Union[x, None] . Optional[x] can also be used to specify a required field that can take None as a value. See more details in Required Fields .","title":"Unions"},{"location":"usage/types/#discriminated-unions-aka-tagged-unions","text":"When Union is used with multiple submodels, you sometimes know exactly which submodel needs to be checked and validated and want to enforce this. To do that you can set the same field - let's call it my_discriminator - in each of the submodels with a discriminated value, which is one (or many) Literal value(s). For your Union , you can set the discriminator in its value: Field(discriminator='my_discriminator') . Setting a discriminated union has many benefits: validation is faster since it is only attempted against one model only one explicit error is raised in case of failure the generated JSON schema implements the associated OpenAPI specification Note Using the Annotated Fields syntax can be handy to regroup the Union and discriminator information. See below for an example! Warning Discriminated unions cannot be used with only a single variant, such as Union[Cat] . Python changes Union[T] into T at interpretation time, so it is not possible for pydantic to distinguish fields of Union[T] from T .","title":"Discriminated Unions (a.k.a. Tagged Unions)"},{"location":"usage/types/#nested-discriminated-unions","text":"Only one discriminator can be set for a field but sometimes you want to combine multiple discriminators. In this case you can always create \"intermediate\" models with __root__ and add your discriminator.","title":"Nested Discriminated Unions"},{"location":"usage/types/#enums-and-choices","text":"pydantic uses Python's standard enum classes to define choices.","title":"Enums and Choices"},{"location":"usage/types/#datetime-types","text":"Pydantic supports the following datetime types: datetime fields can be: datetime , existing datetime object int or float , assumed as Unix time, i.e. seconds (if >= -2e10 or <= 2e10 ) or milliseconds (if < -2e10 or > 2e10 ) since 1 January 1970 str , following formats work: YYYY-MM-DD[T]HH:MM[:SS[.ffffff]][Z or [\u00b1]HH[:]MM] int or float as a string (assumed as Unix time) date fields can be: date , existing date object int or float , see datetime str , following formats work: YYYY-MM-DD int or float , see datetime time fields can be: time , existing time object str , following formats work: HH:MM[:SS[.ffffff]][Z or [\u00b1]HH[:]MM] timedelta fields can be: timedelta , existing timedelta object int or float , assumed as seconds str , following formats work: [-][DD ][HH:MM]SS[.ffffff] [\u00b1]P[DD]DT[HH]H[MM]M[SS]S ( ISO 8601 format for timedelta)","title":"Datetime Types"},{"location":"usage/types/#booleans","text":"Warning The logic for parsing bool fields has changed as of version v1.0 . Prior to v1.0 , bool parsing never failed, leading to some unexpected results. The new logic is described below. A standard bool field will raise a ValidationError if the value is not one of the following: A valid boolean (i.e. True or False ), The integers 0 or 1 , a str which when converted to lower case is one of '0', 'off', 'f', 'false', 'n', 'no', '1', 'on', 't', 'true', 'y', 'yes' a bytes which is valid (per the previous rule) when decoded to str Note If you want stricter boolean logic (e.g. a field which only permits True and False ) you can use StrictBool . Here is a script demonstrating some of these behaviors:","title":"Booleans"},{"location":"usage/types/#callable","text":"Fields can also be of type Callable : Warning Callable fields only perform a simple check that the argument is callable; no validation of arguments, their types, or the return type is performed.","title":"Callable"},{"location":"usage/types/#type","text":"pydantic supports the use of Type[T] to specify that a field may only accept classes (not instances) that are subclasses of T . You may also use Type to specify that any class is allowed.","title":"Type"},{"location":"usage/types/#typevar","text":"TypeVar is supported either unconstrained, constrained or with a bound.","title":"TypeVar"},{"location":"usage/types/#literal-type","text":"Note This is a new feature of the Python standard library as of Python 3.8; prior to Python 3.8, it requires the typing-extensions package. pydantic supports the use of typing.Literal (or typing_extensions.Literal prior to Python 3.8) as a lightweight way to specify that a field may accept only specific literal values: One benefit of this field type is that it can be used to check for equality with one or more specific values without needing to declare custom validators: With proper ordering in an annotated Union , you can use this to parse types of decreasing specificity:","title":"Literal Type"},{"location":"usage/types/#annotated-types","text":"","title":"Annotated Types"},{"location":"usage/types/#namedtuple","text":"","title":"NamedTuple"},{"location":"usage/types/#typeddict","text":"Note This is a new feature of the Python standard library as of Python 3.8. Prior to Python 3.8, it requires the typing-extensions package. But required and optional fields are properly differentiated only since Python 3.9. We therefore recommend using typing-extensions with Python 3.8 as well.","title":"TypedDict"},{"location":"usage/types/#pydantic-types","text":"pydantic also provides a variety of other useful types: FilePath like Path , but the path must exist and be a file DirectoryPath like Path , but the path must exist and be a directory PastDate like date , but the date should be in the past FutureDate like date , but the date should be in the future EmailStr requires email-validator to be installed; the input string must be a valid email address, and the output is a simple string NameEmail requires email-validator to be installed; the input string must be either a valid email address or in the format Fred Bloggs <fred.bloggs@example.com> , and the output is a NameEmail object which has two properties: name and email . For Fred Bloggs <fred.bloggs@example.com> the name would be \"Fred Bloggs\" ; for fred.bloggs@example.com it would be \"fred.bloggs\" . PyObject expects a string and loads the Python object importable at that dotted path; e.g. if 'math.cos' was provided, the resulting field value would be the function cos Color for parsing HTML and CSS colors; see Color Type Json a special type wrapper which loads JSON before parsing; see JSON Type PaymentCardNumber for parsing and validating payment cards; see payment cards AnyUrl any URL; see URLs AnyHttpUrl an HTTP URL; see URLs HttpUrl a stricter HTTP URL; see URLs FileUrl a file path URL; see URLs PostgresDsn a postgres DSN style URL; see URLs CockroachDsn a cockroachdb DSN style URL; see URLs AmqpDsn an AMQP DSN style URL as used by RabbitMQ, StormMQ, ActiveMQ etc.; see URLs RedisDsn a redis DSN style URL; see URLs MongoDsn a MongoDB DSN style URL; see URLs KafkaDsn a kafka DSN style URL; see URLs stricturl a type method for arbitrary URL constraints; see URLs UUID1 requires a valid UUID of type 1; see UUID above UUID3 requires a valid UUID of type 3; see UUID above UUID4 requires a valid UUID of type 4; see UUID above UUID5 requires a valid UUID of type 5; see UUID above SecretBytes bytes where the value is kept partially secret; see Secrets SecretStr string where the value is kept partially secret; see Secrets IPvAnyAddress allows either an IPv4Address or an IPv6Address IPvAnyInterface allows either an IPv4Interface or an IPv6Interface IPvAnyNetwork allows either an IPv4Network or an IPv6Network NegativeFloat allows a float which is negative; uses standard float parsing then checks the value is less than 0; see Constrained Types NegativeInt allows an int which is negative; uses standard int parsing then checks the value is less than 0; see Constrained Types PositiveFloat allows a float which is positive; uses standard float parsing then checks the value is greater than 0; see Constrained Types PositiveInt allows an int which is positive; uses standard int parsing then checks the value is greater than 0; see Constrained Types conbytes type method for constraining bytes; see Constrained Types condecimal type method for constraining Decimals; see Constrained Types confloat type method for constraining floats; see Constrained Types conint type method for constraining ints; see Constrained Types condate type method for constraining dates; see Constrained Types conlist type method for constraining lists; see Constrained Types conset type method for constraining sets; see Constrained Types confrozenset type method for constraining frozen sets; see Constrained Types constr type method for constraining strs; see Constrained Types","title":"Pydantic Types"},{"location":"usage/types/#urls","text":"For URI/URL validation the following types are available: AnyUrl : any scheme allowed, TLD not required, host required AnyHttpUrl : scheme http or https , TLD not required, host required HttpUrl : scheme http or https , TLD required, host required, max length 2083 FileUrl : scheme file , host not required PostgresDsn : user info required, TLD not required, host required, as of V.10 PostgresDsn supports multiple hosts. The following schemes are supported: postgres postgresql postgresql+asyncpg postgresql+pg8000 postgresql+psycopg postgresql+psycopg2 postgresql+psycopg2cffi postgresql+py-postgresql postgresql+pygresql CockroachDsn : scheme cockroachdb , user info required, TLD not required, host required. Also, its supported DBAPI dialects: cockroachdb+asyncpg cockroachdb+psycopg2 AmqpDsn : schema amqp or amqps , user info not required, TLD not required, host not required RedisDsn : scheme redis or rediss , user info not required, tld not required, host not required (CHANGED: user info) (e.g., rediss://:pass@localhost ) MongoDsn : scheme mongodb , user info not required, database name not required, port not required from v1.6 onwards), user info may be passed without user part (e.g., mongodb://mongodb0.example.com:27017 ) stricturl : method with the following keyword arguments: - strip_whitespace: bool = True - min_length: int = 1 - max_length: int = 2 ** 16 - tld_required: bool = True - host_required: bool = True - allowed_schemes: Optional[Set[str]] = None Warning In V1.10.0 and v1.10.1 stricturl also took an optional quote_plus argument and URL components were percent encoded in some cases. This feature was removed in v1.10.2, see #4470 for explanation and more details. The above types (which all inherit from AnyUrl ) will attempt to give descriptive errors when invalid URLs are provided: If you require a custom URI/URL type, it can be created in a similar way to the types defined above.","title":"URLs"},{"location":"usage/types/#url-properties","text":"Assuming an input URL of http://samuel:pass@example.com:8000/the/path/?query=here#fragment=is;this=bit , the above types export the following properties: scheme : always set - the url scheme ( http above) host : always set - the url host ( example.com above) host_type : always set - describes the type of host, either: domain : e.g. example.com , int_domain : international domain, see below , e.g. exampl\u00a3e.org , ipv4 : an IP V4 address, e.g. 127.0.0.1 , or ipv6 : an IP V6 address, e.g. 2001:db8:ff00:42 user : optional - the username if included ( samuel above) password : optional - the password if included ( pass above) tld : optional - the top level domain ( com above), Note: this will be wrong for any two-level domain, e.g. \"co.uk\". You'll need to implement your own list of TLDs if you require full TLD validation port : optional - the port ( 8000 above) path : optional - the path ( /the/path/ above) query : optional - the URL query (aka GET arguments or \"search string\") ( query=here above) fragment : optional - the fragment ( fragment=is;this=bit above) If further validation is required, these properties can be used by validators to enforce specific behaviour:","title":"URL Properties"},{"location":"usage/types/#international-domains","text":"\"International domains\" (e.g. a URL where the host or TLD includes non-ascii characters) will be encoded via punycode (see this article for a good description of why this is important): Warning","title":"International Domains"},{"location":"usage/types/#underscores-in-hostnames","text":"In pydantic underscores are allowed in all parts of a domain except the tld. Technically this might be wrong - in theory the hostname cannot have underscores, but subdomains can. To explain this; consider the following two cases: exam_ple.co.uk : the hostname is exam_ple , which should not be allowed since it contains an underscore foo_bar.example.com the hostname is example , which should be allowed since the underscore is in the subdomain Without having an exhaustive list of TLDs, it would be impossible to differentiate between these two. Therefore underscores are allowed, but you can always do further validation in a validator if desired. Also, Chrome, Firefox, and Safari all currently accept http://exam_ple.com as a URL, so we're in good (or at least big) company.","title":"Underscores in Hostnames"},{"location":"usage/types/#color-type","text":"You can use the Color data type for storing colors as per CSS3 specification . Colors can be defined via: name (e.g. \"Black\" , \"azure\" ) hexadecimal value (e.g. \"0x000\" , \"#FFFFFF\" , \"7fffd4\" ) RGB/RGBA tuples (e.g. (255, 255, 255) , (255, 255, 255, 0.5) ) RGB/RGBA strings (e.g. \"rgb(255, 255, 255)\" , \"rgba(255, 255, 255, 0.5)\" ) HSL strings (e.g. \"hsl(270, 60%, 70%)\" , \"hsl(270, 60%, 70%, .5)\" ) Color has the following methods: original the original string or tuple passed to Color as_named returns a named CSS3 color; fails if the alpha channel is set or no such color exists unless fallback=True is supplied, in which case it falls back to as_hex as_hex returns a string in the format #fff or #ffffff ; will contain 4 (or 8) hex values if the alpha channel is set, e.g. #7f33cc26 as_rgb returns a string in the format rgb(<red>, <green>, <blue>) , or rgba(<red>, <green>, <blue>, <alpha>) if the alpha channel is set as_rgb_tuple returns a 3- or 4-tuple in RGB(a) format. The alpha keyword argument can be used to define whether the alpha channel should be included; options: True - always include, False - never include, None (default) - include if set as_hsl string in the format hsl(<hue deg>, <saturation %>, <lightness %>) or hsl(<hue deg>, <saturation %>, <lightness %>, <alpha>) if the alpha channel is set as_hsl_tuple returns a 3- or 4-tuple in HSL(a) format. The alpha keyword argument can be used to define whether the alpha channel should be included; options: True - always include, False - never include, None (the default) - include if set The __str__ method for Color returns self.as_named(fallback=True) . Note the as_hsl* refer to hue, saturation, lightness \"HSL\" as used in html and most of the world, not \"HLS\" as used in Python's colorsys .","title":"Color Type"},{"location":"usage/types/#secret-types","text":"You can use the SecretStr and the SecretBytes data types for storing sensitive information that you do not want to be visible in logging or tracebacks. SecretStr and SecretBytes can be initialized idempotently or by using str or bytes literals respectively. The SecretStr and SecretBytes will be formatted as either '**********' or '' on conversion to json.","title":"Secret Types"},{"location":"usage/types/#json-type","text":"You can use Json data type to make pydantic first load a raw JSON string. It can also optionally be used to parse the loaded object into another type base on the type Json is parameterised with:","title":"Json Type"},{"location":"usage/types/#payment-card-numbers","text":"The PaymentCardNumber type validates payment cards (such as a debit or credit card). PaymentCardBrand can be one of the following based on the BIN: PaymentCardBrand.amex PaymentCardBrand.mastercard PaymentCardBrand.visa PaymentCardBrand.other The actual validation verifies the card number is: a str of only digits luhn valid the correct length based on the BIN, if Amex, Mastercard or Visa, and between 12 and 19 digits for all other brands","title":"Payment Card Numbers"},{"location":"usage/types/#constrained-types","text":"The value of numerous common types can be restricted using con* type functions: Where Field refers to the field function .","title":"Constrained Types"},{"location":"usage/types/#arguments-to-conlist","text":"The following arguments are available when using the conlist type function item_type: Type[T] : type of the list items min_items: int = None : minimum number of items in the list max_items: int = None : maximum number of items in the list unique_items: bool = None : enforces list elements to be unique","title":"Arguments to conlist"},{"location":"usage/types/#arguments-to-conset","text":"The following arguments are available when using the conset type function item_type: Type[T] : type of the set items min_items: int = None : minimum number of items in the set max_items: int = None : maximum number of items in the set","title":"Arguments to conset"},{"location":"usage/types/#arguments-to-confrozenset","text":"The following arguments are available when using the confrozenset type function item_type: Type[T] : type of the frozenset items min_items: int = None : minimum number of items in the frozenset max_items: int = None : maximum number of items in the frozenset","title":"Arguments to confrozenset"},{"location":"usage/types/#arguments-to-conint","text":"The following arguments are available when using the conint type function strict: bool = False : controls type coercion gt: int = None : enforces integer to be greater than the set value ge: int = None : enforces integer to be greater than or equal to the set value lt: int = None : enforces integer to be less than the set value le: int = None : enforces integer to be less than or equal to the set value multiple_of: int = None : enforces integer to be a multiple of the set value","title":"Arguments to conint"},{"location":"usage/types/#arguments-to-confloat","text":"The following arguments are available when using the confloat type function strict: bool = False : controls type coercion gt: float = None : enforces float to be greater than the set value ge: float = None : enforces float to be greater than or equal to the set value lt: float = None : enforces float to be less than the set value le: float = None : enforces float to be less than or equal to the set value multiple_of: float = None : enforces float to be a multiple of the set value allow_inf_nan: bool = True : whether to allows infinity ( +inf an -inf ) and NaN values, defaults to True , set to False for compatibility with JSON , see #3994 for more details, added in V1.10","title":"Arguments to confloat"},{"location":"usage/types/#arguments-to-condecimal","text":"The following arguments are available when using the condecimal type function gt: Decimal = None : enforces decimal to be greater than the set value ge: Decimal = None : enforces decimal to be greater than or equal to the set value lt: Decimal = None : enforces decimal to be less than the set value le: Decimal = None : enforces decimal to be less than or equal to the set value max_digits: int = None : maximum number of digits within the decimal. it does not include a zero before the decimal point or trailing decimal zeroes decimal_places: int = None : max number of decimal places allowed. it does not include trailing decimal zeroes multiple_of: Decimal = None : enforces decimal to be a multiple of the set value","title":"Arguments to condecimal"},{"location":"usage/types/#arguments-to-constr","text":"The following arguments are available when using the constr type function strip_whitespace: bool = False : removes leading and trailing whitespace to_upper: bool = False : turns all characters to uppercase to_lower: bool = False : turns all characters to lowercase strict: bool = False : controls type coercion min_length: int = None : minimum length of the string max_length: int = None : maximum length of the string curtail_length: int = None : shrinks the string length to the set value when it is longer than the set value regex: str = None : regex to validate the string against","title":"Arguments to constr"},{"location":"usage/types/#arguments-to-conbytes","text":"The following arguments are available when using the conbytes type function strip_whitespace: bool = False : removes leading and trailing whitespace to_upper: bool = False : turns all characters to uppercase to_lower: bool = False : turns all characters to lowercase min_length: int = None : minimum length of the byte string max_length: int = None : maximum length of the byte string strict: bool = False : controls type coercion","title":"Arguments to conbytes"},{"location":"usage/types/#arguments-to-condate","text":"The following arguments are available when using the condate type function gt: date = None : enforces date to be greater than the set value ge: date = None : enforces date to be greater than or equal to the set value lt: date = None : enforces date to be less than the set value le: date = None : enforces date to be less than or equal to the set value","title":"Arguments to condate"},{"location":"usage/types/#strict-types","text":"You can use the StrictStr , StrictBytes , StrictInt , StrictFloat , and StrictBool types to prevent coercion from compatible types. These types will only pass validation when the validated value is of the respective type or is a subtype of that type. This behavior is also exposed via the strict field of the ConstrainedStr , ConstrainedBytes , ConstrainedFloat and ConstrainedInt classes and can be combined with a multitude of complex validation rules. The following caveats apply: StrictBytes (and the strict option of ConstrainedBytes ) will accept both bytes , and bytearray types. StrictInt (and the strict option of ConstrainedInt ) will not accept bool types, even though bool is a subclass of int in Python. Other subclasses will work. StrictFloat (and the strict option of ConstrainedFloat ) will not accept int .","title":"Strict Types"},{"location":"usage/types/#bytesize","text":"You can use the ByteSize data type to convert byte string representation to raw bytes and print out human readable versions of the bytes as well. Info Note that 1b will be parsed as \"1 byte\" and not \"1 bit\".","title":"ByteSize"},{"location":"usage/types/#custom-data-types","text":"You can also define your own custom data types. There are several ways to achieve it.","title":"Custom Data Types"},{"location":"usage/types/#classes-with-__get_validators__","text":"You use a custom class with a classmethod __get_validators__ . It will be called to get validators to parse and validate the input data. Tip These validators have the same semantics as in Validators , you can declare a parameter config , field , etc. Similar validation could be achieved using constr(regex=...) except the value won't be formatted with a space, the schema would just include the full pattern and the returned value would be a vanilla string. See schema for more details on how the model's schema is generated.","title":"Classes with __get_validators__"},{"location":"usage/types/#arbitrary-types-allowed","text":"You can allow arbitrary types using the arbitrary_types_allowed config in the Model Config .","title":"Arbitrary Types Allowed"},{"location":"usage/types/#undefined-types-warning","text":"You can suppress the Undefined Types Warning by setting undefined_types_warning to False in the Model Config .","title":"Undefined Types Warning"},{"location":"usage/types/#generic-classes-as-types","text":"Warning This is an advanced technique that you might not need in the beginning. In most of the cases you will probably be fine with standard pydantic models. You can use Generic Classes as field types and perform custom validation based on the \"type parameters\" (or sub-types) with __get_validators__ . If the Generic class that you are using as a sub-type has a classmethod __get_validators__ you don't need to use arbitrary_types_allowed for it to work. Because you can declare validators that receive the current field , you can extract the sub_fields (from the generic class type parameters) and validate data with them.","title":"Generic Classes as Types"},{"location":"usage/validation_decorator/","text":"The validate_arguments decorator allows the arguments passed to a function to be parsed and validated using the function's annotations before the function is called. While under the hood this uses the same approach of model creation and initialisation; it provides an extremely easy way to apply validation to your code with minimal boilerplate. In Beta The validate_arguments decorator is in beta , it has been added to pydantic in v1.5 on a provisional basis . It may change significantly in future releases and its interface will not be concrete until v2 . Feedback from the community while it's still provisional would be extremely useful; either comment on #1205 or create a new issue. Example of usage: Argument Types \u00b6 Argument types are inferred from type annotations on the function, arguments without a type decorator are considered as Any . Since validate_arguments internally uses a standard BaseModel , all types listed in types can be validated, including pydantic models and custom types . As with the rest of pydantic , types can be coerced by the decorator before they're passed to the actual function: A few notes: though they're passed as strings, path and regex are converted to a Path object and regex respectively by the decorator max has no type annotation, so will be considered as Any by the decorator Type coercion like this can be extremely helpful but also confusing or not desired, see below for a discussion of validate_arguments 's limitations in this regard. Function Signatures \u00b6 The decorator is designed to work with functions using all possible parameter configurations and all possible combinations of these: positional or keyword arguments with or without defaults variable positional arguments defined via * (often *args ) variable keyword arguments defined via ** (often **kwargs ) keyword only arguments - arguments after *, positional only arguments - arguments before , / (new in Python 3.8) To demonstrate all the above parameter types: Using Field to describe function arguments \u00b6 Field can also be used with validate_arguments to provide extra information about the field and validations. In general it should be used in a type hint with Annotated , unless default_factory is specified, in which case it should be used as the default value of the field: The alias can be used with the decorator as normal. Usage with mypy \u00b6 The validate_arguments decorator should work \"out of the box\" with mypy since it's defined to return a function with the same signature as the function it decorates. The only limitation is that since we trick mypy into thinking the function returned by the decorator is the same as the function being decorated; access to the raw function or other attributes will require type: ignore . Validate without calling the function \u00b6 By default, arguments validation is done by directly calling the decorated function with parameters. But what if you wanted to validate them without actually calling the function? To do that you can call the validate method bound to the decorated function. Raw function \u00b6 The raw function which was decorated is accessible, this is useful if in some scenarios you trust your input arguments and want to call the function in the most performant way (see notes on performance below): Async Functions \u00b6 validate_arguments can also be used on async functions: Custom Config \u00b6 The model behind validate_arguments can be customised using a config setting which is equivalent to setting the Config sub-class in normal models. Warning The fields and alias_generator properties of Config which allow aliases to be configured are not supported yet with @validate_arguments , using them will raise an error. Configuration is set using the config keyword argument to the decorator, it may be either a config class or a dict of properties which are converted to a class later. Limitations \u00b6 validate_arguments has been released on a provisional basis without all the bells and whistles, which may be added later, see #1205 for some more discussion of this. In particular: Validation Exception \u00b6 Currently upon validation failure, a standard pydantic ValidationError is raised, see model error handling . This is helpful since it's str() method provides useful details of the error which occurred and methods like .errors() and .json() can be useful when exposing the errors to end users, however ValidationError inherits from ValueError not TypeError which may be unexpected since Python would raise a TypeError upon invalid or missing arguments. This may be addressed in future by either allow a custom error or raising a different exception by default, or both. Coercion and Strictness \u00b6 pydantic currently leans on the side of trying to coerce types rather than raise an error if a type is wrong, see model data conversion and validate_arguments is no different. See #1098 and other issues with the \"strictness\" label for a discussion of this. If pydantic gets a \"strict\" mode in future, validate_arguments will have an option to use this, it may even become the default for the decorator. Performance \u00b6 We've made a big effort to make pydantic as performant as possible and argument inspect and model creation is only performed once when the function is defined, however there will still be a performance impact to using the validate_arguments decorator compared to calling the raw function. In many situations this will have little or no noticeable effect, however be aware that validate_arguments is not an equivalent or alternative to function definitions in strongly typed languages; it never will be. Return Value \u00b6 The return value of the function is not validated against its return type annotation, this may be added as an option in future. Config and Validators \u00b6 fields and alias_generator on custom Config are not supported, see above . Neither are validators . Model fields and reserved arguments \u00b6 The following names may not be used by arguments since they can be used internally to store information about the function's signature: v__args v__kwargs v__positional_only These names (together with \"args\" and \"kwargs\" ) may or may not (depending on the function's signature) appear as fields on the internal pydantic model accessible via .model thus this model isn't especially useful (e.g. for generating a schema) at the moment. This should be fixable in future as the way error are raised is changed.","title":"Validation decorator"},{"location":"usage/validation_decorator/#argument-types","text":"Argument types are inferred from type annotations on the function, arguments without a type decorator are considered as Any . Since validate_arguments internally uses a standard BaseModel , all types listed in types can be validated, including pydantic models and custom types . As with the rest of pydantic , types can be coerced by the decorator before they're passed to the actual function: A few notes: though they're passed as strings, path and regex are converted to a Path object and regex respectively by the decorator max has no type annotation, so will be considered as Any by the decorator Type coercion like this can be extremely helpful but also confusing or not desired, see below for a discussion of validate_arguments 's limitations in this regard.","title":"Argument Types"},{"location":"usage/validation_decorator/#function-signatures","text":"The decorator is designed to work with functions using all possible parameter configurations and all possible combinations of these: positional or keyword arguments with or without defaults variable positional arguments defined via * (often *args ) variable keyword arguments defined via ** (often **kwargs ) keyword only arguments - arguments after *, positional only arguments - arguments before , / (new in Python 3.8) To demonstrate all the above parameter types:","title":"Function Signatures"},{"location":"usage/validation_decorator/#using-field-to-describe-function-arguments","text":"Field can also be used with validate_arguments to provide extra information about the field and validations. In general it should be used in a type hint with Annotated , unless default_factory is specified, in which case it should be used as the default value of the field: The alias can be used with the decorator as normal.","title":"Using Field to describe function arguments"},{"location":"usage/validation_decorator/#usage-with-mypy","text":"The validate_arguments decorator should work \"out of the box\" with mypy since it's defined to return a function with the same signature as the function it decorates. The only limitation is that since we trick mypy into thinking the function returned by the decorator is the same as the function being decorated; access to the raw function or other attributes will require type: ignore .","title":"Usage with mypy"},{"location":"usage/validation_decorator/#validate-without-calling-the-function","text":"By default, arguments validation is done by directly calling the decorated function with parameters. But what if you wanted to validate them without actually calling the function? To do that you can call the validate method bound to the decorated function.","title":"Validate without calling the function"},{"location":"usage/validation_decorator/#raw-function","text":"The raw function which was decorated is accessible, this is useful if in some scenarios you trust your input arguments and want to call the function in the most performant way (see notes on performance below):","title":"Raw function"},{"location":"usage/validation_decorator/#async-functions","text":"validate_arguments can also be used on async functions:","title":"Async Functions"},{"location":"usage/validation_decorator/#custom-config","text":"The model behind validate_arguments can be customised using a config setting which is equivalent to setting the Config sub-class in normal models. Warning The fields and alias_generator properties of Config which allow aliases to be configured are not supported yet with @validate_arguments , using them will raise an error. Configuration is set using the config keyword argument to the decorator, it may be either a config class or a dict of properties which are converted to a class later.","title":"Custom Config"},{"location":"usage/validation_decorator/#limitations","text":"validate_arguments has been released on a provisional basis without all the bells and whistles, which may be added later, see #1205 for some more discussion of this. In particular:","title":"Limitations"},{"location":"usage/validation_decorator/#validation-exception","text":"Currently upon validation failure, a standard pydantic ValidationError is raised, see model error handling . This is helpful since it's str() method provides useful details of the error which occurred and methods like .errors() and .json() can be useful when exposing the errors to end users, however ValidationError inherits from ValueError not TypeError which may be unexpected since Python would raise a TypeError upon invalid or missing arguments. This may be addressed in future by either allow a custom error or raising a different exception by default, or both.","title":"Validation Exception"},{"location":"usage/validation_decorator/#coercion-and-strictness","text":"pydantic currently leans on the side of trying to coerce types rather than raise an error if a type is wrong, see model data conversion and validate_arguments is no different. See #1098 and other issues with the \"strictness\" label for a discussion of this. If pydantic gets a \"strict\" mode in future, validate_arguments will have an option to use this, it may even become the default for the decorator.","title":"Coercion and Strictness"},{"location":"usage/validation_decorator/#performance","text":"We've made a big effort to make pydantic as performant as possible and argument inspect and model creation is only performed once when the function is defined, however there will still be a performance impact to using the validate_arguments decorator compared to calling the raw function. In many situations this will have little or no noticeable effect, however be aware that validate_arguments is not an equivalent or alternative to function definitions in strongly typed languages; it never will be.","title":"Performance"},{"location":"usage/validation_decorator/#return-value","text":"The return value of the function is not validated against its return type annotation, this may be added as an option in future.","title":"Return Value"},{"location":"usage/validation_decorator/#config-and-validators","text":"fields and alias_generator on custom Config are not supported, see above . Neither are validators .","title":"Config and Validators"},{"location":"usage/validation_decorator/#model-fields-and-reserved-arguments","text":"The following names may not be used by arguments since they can be used internally to store information about the function's signature: v__args v__kwargs v__positional_only These names (together with \"args\" and \"kwargs\" ) may or may not (depending on the function's signature) appear as fields on the internal pydantic model accessible via .model thus this model isn't especially useful (e.g. for generating a schema) at the moment. This should be fixable in future as the way error are raised is changed.","title":"Model fields and reserved arguments"},{"location":"usage/validators/","text":"Custom validation and complex relationships between objects can be achieved using the validator decorator. A few things to note on validators: validators are \"class methods\", so the first argument value they receive is the UserModel class, not an instance of UserModel . the second argument is always the field value to validate; it can be named as you please you can also add any subset of the following arguments to the signature (the names must match): values : a dict containing the name-to-value mapping of any previously-validated fields config : the model config field : the field being validated. Type of object is pydantic.fields.ModelField . **kwargs : if provided, this will include the arguments above not explicitly listed in the signature validators should either return the parsed value or raise a ValueError , TypeError , or AssertionError ( assert statements may be used). Warning If you make use of assert statements, keep in mind that running Python with the -O optimization flag disables assert statements, and validators will stop working . where validators rely on other values, you should be aware that: Validation is done in the order fields are defined. E.g. in the example above, password2 has access to password1 (and name ), but password1 does not have access to password2 . See Field Ordering for more information on how fields are ordered If validation fails on another field (or that field is missing) it will not be included in values , hence if 'password1' in values and ... in this example. Pre and per-item validators \u00b6 Validators can do a few more complex things: A few more things to note: a single validator can be applied to multiple fields by passing it multiple field names a single validator can also be called on all fields by passing the special value '*' the keyword argument pre will cause the validator to be called prior to other validation passing each_item=True will result in the validator being applied to individual values (e.g. of List , Dict , Set , etc.), rather than the whole object Subclass Validators and each_item \u00b6 If using a validator with a subclass that references a List type field on a parent class, using each_item=True will cause the validator not to run; instead, the list must be iterated over programmatically. Validate Always \u00b6 For performance reasons, by default validators are not called for fields when a value is not supplied. However there are situations where it may be useful or required to always call the validator, e.g. to set a dynamic default value. You'll often want to use this together with pre , since otherwise with always=True pydantic would try to validate the default None which would cause an error. Reuse validators \u00b6 Occasionally, you will want to use the same validator on multiple fields/models (e.g. to normalize some input data). The \"naive\" approach would be to write a separate function, then call it from multiple decorators. Obviously, this entails a lot of repetition and boiler plate code. To circumvent this, the allow_reuse parameter has been added to pydantic.validator in v1.2 ( False by default): As it is obvious, repetition has been reduced and the models become again almost declarative. Tip If you have a lot of fields that you want to validate, it usually makes sense to define a help function with which you will avoid setting allow_reuse=True over and over again. Root Validators \u00b6 Validation can also be performed on the entire model's data. As with field validators, root validators can have pre=True , in which case they're called before field validation occurs (and are provided with the raw input data), or pre=False (the default), in which case they're called after field validation. Field validation will not occur if pre=True root validators raise an error. As with field validators, \"post\" (i.e. pre=False ) root validators by default will be called even if prior validators fail; this behaviour can be changed by setting the skip_on_failure=True keyword argument to the validator. The values argument will be a dict containing the values which passed field validation and field defaults where applicable. Field Checks \u00b6 On class creation, validators are checked to confirm that the fields they specify actually exist on the model. Occasionally however this is undesirable: e.g. if you define a validator to validate fields on inheriting models. In this case you should set check_fields=False on the validator. Dataclass Validators \u00b6 Validators also work with pydantic dataclasses.","title":"Validators"},{"location":"usage/validators/#pre-and-per-item-validators","text":"Validators can do a few more complex things: A few more things to note: a single validator can be applied to multiple fields by passing it multiple field names a single validator can also be called on all fields by passing the special value '*' the keyword argument pre will cause the validator to be called prior to other validation passing each_item=True will result in the validator being applied to individual values (e.g. of List , Dict , Set , etc.), rather than the whole object","title":"Pre and per-item validators"},{"location":"usage/validators/#subclass-validators-and-each_item","text":"If using a validator with a subclass that references a List type field on a parent class, using each_item=True will cause the validator not to run; instead, the list must be iterated over programmatically.","title":"Subclass Validators and each_item"},{"location":"usage/validators/#validate-always","text":"For performance reasons, by default validators are not called for fields when a value is not supplied. However there are situations where it may be useful or required to always call the validator, e.g. to set a dynamic default value. You'll often want to use this together with pre , since otherwise with always=True pydantic would try to validate the default None which would cause an error.","title":"Validate Always"},{"location":"usage/validators/#reuse-validators","text":"Occasionally, you will want to use the same validator on multiple fields/models (e.g. to normalize some input data). The \"naive\" approach would be to write a separate function, then call it from multiple decorators. Obviously, this entails a lot of repetition and boiler plate code. To circumvent this, the allow_reuse parameter has been added to pydantic.validator in v1.2 ( False by default): As it is obvious, repetition has been reduced and the models become again almost declarative. Tip If you have a lot of fields that you want to validate, it usually makes sense to define a help function with which you will avoid setting allow_reuse=True over and over again.","title":"Reuse validators"},{"location":"usage/validators/#root-validators","text":"Validation can also be performed on the entire model's data. As with field validators, root validators can have pre=True , in which case they're called before field validation occurs (and are provided with the raw input data), or pre=False (the default), in which case they're called after field validation. Field validation will not occur if pre=True root validators raise an error. As with field validators, \"post\" (i.e. pre=False ) root validators by default will be called even if prior validators fail; this behaviour can be changed by setting the skip_on_failure=True keyword argument to the validator. The values argument will be a dict containing the values which passed field validation and field defaults where applicable.","title":"Root Validators"},{"location":"usage/validators/#field-checks","text":"On class creation, validators are checked to confirm that the fields they specify actually exist on the model. Occasionally however this is undesirable: e.g. if you define a validator to validate fields on inheriting models. In this case you should set check_fields=False on the validator.","title":"Field Checks"},{"location":"usage/validators/#dataclass-validators","text":"Validators also work with pydantic dataclasses.","title":"Dataclass Validators"}]}